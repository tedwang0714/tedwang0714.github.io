<!DOCTYPE html><html lang="en"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1"><meta name="format-detection" content="telephone=no"><meta name="apple-mobile-web-app-capable" content="yes"><meta name="apple-mobile-web-app-status-bar-style" content="black"><meta name="keywords" content="博铭是个傻宝宝, 专属匣宝宝"><meta name="description" content="大数据学习"><title>Spark Core 第一课</title><link rel="icon" href="/images/icons/favicon-16x16.png?v=1.7.0" type="image/png" sizes="16x16"><link rel="icon" href="/images/icons/favicon-32x32.png?v=1.7.0" type="image/png" sizes="32x32"><link rel="dns-prefetch" href="https://cdn.jsdelivr.net"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/font-awesome@4.7.0/css/font-awesome.min.css" type="text/css"><link rel="stylesheet" href="/css/index.css?v=1.7.0"><script>var Stun = window.Stun || {};
var CONFIG = {
  root: '/',
  algolia: undefined,
  fontawesome: {"prefix":"fa"},
  sidebar: {"offsetTop":"20px","tocMaxDepth":6},
  header: {"enable":true,"showOnPost":true,"scrollDownIcon":false},
  post_widget: {"end_text":true},
  night_mode: {"enable":true},
  back2top: {"enable":true},
  codeblock: {"style":"default","highlight":"light","word_wrap":false},
  reward: false,
  fancybox: false,
  zoom_image: {"enable":true,"mask_color":"rgba(0,0,0,0.6)"},
  gallery_waterfall: undefined,
  lazyload: undefined,
  pjax: undefined,
  external_link: {"icon":{"enable":true,"name":"external-link"}},
  shortcuts: undefined,
  prompt: {"copy_success":"Copy Success","copy_error":"Copy Error","creative_commons":"Creative Commons","copy_button":"Copy"}
};

window.CONFIG = CONFIG;</script><meta name="generator" content="Hexo 4.2.0"></head><body><div class="container" id="container"><header class="header" id="header"><div class="header-inner"><nav class="header-nav header-nav--fixed"><div class="header-nav-inner"><div class="header-nav-btn fa fa-bars"></div><div class="header-nav-menu"><div class="header-nav-menu-item"><a class="header-nav-menu-item__a" href="/"><i class="fa fa-home"></i>Home</a></div><div class="header-nav-menu-item"><a class="header-nav-menu-item__a" href="/archives/"><i class="fa fa-folder-open"></i>Archives</a></div></div><div class="header-nav-mode"><div class="mode"><div class="mode-track"><span class="mode-track-moon"></span><span class="mode-track-sun"></span></div><div class="mode-thumb"></div></div></div></div></nav><div class="header-info"><div class="header-info-inner"><div class="header-info-title">专属匣宝宝</div><div class="header-info-subtitle"></div></div></div></div></header><main class="main" id="main"><div class="main-inner"><div class="content-wrap" id="content-wrap"><div class="content"><div id="is-post"></div><div class="post"><header class="post-header"><h1 class="post-header-title">Spark Core 第一课</h1><div class="post-header-meta"><span class="post-header-meta-create"><i class="fa fa-calendar-o"></i><span>Posted </span><span>2017-03-05</span></span><span class="post-header-meta-update"><i class="fa fa-calendar-check-o"></i><span>updated </span><span>2020-02-13</span></span></div></header><div class="post-body">
        <h1   id="Spark-Core01" >
          <span class="heading-link">Spark Core01</span>
        </h1>
      <p>学习目标：</p>
<ol>
<li>RDD 5大特性</li>
<li>SparkCore 的编程规范</li>
<li>RDD 创建方式</li>
<li>RDD 的算子</li>
</ol>
<p>一. RDD 的5 大特性</p>
<ul>
<li><ol>
<li>RDD 是 一系列的 partition ： A list of partition<br>比如一个数据 val data = Array(1,2,3,4,5,6,7,8,9)<br>会把数据分成一个一个的partition，比如<br>(1,2,3) , (4,5,6), (7,8,9)</li>
</ol>
</li>
<li><ol start="2">
<li>函数(方法) 是作用在每一个 partition 上的: A function for computing each split<br>比如 data.map(_ * 2)  表示 对 每个分区的 元素 * 2</li>
</ol>
</li>
<li><ol start="3">
<li>RDD 之间存在依赖关系：A list of dependency on other RDDS<br>这种依赖指的是 某个RDD 是由另外的RDD 计算推导出来的<br>比如 data.map(_ * 10 )  ==&gt;  data2<br>那么 data2中的 rdd就是由 data 每个元素 * 10 得到的</li>
</ol>
</li>
<li><ol start="4">
<li>对于K,V 结构的RDD，可以进行 partitioner，即把具有相同特征的RDD分到一组</li>
</ol>
</li>
<li><ol start="5">
<li>把计算拉到数据节点</li>
</ol>
</li>
</ul>
<p>二. SparKCore 的编程规范</p>
<figure class="highlight plain"><div class="table-container"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">val conf &#x3D; new SparkConf()</span><br><span class="line">  .setMaster(&quot;local[2]&quot;)  传入 master 的类型，一般测试传入 local[2] 即可 </span><br><span class="line">  .setAppName(this.getClass.getSimpleName)  app的名称</span><br><span class="line"></span><br><span class="line">val sc &#x3D; new SparkContext(conf)</span><br></pre></td></tr></table></div></figure>


<p>如果不传入这两个会报错误，在源码中的体现：<br><img src="D:/HEXO/tedatablog/source/pictures/sparkcore01.png" alt="avatar"></p>
<p>三.  RDD 的创建方式</p>
<p>rdd创建方式一:  通过生成数据转换创建</p>
<figure class="highlight plain"><div class="table-container"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">scala&gt; val rdd1 &#x3D; sc.parallelize(List(1,2,3,4,5))</span><br><span class="line">rdd1: org.apache.spark.rdd.RDD[Int] &#x3D; ParallelCollectionRDD[6] at parallelize at &lt;console&gt;:24</span><br><span class="line"></span><br><span class="line">scala&gt; rdd1.map(_*2).collect</span><br><span class="line">res4: Array[Int] &#x3D; Array(2, 4, 6, 8, 10)</span><br></pre></td></tr></table></div></figure>


<p>rdd创建方式二：通过读取文件创建</p>
<figure class="highlight plain"><div class="table-container"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">scala&gt; val rdd2 &#x3D; sc.textFile(&quot;file:&#x2F;&#x2F;&#x2F;home&#x2F;bdp&#x2F;tmp&#x2F;wc.data&quot;)</span><br><span class="line">rdd2: org.apache.spark.rdd.RDD[String] &#x3D; file:&#x2F;&#x2F;&#x2F;home&#x2F;bdp&#x2F;tmp&#x2F;wc.data MapPartitionsRDD[5] at textFile at &lt;console&gt;:24</span><br><span class="line"></span><br><span class="line">scala&gt; rdd2.collect</span><br><span class="line">res3: Array[String] &#x3D; Array(boming,boming,boming, niuniu,niuniu, simao)</span><br></pre></td></tr></table></div></figure>


<p>rdd 创建方式三:<br>通过其他的rdd创建</p>
<p>四.  RDD 算子</p>
<p>RDD 算子可以分为两种类型：transformation 和 action<br>transformation 指的是中间的转换过程，是没有真正执行的<br>action 是真正执行的</p>
<p>transformation 算子</p>
<p>一定要记得，现在的操作都是对RDD 进行的操作，一定要转换成RDD</p>
<ol>
<li>map：作用在rdd 的每个元素上做相同的操作</li>
</ol>
<figure class="highlight plain"><div class="table-container"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">val rdd &#x3D; sc.parallelize(Array(1,2,3,4,5,6))</span><br><span class="line">scala&gt; rdd.map(x &#x3D;&gt; &#123;x*2&#125;)</span><br><span class="line">res6: Array[Int] &#x3D; Array(2, 4, 6, 8, 10, 12)</span><br></pre></td></tr></table></div></figure>




<ol start="2">
<li>filter： 过滤出符合条件的元素</li>
</ol>
<figure class="highlight plain"><div class="table-container"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">scala&gt; rdd.map(x &#x3D;&gt; &#123;x*2&#125;).filter(_ &gt; 6)</span><br><span class="line">res8: Array[Int] &#x3D; Array(8, 10, 12)</span><br><span class="line"></span><br><span class="line">scala&gt; rdd.map(x &#x3D;&gt; &#123;x*2&#125;).filter(x &#x3D;&gt; &#123;x &gt; 6 &#125;)</span><br><span class="line">res9: Array[Int] &#x3D; Array(8, 10, 12)</span><br></pre></td></tr></table></div></figure>


<ol start="3">
<li>mapPartitions</li>
</ol>
<figure class="highlight plain"><div class="table-container"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">scala&gt; rdd.mapPartitions( par &#x3D;&gt; &#123;par.map(_*2)&#125;).collect</span><br><span class="line">res20: Array[Int] &#x3D; Array(2, 4, 6, 8, 10, 12)</span><br></pre></td></tr></table></div></figure>

<p>这个 算子的作用, 比如 有 100个元素, 10个分区, 使用 map 要调用 100次, 使用 mapPartitions 只需10次<br>可以引申到 MySQL的connection, 使用 mapPartitions可以减少很多的连接</p>
<ol start="4">
<li>flatMap : map 之后把 元素拍扁</li>
</ol>
<figure class="highlight plain"><div class="table-container"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">scala&gt;  val rdd2 &#x3D; sc.parallelize(Array(Array(1,2,3),Array(4,5,6),Array(7,8,9)))</span><br><span class="line">scala&gt; rdd2.flatMap(x &#x3D;&gt; x).collect</span><br><span class="line">res26: Array[Int] &#x3D; Array(1, 2, 3, 4, 5, 6, 7, 8, 9)</span><br></pre></td></tr></table></div></figure>




<ol start="5">
<li>mapPartitionWIthIndex</li>
</ol>
<figure class="highlight plain"><div class="table-container"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">rdd1.mapPartitionsWithIndex((index, partition) &#x3D;&gt; &#123;</span><br><span class="line">  partition.map( x &#x3D;&gt; &#123;</span><br><span class="line">    println(s&quot;$&#123;index&#125;, $x&quot;)</span><br><span class="line">  &#125;)</span><br><span class="line">&#125;)</span><br></pre></td></tr></table></div></figure>

<p>把 partition 和 index 输出</p>
<ol start="6">
<li>mapValues<br>只对KV 结构的V做相同的操作</li>
</ol>
<figure class="highlight plain"><div class="table-container"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">scala&gt; rdd1.map((_,1)).mapValues(_*3).collect</span><br><span class="line">res4: Array[(String, Int)] &#x3D; Array((iphone11 7000 20,3), (hwpro30 5000 100,3), (xiaomi 3000 200,3), (sumsung 6000 1000,3))</span><br></pre></td></tr></table></div></figure>


<ol start="7">
<li>reduceByKey</li>
</ol>
<p>把相同的key的元素放在一块，再进行两两相邻的操作</p>
<figure class="highlight plain"><div class="table-container"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">scala&gt; val rdd2 &#x3D; sc.parallelize(List(1,2,1,2,3,6,3,1,5))</span><br><span class="line">rdd2: org.apache.spark.rdd.RDD[Int] &#x3D; ParallelCollectionRDD[11] at parallelize at &lt;console&gt;:24</span><br><span class="line"></span><br><span class="line">scala&gt; rdd2.map((_,1)).reduceByKey(_+_).collect</span><br><span class="line">res9: Array[(Int, Int)] &#x3D; Array((1,3), (6,1), (3,2), (5,1), (2,2))</span><br></pre></td></tr></table></div></figure>



<ol start="8">
<li>union<br>把相同类型的数据放在一起</li>
</ol>
<p>注意，rdd1 和 rdd2 如果类型不同，不能进行 union<br>比如 rdd1为 String，rdd2 为 Int，会报下面的错误</p>
<figure class="highlight plain"><div class="table-container"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">scala&gt; rdd1.union(rdd2).collect</span><br><span class="line">&lt;console&gt;:28: error: type mismatch;</span><br><span class="line"> found   : org.apache.spark.rdd.RDD[Int]</span><br><span class="line"> required: org.apache.spark.rdd.RDD[String]</span><br><span class="line">       rdd1.union(rdd2).collect</span><br><span class="line"></span><br><span class="line">scala&gt; val rdd2 &#x3D; sc.parallelize(Array(&quot;ok haixing&quot;))</span><br><span class="line">rdd2: org.apache.spark.rdd.RDD[String] &#x3D; ParallelCollectionRDD[21] at parallelize at &lt;console&gt;:24</span><br><span class="line"></span><br><span class="line">scala&gt; rdd1.union(rdd2).collect</span><br><span class="line">res16: Array[String] &#x3D; Array(iphone11 7000 20, hwpro30 5000 100, xiaomi 3000 200, sumsung 6000 1000, ok haixing)</span><br></pre></td></tr></table></div></figure>


<ol start="9">
<li>distinct</li>
</ol>
<p>去除 rdd的重复的元素</p>
<figure class="highlight plain"><div class="table-container"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">scala&gt; val rdd2 &#x3D; sc.parallelize(List(1,2,1,2,3,6,3,1,5))</span><br><span class="line">rdd2: org.apache.spark.rdd.RDD[Int] &#x3D; ParallelCollectionRDD[23] at parallelize at &lt;console&gt;:24</span><br><span class="line"></span><br><span class="line">scala&gt; rdd2.distinct.collect</span><br><span class="line">res19: Array[Int] &#x3D; Array(1, 6, 3, 5, 2)</span><br></pre></td></tr></table></div></figure>


<ol start="10">
<li>groupByKey<br>把相同key的元素放到一组，得到的是 CompactBuffer 类型的value</li>
</ol>
<figure class="highlight plain"><div class="table-container"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">scala&gt; rdd2.map((_,1)).groupByKey().collect</span><br><span class="line">res27: Array[(Int, Iterable[Int])] &#x3D; Array((1,CompactBuffer(1, 1, 1)), (6,CompactBuffer(1)), (3,CompactBuffer(1, 1)), (5,CompactBuffer(1)), (2,CompactBuffer(1, 1)))</span><br></pre></td></tr></table></div></figure>


<ol start="11">
<li><p>groupBy<br>可以按照不同的分组条件进行分组<br>比如，对 value 进行分组<br>rdd.map((<em>,1)).groupBy(</em>._2).foreach(println)<br>输出：<br>因为value都是1，所以输出的就只有1组<br>(1,CompactBuffer((1,1), (2,1), (3,1), (4,1), (5,1), (6,1)))</p>
</li>
<li><p>join</p>
</li>
</ol>
<figure class="highlight plain"><div class="table-container"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line">val rdd1 &#x3D; sc.textFile(&quot;data&#x2F;dept.txt&quot;）</span><br><span class="line">val rdd2 &#x3D; sc.textFile(&quot;data&#x2F;emp.txt&quot;)</span><br><span class="line"></span><br><span class="line">val deptRdd &#x3D; rdd1.map(x &#x3D;&gt; &#123;</span><br><span class="line">  val splits &#x3D; x.split(&quot;,&quot;)</span><br><span class="line">  (splits(0).trim, (splits(1).trim, splits(2).trim))</span><br><span class="line">&#125;)</span><br><span class="line">val empRdd &#x3D; rdd2.map(x &#x3D;&gt; &#123;</span><br><span class="line">  val splits &#x3D; x.split(&quot; &quot;)</span><br><span class="line">  (splits(4).trim, (splits(0).trim, splits(1).trim, splits(2).trim, splits(3).trim))</span><br><span class="line">&#125;)</span><br><span class="line">deptRdd.join(empRdd)</span><br><span class="line">deptRdd.leftOutJoin(empRdd)</span><br><span class="line">deptRdd.rightOutJoin(empRdd)</span><br><span class="line">deptRdd.fullOutJoin(empRdd)</span><br><span class="line">(20,(Some((RESEARCH,DALLAS)),Some((1002,bill,ceo,55))))</span><br><span class="line">(30,(Some((SALES,CHICAGO)),Some((1003,cindy,cw,32))))</span><br><span class="line">(40,(Some((OPERATIONS,BOSTON)),None))</span><br><span class="line">(10,(Some((ACCOUNTIN,NEW YORK)),Some((1001,bob,sales,30))))</span><br></pre></td></tr></table></div></figure>


<ol start="13">
<li>cogroup</li>
</ol>
<figure class="highlight plain"><div class="table-container"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">deptRdd.cogroup(empRdd).foreach(println)</span><br><span class="line">输出：</span><br><span class="line">(20,(CompactBuffer((RESEARCH,DALLAS)),CompactBuffer((1002,bill,ceo,55))))</span><br><span class="line">(30,(CompactBuffer((SALES,CHICAGO)),CompactBuffer((1003,cindy,cw,32))))</span><br><span class="line">(40,(CompactBuffer((OPERATIONS,BOSTON)),CompactBuffer()))</span><br><span class="line">(10,(CompactBuffer((ACCOUNTIN,NEW YORK)),CompactBuffer((1001,bob,sales,30))))</span><br><span class="line"></span><br><span class="line">join 和 从group的关系</span><br><span class="line">join 底层调用 了 从group</span><br><span class="line">def join[W](other: RDD[(K, W)], partitioner: Partitioner): RDD[(K, (V, W))] &#x3D; 				  self.withScope &#123;</span><br><span class="line">  this.cogroup(other, partitioner).flatMapValues( pair &#x3D;&gt;</span><br><span class="line">    for (v &lt;- pair._1.iterator; w &lt;- pair._2.iterator) yield (v, w)</span><br><span class="line">  )</span><br><span class="line">&#125;</span><br></pre></td></tr></table></div></figure>



<ol start="14">
<li>zipWithIndex<br>相当于给每个元素加个索引，形成K,V 结构    </li>
</ol>
<figure class="highlight plain"><div class="table-container"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">scala&gt; rdd1.zipWithIndex.collectAsMap</span><br><span class="line">res18: scala.collection.Map[Int,Long] &#x3D; Map(2 -&gt; 1, 5 -&gt; 4, 4 -&gt; 3, 1 -&gt; 0, 3 -&gt; 2, 6 -&gt; 5)</span><br></pre></td></tr></table></div></figure>
</div><footer class="post-footer"><div class="post-end"><p><span>------ </span><span>End of the article, thanks for your reading</span><span> ------</span></p></div><div class="post-copyright"><div class="post-copyright-author"><span class="post-copyright-author-name">Author: </span><span class="post-copyright-author-value"><a href="http://yoursite.com">博铭是个傻宝宝</a></span></div><div class="post-copyright-link"><span class="post-copyright-link-name">Link: </span><span class="post-copyright-link-value"><a href="http://yoursite.com/2017/03/05/Spark%20Core%2001/">http://yoursite.com/2017/03/05/Spark%20Core%2001/</a></span></div><div class="post-copyright-notice"><span class="post-copyright-notice-name">Copyright: </span><span class="post-copyright-notice-value">All articles in this blog are licensed under <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/deed.en" rel="external nofollow" target="_blank">BY-NC-SA</a> unless stating additionally</span></div></div><div class="post-tags"><span class="post-tags-item"><i class="post-tags-item__i fa fa-tags"></i><a class="post-tags-item__a" href="http://yoursite.com/tags/tedata/">tedata</a></span></div><nav class="paginator"><div class="paginator-post"><div class="paginator-post-prev"><a href="/2017/03/07/Spark%20Core%2002/"><i class="fa fa-chevron-left"></i><span>Spark Core 第二课</span></a></div></div></nav></footer></div></div></div><aside class="sidebar" id="sidebar"><div class="sidebar-inner"><div class="sidebar-nav"><span class="sidebar-nav-toc current">Catalog</span><span class="sidebar-nav-ov">Overview</span></div><section class="sidebar-toc"><div><ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#Spark-Core01"><span class="toc-number">1.</span> <span class="toc-text">
          Spark Core01
        </span></a></li></ol></div></section><!-- ov = overview --><section class="sidebar-ov hide"><div class="sidebar-ov-author"><div class="sidebar-ov-author__avatar"><img class="sidebar-ov-author__avatar_img" src="/images/avatar.png" alt="avatar"></div><p class="sidebar-ov-author__p">hello world</p></div><div class="sidebar-ov-state"><a class="sidebar-ov-state__a posts" href="/archives/"><div class="sidebar-ov-state__a--count">4</div><div class="sidebar-ov-state__a--name">Archives</div></a></div><div class="sidebar-ov-cc"><a href="https://creativecommons.org/licenses/by-nc-sa/4.0/deed.en" target="_blank" rel="noopener" data-popover="Creative Commons" data-popover-pos="up"><img src="/images/cc-by-nc-sa.svg" alt="Creative Commons"></a></div></section><div class="sidebar-reading"><div class="sidebar-reading-info"><span>You have read </span><span class="sidebar-reading-info-num">0</span></div><div class="sidebar-reading-line"></div></div></div></aside><div class="clearfix"></div></div></main><footer class="footer" id="footer"><div class="footer-inner"><div><span>Copyright &copy; 2020</span><span class="fa fa-heart footer-icon"></span><span>博铭是个傻宝宝.</span></div><div><span>Powered by <a href="http://hexo.io/" title="hexo" target="_blank" rel="noopener">hexo</a></span><span> v4.2.0.</span><span class="separator">|</span><span>Theme - <a href="https://github.com/liuyib/hexo-theme-stun/" title="stun" target="_blank" rel="noopener">stun</a></span><span> v1.7.0.</span></div></div></footer><div class="loading-bar" id="loading-bar"><div class="progress"></div></div><div class="back2top" id="back2top"><i class="back2top-icon fa fa-rocket"></i></div></div><script src="https://cdn.jsdelivr.net/npm/jquery@v3.4.1/dist/jquery.min.js"></script><script src="https://cdn.jsdelivr.net/npm/velocity-animate@1.5.2/velocity.min.js"></script><script src="https://cdn.jsdelivr.net/npm/velocity-animate@1.5.2/velocity.ui.min.js"></script><script src="/js/utils.js?v=1.7.0"></script><script src="/js/stun-boot.js?v=1.7.0"></script><script src="/js/scroll.js?v=1.7.0"></script><script src="/js/header.js?v=1.7.0"></script><script src="/js/sidebar.js?v=1.7.0"></script></body></html>