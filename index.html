<!DOCTYPE html>
<html lang="en">

<head>
  <meta charset="utf-8" />
    
  <meta name="description" content="大数据学习" />
  
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1" />
  <title>
     TedWang 的大数据
  </title>
  <meta name="generator" content="hexo-theme-yilia-plus">
  
  <link rel="shortcut icon" href="/images/xingji.png" />
  
  
<link rel="stylesheet" href="/css/style.css">

  
<script src="/js/pace.min.js"></script>


  

  

</head>

</html>

<body>
  <div id="app">
    <main class="content">
      
<section class="cover">
    
  <div class="cover-frame">
    <div class="bg-box">
      <img src="/images/xk.jpg" alt="image frame" />
    </div>
    <div class="cover-inner text-center text-white">
      <h1><a href="/">TedWang 的大数据</a></h1>
      <div id="subtitle-box">
        
        <span id="subtitle"></span>
        
      </div>
      <div>
        
      </div>
    </div>
  </div>
  <div class="cover-learn-more">
    <a href="javascript:void(0)" class="anchor"><i class="ri-arrow-down-line"></i></a>
  </div>
</section>



<script src="https://cdn.jsdelivr.net/npm/typed.js@2.0.11/lib/typed.min.js"></script>

<div id="main">
  <section class="outer">
  <article class="articles">
    
    
    
    
    <article id="post-hadoopjob" class="article article-type-post" itemscope
  itemprop="blogPost" data-scroll-reveal>

  <div class="article-inner">
    
    <header class="article-header">
       
<h2 itemprop="name">
  <a class="article-title" href="/2020/02/22/hadoopjob/"
    >hadoopjob</a
  >
</h2>
  

    </header>
    

    
    <div class="article-meta">
      <a href="/2020/02/22/hadoopjob/" class="article-date">
  <time datetime="2020-02-22T03:29:16.682Z" itemprop="datePublished">2020-02-22</time>
</a>
      
      
      
      
    </div>
    

    

    
    <div class="article-entry" itemprop="articleBody">
      


      

      
      <h1 id="Hadoop-job-的执行流程"><a href="#Hadoop-job-的执行流程" class="headerlink" title="Hadoop job 的执行流程"></a>Hadoop job 的执行流程</h1><p>Hadoop 中的 job 任务包含 Mapper 和 Reducer 过程；但这只是最简单的划分，为了理清 job的执行过程将其划分为：job 层、MR 层，每层又由很多小部分组成。</p>
<p>job 层<br>job.waitForCompletion(true) 是我们提交 job 任务代码，从这个切入点出发：</p>
      
      <a class="article-more-link" href="/2020/02/22/hadoopjob/">点我继续看吧...</a>
      
      
      <!-- reward -->
      
    </div>
    
    
      <!-- copyright -->
      
    <footer class="article-footer">
      
      

    </footer>

  </div>

  

  
  
  

  

</article>
    
    <article id="post-flinkhbase01" class="article article-type-post" itemscope
  itemprop="blogPost" data-scroll-reveal>

  <div class="article-inner">
    
    <header class="article-header">
       
<h2 itemprop="name">
  <a class="article-title" href="/2020/02/21/flinkhbase01/"
    >flinkhbase01</a
  >
</h2>
  

    </header>
    

    
    <div class="article-meta">
      <a href="/2020/02/21/flinkhbase01/" class="article-date">
  <time datetime="2020-02-21T02:30:43.828Z" itemprop="datePublished">2020-02-21</time>
</a>
      
      
      
      
    </div>
    

    

    
    <div class="article-entry" itemprop="articleBody">
      


      

      
      <h1 id="flink-对接-HBase-开发"><a href="#flink-对接-HBase-开发" class="headerlink" title="flink 对接 HBase 开发"></a>flink 对接 HBase 开发</h1><h3 id="一-开发背景"><a href="#一-开发背景" class="headerlink" title="一. 开发背景"></a>一. 开发背景</h3><ol>
<li>简介<br>flink 官方并没有提供现成的API 进行HBase 的调用</li>
</ol>
<p>其实，在flink 中，有关于 Hadoop 的 InputFormat/OutputFormat 的描述<br><a href="https://ci.apache.org/projects/flink/flink-docs-release-1.9/dev/batch/hadoop_compatibility.html#using-hadoop-outputformats" target="_blank" rel="noopener">https://ci.apache.org/projects/flink/flink-docs-release-1.9/dev/batch/hadoop_compatibility.html#using-hadoop-outputformats</a></p>
<p>我们知道，HBase的数据是存放在HDFS之上的，所有沿着这个思路，从更底层去探索HBase的<br>Source/Sink 开发</p>
<ol start="2">
<li>前期准备</li>
</ol>
<p>开发准备：<br>开发需要的依赖包</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">&lt;dependency&gt;</span><br><span class="line">  &lt;groupId&gt;org.apache.flink&lt;&#x2F;groupId&gt;</span><br><span class="line">  &lt;artifactId&gt;flink-hbase_2.11&lt;&#x2F;artifactId&gt;</span><br><span class="line">  &lt;version&gt;$&#123;flink.version&#125;&lt;&#x2F;version&gt;</span><br><span class="line">&lt;&#x2F;dependency&gt;</span><br><span class="line"></span><br><span class="line">&lt;dependency&gt;</span><br><span class="line">  &lt;groupId&gt;org.apache.flink&lt;&#x2F;groupId&gt;</span><br><span class="line">  &lt;artifactId&gt;flink-hadoop-compatibility_2.11&lt;&#x2F;artifactId&gt;</span><br><span class="line">  &lt;version&gt;$&#123;flink.version&#125;&lt;&#x2F;version&gt;</span><br><span class="line">&lt;&#x2F;dependency&gt;</span><br></pre></td></tr></table></figure>



<h3 id="二-自定义Sink"><a href="#二-自定义Sink" class="headerlink" title="二. 自定义Sink"></a>二. 自定义Sink</h3><p>根据官网的内容，实际上是把数据转成HBase的格式，用 HadoopOutputFormat 写出去</p>
<p>即</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">new HadoopOutputFormat[Text, Mutation](new TableOutputFormat[Text](), Job.getInstance(configuration))</span><br><span class="line">Text 是CF的类型， Mutation 是value的类型，里面传入的参数就是 TableOutputFormat实例，和 Job实例</span><br></pre></td></tr></table></figure>


<p>这里面还需要注意，需要把造的数据转成 HBase支持的格式</p>
<p>这个代码如下：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br></pre></td><td class="code"><pre><span class="line">def convertToHBase(input: DataSet[(String, String, Int, String)]) &#x3D; &#123;</span><br><span class="line">  input.map(new RichMapFunction[(String, String, Int, String), (Text, Mutation)] &#123;</span><br><span class="line"></span><br><span class="line">    val cf &#x3D; &quot;o&quot;.getBytes()  &#x2F;&#x2F; cf的名称</span><br><span class="line"></span><br><span class="line">&#x2F;&#x2F; 重写 map 方法</span><br><span class="line">    override def map(value: (String, String, Int, String)): (Text, Mutation) &#x3D; &#123;</span><br><span class="line"></span><br><span class="line">      val id &#x3D; value._1</span><br><span class="line">      val name &#x3D; value._2</span><br><span class="line">      val age &#x3D; value._3</span><br><span class="line">      val city &#x3D; value._4</span><br><span class="line"></span><br><span class="line">      val text &#x3D; new Text(id)</span><br><span class="line">      val put &#x3D; new Put(id.getBytes())</span><br><span class="line"></span><br><span class="line">      if(StringUtils.isNotEmpty(name)) &#123;</span><br><span class="line">&#x2F;&#x2F;判断字段是否存在</span><br><span class="line">        put.addColumn(cf, Bytes.toBytes(&quot;name&quot;), Bytes.toBytes(name))</span><br><span class="line">      &#125;</span><br><span class="line"></span><br><span class="line">      if(StringUtils.isNotEmpty(age+&quot;&quot;)) &#123;</span><br><span class="line">        put.addColumn(cf, Bytes.toBytes(&quot;age&quot;), Bytes.toBytes(age.toString))</span><br><span class="line">      &#125;</span><br><span class="line"></span><br><span class="line">      if(StringUtils.isNotEmpty(city)) &#123;</span><br><span class="line">        put.addColumn(cf, Bytes.toBytes(&quot;city&quot;), Bytes.toBytes(city))</span><br><span class="line">      &#125;</span><br><span class="line"></span><br><span class="line">      (text, put)</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;)</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>


<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line">def main(args: Array[String]): Unit &#x3D; &#123;</span><br><span class="line"></span><br><span class="line">  val environment &#x3D; ExecutionEnvironment.getExecutionEnvironment</span><br><span class="line"></span><br><span class="line">  val students &#x3D; ListBuffer[(String, String, Int, String)]()</span><br><span class="line"></span><br><span class="line">  for(i&lt;- 1 to 10) &#123;</span><br><span class="line">    students.append((i+ &quot;&quot;, &quot;Tedata&quot; +i, 20+i, &quot;SZ&quot; +i))</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  val input &#x3D; environment.fromCollection(students)</span><br><span class="line">  val result &#x3D; convertToHBase(input) </span><br><span class="line"></span><br><span class="line">    &#x2F;&#x2F; 获得HBase的configuration，以设置连接的信息，包括zk的服务器，端口，操作的表，mapreduce输出目录</span><br><span class="line">  val configuration &#x3D; HBaseConfiguration.create()</span><br><span class="line">  configuration.set(&quot;hbase.zookeeper.quorum&quot;,&quot;tedata01&quot;)</span><br><span class="line">  configuration.set(&quot;hbase.zookeeper.property.clientPort&quot;,&quot;2181&quot;)</span><br><span class="line">  configuration.set(TableOutputFormat.OUTPUT_TABLE, &quot;tedata_stu2&quot;)</span><br><span class="line">  configuration.set(&quot;mapreduce.output.fileoutputformat.outputdir&quot;,&quot;&#x2F;tmp&quot;)</span><br><span class="line"></span><br><span class="line">  val job &#x3D; Job.getInstance(configuration)</span><br><span class="line">  result.output(new HadoopOutputFormat[Text, Mutation](new TableOutputFormat[Text](), job))</span><br><span class="line"></span><br><span class="line">  environment.execute(this.getClass.getSimpleName)</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">input.map(new RichMapFunction[(String, String, Int, String), (Text, Mutation)] 这里是重写了map方法</span><br></pre></td></tr></table></figure>




<h3 id="三-自定义-Source"><a href="#三-自定义-Source" class="headerlink" title="三. 自定义 Source"></a>三. 自定义 Source</h3><p>首先，按照 TableInuptFormat开发</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line">def main(args: Array[String]): Unit &#x3D; &#123;</span><br><span class="line"></span><br><span class="line">  val environment &#x3D; StreamExecutionEnvironment.getExecutionEnvironment</span><br><span class="line"></span><br><span class="line">  val cf &#x3D; &quot;o&quot;.getBytes()</span><br><span class="line"></span><br><span class="line">  val stream &#x3D; environment.createInput(new TableInputFormat[(String, String, Int, String)] &#123;</span><br><span class="line">    override def getScanner: Scan &#x3D; &#123;</span><br><span class="line"></span><br><span class="line">      val scan &#x3D; new Scan()</span><br><span class="line">      scan.addFamily(cf)</span><br><span class="line">      scan</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    override def getTableName: String &#x3D; &#123;</span><br><span class="line">      &quot;tedata_sku3&quot;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    override def mapResultToTuple(result: Result): (String, String, Int, String) &#x3D; &#123;</span><br><span class="line">      (Bytes.toString(result.getRow)</span><br><span class="line">        ,</span><br><span class="line">        Bytes.toString(result.getValue(cf, &quot;name&quot;.getBytes()))</span><br><span class="line">        ,</span><br><span class="line">        Bytes.toString(result.getValue(cf, &quot;age&quot;.getBytes())).toInt</span><br><span class="line">        ,</span><br><span class="line">        Bytes.toString(result.getValue(cf, &quot;city&quot;.getBytes()))</span><br><span class="line">      )</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">  )</span><br><span class="line">  stream.print()</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>


<p>是没有报错的，但是执行的时候，会报错：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">Error:(18, 46) type arguments [(String, String, Int, String)] do not conform to class TableInputFormat&#39;s type parameter bounds [T &lt;: org.apache.flink.api.java.tuple.Tuple]</span><br><span class="line">    val stream &#x3D; environment.createInput(new TableInputFormat[(String, String, Int, String)] &#123;</span><br></pre></td></tr></table></figure>

<p>显然，是报类型错误，<br>需要这样 构造 </p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">new TableInputFormat[Tuple4[String, String, Int, String]]</span><br></pre></td></tr></table></figure>


<p>写好后，执行，又报错：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">ERROR ZooKeeperWatcher: hconnection-0x40e60ece0x0, quorum&#x3D;localhost:2181, baseZNode&#x3D;&#x2F;hbase Received unexpected KeeperException, re-throwing exception [main]</span><br><span class="line">org.apache.zookeeper.KeeperException$ConnectionLossException: KeeperErrorCode &#x3D; ConnectionLoss for &#x2F;hbase&#x2F;hbaseid</span><br></pre></td></tr></table></figure>


<p>这个错误实际上是因为：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">override def configure(parameters: Configuration)&#x3D;&#123;</span><br><span class="line">  table &#x3D; createTable</span><br><span class="line">  if (table !&#x3D; null) &#123;</span><br><span class="line">    scan &#x3D; getScanner</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>


<p>完整代码为：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br></pre></td><td class="code"><pre><span class="line">def main(args: Array[String]): Unit &#x3D; &#123;</span><br><span class="line"></span><br><span class="line">  val environment &#x3D; ExecutionEnvironment.getExecutionEnvironment</span><br><span class="line"></span><br><span class="line">  val cf &#x3D; &quot;o&quot;.getBytes()</span><br><span class="line"></span><br><span class="line">  val stream &#x3D; environment.createInput(new TableInputFormat[Tuple4[String, String, Int, String]] &#123;</span><br><span class="line"></span><br><span class="line">    def createTable()&#x3D; &#123;</span><br><span class="line">      val configuration &#x3D; HBaseConfiguration.create()</span><br><span class="line">      configuration.set(&quot;hbase.zookeeper.quorum&quot;, &quot;tedata01&quot;)</span><br><span class="line">      configuration.set(&quot;hbase.zookeeper.property.clientPort&quot;, &quot;2181&quot;)</span><br><span class="line">      new HTable(configuration, getTableName)</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    override def configure(parameters: Configuration)&#x3D;&#123;</span><br><span class="line">      table &#x3D; createTable</span><br><span class="line">      if (table !&#x3D; null) &#123;</span><br><span class="line">        scan &#x3D; getScanner</span><br><span class="line">      &#125;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    override def getScanner: Scan &#x3D; &#123;</span><br><span class="line">      val scan &#x3D; new Scan()</span><br><span class="line">      scan.addFamily(cf)</span><br><span class="line">      scan</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    override def getTableName: String &#x3D; &#123;</span><br><span class="line">      &quot;tedata_stu3&quot;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    override def mapResultToTuple(result: Result): Tuple4[String, String, Int, String] &#x3D;&#123;</span><br><span class="line">      new Tuple4(Bytes.toString(result.getRow)</span><br><span class="line">        ,</span><br><span class="line">        Bytes.toString(result.getValue(cf, &quot;name&quot;.getBytes()))</span><br><span class="line">        ,</span><br><span class="line">        Bytes.toString(result.getValue(cf, &quot;age&quot;.getBytes())).toInt</span><br><span class="line">        ,</span><br><span class="line">        Bytes.toString(result.getValue(cf, &quot;city&quot;.getBytes())))</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">  )</span><br><span class="line">  stream.print()</span><br><span class="line"></span><br><span class="line">  environment.execute(this.getClass.getSimpleName)</span><br></pre></td></tr></table></figure>

<p>}</p>

      
      <!-- reward -->
      
    </div>
    
    
      <!-- copyright -->
      
    <footer class="article-footer">
      
      

    </footer>

  </div>

  

  
  
  

  

</article>
    
    <article id="post-scala 种种" class="article article-type-post" itemscope
  itemprop="blogPost" data-scroll-reveal>

  <div class="article-inner">
    
    <header class="article-header">
       
<h2 itemprop="name">
  <a class="article-title" href="/2019/10/10/scala%20%E7%A7%8D%E7%A7%8D/"
    >Scala 种种</a
  >
</h2>
  

    </header>
    

    
    <div class="article-meta">
      <a href="/2019/10/10/scala%20%E7%A7%8D%E7%A7%8D/" class="article-date">
  <time datetime="2019-10-10T04:00:00.000Z" itemprop="datePublished">2019-10-10</time>
</a>
      
  <div class="article-category">
    <a class="article-category-link" href="/categories/scala/">scala</a>
  </div>

      
      
      
    </div>
    

    

    
    <div class="article-entry" itemprop="articleBody">
      


      

      
      <p>scala 种种</p>
<p>学习目标：</p>
<p>scala 隐式转换</p>
<p>scala 伴生对象和伴生类</p>
<p>scala 模式匹配</p>
<p>scala 泛型 </p>
<p>scala 偏函数</p>
<p>scala 类边界</p>
<p>一.  scala 隐式转换</p>
<p>隐式实例一<br>def main(args: Array[String]): Unit = {<br>    implicit def shop2supermarket(shop: Shop) = new Supermarket(“foods”)<br>    val shop = new Shop()<br>    shop.restrant()<br>  }<br>}<br>class Shop()<br>class Supermarket(goods: String) {<br>  def restrant(): Unit = {<br>    println(s”supermarket has restrant…, so many $goods”)<br>  }<br>}<br>该隐式转换实现了 把 shop 类拥有了 supermarket 类的变量 和 方法</p>
      
      <a class="article-more-link" href="/2019/10/10/scala%20%E7%A7%8D%E7%A7%8D/">点我继续看吧...</a>
      
      
      <!-- reward -->
      
    </div>
    
    
      <!-- copyright -->
      
    <footer class="article-footer">
      
      
  <ul class="article-tag-list" itemprop="keywords"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/scala/" rel="tag">scala</a></li></ul>


    </footer>

  </div>

  

  
  
  

  

</article>
    
    <article id="post-sqoop01" class="article article-type-post" itemscope
  itemprop="blogPost" data-scroll-reveal>

  <div class="article-inner">
    
    <header class="article-header">
       
<h2 itemprop="name">
  <a class="article-title" href="/2019/04/25/sqoop01/"
    >记一次Sqoop 抽数失败问题</a
  >
</h2>
  

    </header>
    

    
    <div class="article-meta">
      <a href="/2019/04/25/sqoop01/" class="article-date">
  <time datetime="2019-04-25T12:43:27.000Z" itemprop="datePublished">2019-04-25</time>
</a>
      
      
      
      
    </div>
    

    

    
    <div class="article-entry" itemprop="articleBody">
      


      

      
      <h1 id="记一次Sqoop-抽数失败问题"><a href="#记一次Sqoop-抽数失败问题" class="headerlink" title="记一次Sqoop 抽数失败问题"></a>记一次Sqoop 抽数失败问题</h1><p>需求:</p>
<p>需要从  hive 导数到 mysql</p>
<p>hive 里面的 表是以 bdp_log  源里的 dashboard_access 表 的 storage_id 存储的, 字段名是 字段id</p>
<p>如下图:<br><img src="/2019/04/25/sqoop01/sqoop1.png" alt="avatar"></p>
<p>执行以下语句导数:</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">sqoop export --connect  &quot;jdbc:mysql:&#x2F;&#x2F;99.13.106.97&#x2F;bdp_service_zh?userUnicode&#x3D;true&amp;characterEncoding&#x3D;utf-8&quot; \</span><br><span class="line">--username root \</span><br><span class="line">--password 123456 \</span><br><span class="line">--table dashboard_access \</span><br><span class="line">--input-fields-terminated-by &#39;,&#39; \</span><br></pre></td></tr></table></figure>


<p>执行之后报错:</p>
      
      <a class="article-more-link" href="/2019/04/25/sqoop01/">点我继续看吧...</a>
      
      
      <!-- reward -->
      
    </div>
    
    
      <!-- copyright -->
      
    <footer class="article-footer">
      
      
  <ul class="article-tag-list" itemprop="keywords"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/tedata/" rel="tag">tedata</a></li></ul>


    </footer>

  </div>

  

  
  
  

  

</article>
    
    <article id="post-Flinkwatermarks" class="article article-type-post" itemscope
  itemprop="blogPost" data-scroll-reveal>

  <div class="article-inner">
    
    <header class="article-header">
       
<h2 itemprop="name">
  <a class="article-title" href="/2018/08/25/Flinkwatermarks/"
    >Flink Watermarks介绍</a
  >
</h2>
  

    </header>
    

    
    <div class="article-meta">
      <a href="/2018/08/25/Flinkwatermarks/" class="article-date">
  <time datetime="2018-08-25T12:43:27.000Z" itemprop="datePublished">2018-08-25</time>
</a>
      
      
      
      
    </div>
    

    

    
    <div class="article-entry" itemprop="articleBody">
      


      

      
      <h1 id="Flink-Watermarks-介绍"><a href="#Flink-Watermarks-介绍" class="headerlink" title="Flink Watermarks 介绍"></a>Flink Watermarks 介绍</h1><h3 id="一、watermark的概念"><a href="#一、watermark的概念" class="headerlink" title="一、watermark的概念"></a>一、watermark的概念</h3><p>watermark是一种衡量Event Time进展的机制，它是数据本身的一个隐藏属性。通常基于Event Time的数据，自身都包含一个timestamp，例如1472693399700（2016-09-01 09:29:59.700），而这条数据的watermark时间则可能是：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">watermark(1472693399700) &#x3D; 1472693396700(2016-09-0109:29:56.700)</span><br></pre></td></tr></table></figure>


<p>这条数据的watermark时间是什么含义呢？即：timestamp小于1472693396700(2016-09-01 09:29:56.700)的数据，都已经到达了。</p>
<p><img src="/2018/08/25/Flinkwatermarks/w1.png" alt="avatar"></p>
<p>图中蓝色虚线和实线代表着watermark的时间。</p>
<h3 id="二、watermark有什么用？"><a href="#二、watermark有什么用？" class="headerlink" title="二、watermark有什么用？"></a>二、watermark有什么用？</h3><p>watermark是用于处理乱序事件的，而正确的处理乱序事件，通常用watermark机制结合window来实现。</p>
<p>我们知道，流处理从事件产生，到流经source，再到operator，中间是有一个过程和时间的。虽然大部分情况下，流到operator的数据都是按照事件产生的时间顺序来的，但是也不排除由于网络、背压等原因，导致乱序的产生（out-of-order或者说late element）。</p>
<p>但是对于late element，我们又不能无限期的等下去，必须要有个机制来保证一个特定的时间后，必须触发window去进行计算了。这个特别的机制，就是watermark。</p>
<h3 id="三、watermark如何分配？"><a href="#三、watermark如何分配？" class="headerlink" title="三、watermark如何分配？"></a>三、watermark如何分配？</h3><p>通常，在接收到source的数据后，应该立刻生成watermark；但是，也可以在source后，应用简单的map或者filter操作，然后再生成watermark。</p>
<p>生成watermark的方式主要有2大类：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">(1):WithPeriodicWatermarks</span><br><span class="line">(2):WithPunctuatedWatermarks</span><br></pre></td></tr></table></figure>

<p>第一种可以定义一个最大允许乱序的时间，这种情况应用较多。</p>
<p>我们主要来围绕Periodic Watermarks来说明，下面是生成periodic watermark的方法：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line">&#x2F;**</span><br><span class="line"> * This generator generates watermarks assuming that elements come out of order to a certain degree only.</span><br><span class="line"> * The latest elements for a certain timestamp t will arrive at most n milliseconds after the earliest</span><br><span class="line"> * elements for timestamp t.</span><br><span class="line"> *&#x2F;</span><br><span class="line">class BoundedOutOfOrdernessGenerator extends AssignerWithPeriodicWatermarks[MyEvent] &#123;</span><br><span class="line"></span><br><span class="line">    val maxOutOfOrderness &#x3D; 3500L; &#x2F;&#x2F; 3.5 seconds</span><br><span class="line"></span><br><span class="line">    var currentMaxTimestamp: Long;</span><br><span class="line"></span><br><span class="line">    override def extractTimestamp(element: MyEvent, previousElementTimestamp: Long): Long &#x3D; &#123;</span><br><span class="line">        val timestamp &#x3D; element.getCreationTime()</span><br><span class="line">        currentMaxTimestamp &#x3D; max(timestamp, currentMaxTimestamp)</span><br><span class="line">        timestamp;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    override def getCurrentWatermark(): Watermark &#x3D; &#123;</span><br><span class="line">        &#x2F;&#x2F; return the watermark as current highest timestamp minus the out-of-orderness bound</span><br><span class="line">        new Watermark(currentMaxTimestamp - maxOutOfOrderness);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>


<p>程序中有一个extractTimestamp方法，就是根据数据本身的Event time来获取；还有一个getCurrentWatermar方法，<br>是用currentMaxTimestamp - maxOutOfOrderness来获取的。</p>
<p>这里的概念有点抽象，下面我们结合数据，在window中来实际演示下每个element的timestamp和watermark是多少，以及何时触发window。</p>
<h3 id="四、生成并跟踪watermark代码"><a href="#四、生成并跟踪watermark代码" class="headerlink" title="四、生成并跟踪watermark代码"></a>四、生成并跟踪watermark代码</h3><p>4.1、程序说明</p>
<p>我们从socket接收数据，然后经过map后立刻抽取timetamp并生成watermark，之后应用window来看看watermark和event time如何变化，才导致window被触发的。</p>
<p>4.2、代码如下</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br></pre></td><td class="code"><pre><span class="line">import java.text.SimpleDateFormat</span><br><span class="line"></span><br><span class="line">import org.apache.flink.streaming.api.scala._</span><br><span class="line">import org.apache.flink.streaming.api.TimeCharacteristic</span><br><span class="line">import org.apache.flink.streaming.api.functions.AssignerWithPeriodicWatermarks</span><br><span class="line">import org.apache.flink.streaming.api.scala.StreamExecutionEnvironment</span><br><span class="line">import org.apache.flink.streaming.api.scala.function.WindowFunction</span><br><span class="line">import org.apache.flink.streaming.api.watermark.Watermark</span><br><span class="line">import org.apache.flink.streaming.api.windowing.assigners.TumblingEventTimeWindows</span><br><span class="line">import org.apache.flink.streaming.api.windowing.time.Time</span><br><span class="line">import org.apache.flink.streaming.api.windowing.windows.TimeWindow</span><br><span class="line">import org.apache.flink.util.Collector</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">object WatermarkTest &#123;</span><br><span class="line"></span><br><span class="line">  def main(args: Array[String]): Unit &#x3D; &#123;</span><br><span class="line">    if (args.length !&#x3D; 2) &#123;</span><br><span class="line">      System.err.println(&quot;USAGE:\nSocketWatermarkTest &lt;hostname&gt; &lt;port&gt;&quot;)</span><br><span class="line">      return</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    val hostName &#x3D; args(0)</span><br><span class="line">    val port &#x3D; args(1).toInt</span><br><span class="line"></span><br><span class="line">    val env &#x3D; StreamExecutionEnvironment.getExecutionEnvironment</span><br><span class="line">    env.setStreamTimeCharacteristic(TimeCharacteristic.EventTime)</span><br><span class="line"></span><br><span class="line">    val input &#x3D; env.socketTextStream(hostName,port)</span><br><span class="line"></span><br><span class="line">    val inputMap &#x3D; input.map(f&#x3D;&gt; &#123;</span><br><span class="line">      val arr &#x3D; f.split(&quot;\\W+&quot;)</span><br><span class="line">      val code &#x3D; arr(0)</span><br><span class="line">      val time &#x3D; arr(1).toLong</span><br><span class="line">      (code,time)</span><br><span class="line">    &#125;)</span><br><span class="line"></span><br><span class="line">    val watermark &#x3D; inputMap.assignTimestampsAndWatermarks(new AssignerWithPeriodicWatermarks[(String,Long)] &#123;</span><br><span class="line"></span><br><span class="line">      var currentMaxTimestamp &#x3D; 0L</span><br><span class="line">      val maxOutOfOrderness &#x3D; 10000L&#x2F;&#x2F;最大允许的乱序时间是10s</span><br><span class="line"></span><br><span class="line">      var a : Watermark &#x3D; null</span><br><span class="line"></span><br><span class="line">      val format &#x3D; new SimpleDateFormat(&quot;yyyy-MM-dd HH:mm:ss.SSS&quot;)</span><br><span class="line"></span><br><span class="line">      override def getCurrentWatermark: Watermark &#x3D; &#123;</span><br><span class="line">        a &#x3D; new Watermark(currentMaxTimestamp - maxOutOfOrderness)</span><br><span class="line">        a</span><br><span class="line">      &#125;</span><br><span class="line"></span><br><span class="line">      override def extractTimestamp(t: (String,Long), l: Long): Long &#x3D; &#123;</span><br><span class="line">        val timestamp &#x3D; t._2</span><br><span class="line">        currentMaxTimestamp &#x3D; Math.max(timestamp, currentMaxTimestamp)</span><br><span class="line">        println(&quot;timestamp:&quot; + t._1 +&quot;,&quot;+ t._2 + &quot;|&quot; +format.format(t._2) +&quot;,&quot;+  currentMaxTimestamp + &quot;|&quot;+ format.format(currentMaxTimestamp) + &quot;,&quot;+ a.toString)</span><br><span class="line">        timestamp</span><br><span class="line">      &#125;</span><br><span class="line">    &#125;)</span><br><span class="line"></span><br><span class="line">    val window &#x3D; watermark</span><br><span class="line">    .keyBy(_._1)</span><br><span class="line">    .window(TumblingEventTimeWindows.of(Time.seconds(3)))</span><br><span class="line">    .apply(new WindowFunctionTest)</span><br><span class="line"></span><br><span class="line">    window.print()</span><br><span class="line"></span><br><span class="line">    env.execute()</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  class WindowFunctionTest extends WindowFunction[(String,Long),(String, Int,String,String,String,String),String,TimeWindow]&#123;</span><br><span class="line"></span><br><span class="line">    override def apply(key: String, window: TimeWindow, input: Iterable[(String, Long)], out: Collector[(String, Int,String,String,String,String)]): Unit &#x3D; &#123;</span><br><span class="line">      val list &#x3D; input.toList.sortBy(_._2)</span><br><span class="line">      val format &#x3D; new SimpleDateFormat(&quot;yyyy-MM-dd HH:mm:ss.SSS&quot;)</span><br><span class="line">      out.collect(key,input.size,format.format(list.head._2),format.format(list.last._2),format.format(window.getStart),format.format(window.getEnd))</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>



<p>4.3、程序详解</p>
<p>（1）接收socket数据</p>
<p>（2）将每行数据按照字符分隔，每行map成一个tuple类型（code，time）</p>
<p>（3）抽取timestamp生成watermark。并打印（code，time，格式化的time，currentMaxTimestamp，currentMaxTimestamp的格式化时间，watermark时间）。</p>
<p>（4）event time每隔3秒触发一次窗口，输出（code，窗口内元素个数，窗口内最早元素的时间，窗口内最晚元素的时间，窗口自身开始时间，窗口自身结束时间）</p>
<p>注意：new AssignerWithPeriodicWatermarks[(String,Long)中有抽取timestamp和生成watermark2个方法，在执行时，它是先抽取timestamp，后生成watermark，因此我们在这里print的watermark时间，其实是上一条的watermark时间，我们到数据输出时再解释。<br><img src="/2018/08/25/Flinkwatermarks/w2.png" alt="avatar"></p>
<p>生成的Job Graph</p>
<h3 id="五、通过数据跟踪watermark的时间"><a href="#五、通过数据跟踪watermark的时间" class="headerlink" title="五、通过数据跟踪watermark的时间"></a>五、通过数据跟踪watermark的时间</h3><p>我们重点看看watermark与timestamp的时间，并通过数据来看看window的触发时机。</p>
<p>首先，我们开启socket，输入第一条数据：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">000001,1461756862000</span><br></pre></td></tr></table></figure>

<p>输出的out文件如下：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">timestamp:000001,1461756862000|2016-04-27 19:34:22.000,1461756862000|2016-04-27 19:34:22.000,Watermark @ -10000</span><br></pre></td></tr></table></figure>

<p>这里，看下watermark的值，-10000，即0-10000得到的。这就说明程序先执行timestamp，后执行watermark。<br>所以，每条记录打印出的watermark，都应该是上一条的watermark。为了观察方便，我汇总了输出如下：<br><img src="/2018/08/25/Flinkwatermarks/w3.png" alt="avatar"></p>
<p>此时，wartermark的时间按照逻辑，已经落后于currentMaxTimestamp10秒了。我们继续输入：<br><img src="/2018/08/25/Flinkwatermarks/w4.png" alt="avatar"></p>
<p>此时，输出内容如下：<br><img src="/2018/08/25/Flinkwatermarks/w5.png" alt="avatar"></p>
<p>我们再次汇总，见下表：<br><img src="/2018/08/25/Flinkwatermarks/w6.png" alt="avatar"></p>
<p>我们继续输入，这时我们再次输入：<br><img src="/2018/08/25/Flinkwatermarks/w7.png" alt="avatar"></p>
<p>输出如下：<br><img src="/2018/08/25/Flinkwatermarks/w8.png" alt="avatar"></p>
<p>汇总如下：<br><img src="/2018/08/25/Flinkwatermarks/w9.png" alt="avatar"></p>
<p>到这里，window仍然没有被触发，此时watermark的时间已经等于了第一条数据的Event Time了。那么window到底什么时候被触发呢？我们再次输入：<br><img src="/2018/08/25/Flinkwatermarks/w10.png" alt="avatar"></p>
<p>输出：<br><img src="/2018/08/25/Flinkwatermarks/w11.png" alt="avatar"></p>
<p>汇总：<br><img src="/2018/08/25/Flinkwatermarks/w12.png" alt="avatar"></p>
<p>OK，window仍然没有触发，此时，我们的数据已经发到2016-04-27 19:34:33.000了，最早的数据已经过去了11秒了，还没有开始计算。那是不是要等到13（10+3）秒过去了，才开始触发window呢？答案是否定的。</p>
<p>我们再次增加1秒，输入：<br><img src="/2018/08/25/Flinkwatermarks/w13.png" alt="avatar"></p>
<p>输出：<br><img src="/2018/08/25/Flinkwatermarks/w14.png" alt="avatar"></p>
<p>汇总：<br><img src="/2018/08/25/Flinkwatermarks/w15.png" alt="avatar"></p>
<p>到这里，我们做一个说明：</p>
<p>window的触发机制，是先按照自然时间将window划分，如果window大小是3秒，那么1分钟内会把window划分为如下的形式:</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[00:00:00,00:00:03)[00:00:03,00:00:06)...[00:00:57,00:01:00)</span><br></pre></td></tr></table></figure>


<p>如果window大小是10秒，则window会被分为如下的形式：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[00:00:00,00:00:10)[00:00:10,00:00:20)...[00:00:50,00:01:00)</span><br></pre></td></tr></table></figure>

<p>window的设定无关数据本身，而是系统定义好了的。</p>
<p>输入的数据中，根据自身的Event Time，将数据划分到不同的window中，如果window中有数据，则当watermark时间&gt;=Event Time时，就符合了window触发的条件了，最终决定window触发，还是由数据本身的Event Time所属的window中的window_end_time决定。</p>
<p>上面的测试中，最后一条数据到达后，其水位线已经升至19:34:24秒，正好是最早的一条记录所在window的window_end_time，所以window就被触发了。</p>
<p>为了验证window的触发机制，我们继续输入数据：<br><img src="/2018/08/25/Flinkwatermarks/w16.png" alt="avatar"></p>
<p>输出：<br><img src="/2018/08/25/Flinkwatermarks/w17.png" alt="avatar"></p>
<p>汇总：<br><img src="/2018/08/25/Flinkwatermarks/w18.png" alt="avatar"></p>
<p>此时，watermark时间虽然已经达到了第二条数据的时间，但是由于其没有达到第二条数据所在window的结束时间，所以window并没有被触发。那么，第二条数据所在的window时间是：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[19:34:24,19:34:27)</span><br></pre></td></tr></table></figure>


<p>也就是说，我们必须输入一个19:34:27秒的数据，第二条数据所在的window才会被触发。我们继续输入：<br><img src="/2018/08/25/Flinkwatermarks/w19.png" alt="avatar"></p>
<p>输出：<br><img src="/2018/08/25/Flinkwatermarks/w20.png" alt="avatar"></p>
<p>汇总：<br><img src="/2018/08/25/Flinkwatermarks/w21.png" alt="avatar"></p>
<p>此时，我们已经看到，window的触发要符合以下几个条件：</p>
<p>1、watermark时间 &gt;= window_end_time2、在[window_start_time,window_end_time)中有数据存在<br>1<br>2<br>同时满足了以上2个条件，window才会触发。</p>
<p>而且，这里要强调一点，watermark是一个全局的值，不是某一个key下的值，所以即使不是同一个key的数据，其warmark也会增加，例如：</p>
<p>输入：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">000002,1461756879000</span><br></pre></td></tr></table></figure>

<p>输出：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">timestamp:000002,1461756879000|2016-04-27 19:34:39.000,1461756879000|2016-04-27 19:34:39.000,Watermark @ 1461756867000</span><br></pre></td></tr></table></figure>


<p>我们看到，currentMaxTimestamp也增加了。</p>
<h3 id="六、watermark-window处理乱序"><a href="#六、watermark-window处理乱序" class="headerlink" title="六、watermark+window处理乱序"></a>六、watermark+window处理乱序</h3><p>我们上面的测试，数据都是按照时间顺序递增的，现在，我们输入一些乱序的（late）数据，看看watermark结合window机制，是如何处理乱序的。</p>
<p>输入：<br><img src="/2018/08/25/Flinkwatermarks/w22.png" alt="avatar"></p>
<p>输出：<br><img src="/2018/08/25/Flinkwatermarks/w23.png" alt="avatar"></p>
<p>汇总：<br><img src="/2018/08/25/Flinkwatermarks/w24.png" alt="avatar"></p>
<p>可以看到，虽然我们输入了一个19:34:31的数据，但是currentMaxTimestamp和watermark都没变。此时，按照我们上面提到的公式：</p>
<p>1、watermark时间 &gt;= window_end_time2、在[window_start_time,window_end_time)中有数据存在</p>
<p>watermark时间（19:34:29） &lt; window_end_time（19:34:33），因此不能触发window。</p>
<p>那如果我们再次输入一条19:34:43的数据，此时watermark时间会升高到19:34:33，这时的window一定就会触发了，我们试一试：</p>
<p>输入：<br><img src="/2018/08/25/Flinkwatermarks/w25.png" alt="avatar"></p>
<p>输出：<br><img src="/2018/08/25/Flinkwatermarks/w26.png" alt="avatar"></p>
<p>这里，我么看到，窗口中有2个数据，19:34:31和19:34:32的，但是没有19:34:33的数据，原因是窗口是一个前闭后开的区间，19:34:33的数据是属于[19:34:33,19:34:36)的窗口的。</p>
<p>上边的结果，已经表明，对于out-of-order的数据，Flink可以通过watermark机制结合window的操作，来处理一定范围内的乱序数据。那么对于“迟到”太多的数据，Flink是怎么处理的呢？</p>
<h3 id="七、late-element的处理"><a href="#七、late-element的处理" class="headerlink" title="七、late element的处理"></a>七、late element的处理</h3><p>我们输入一个乱序很多的（其实只要Event Time &lt; watermark时间）数据来测试下：</p>
<p>输入：<br><img src="/2018/08/25/Flinkwatermarks/w27.png" alt="avatar"></p>
<p>输出：<br><img src="/2018/08/25/Flinkwatermarks/w28.png" alt="avatar"></p>
<p>我们看到，我们输入的数据是19:34:32的，而当前watermark时间已经来到了19:34:33，Event Time &lt; watermark时间，所以来一条就触发一个window。</p>
<h3 id="八、总结"><a href="#八、总结" class="headerlink" title="八、总结"></a>八、总结</h3><p>8.1、Flink如何处理乱序？</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">watermark+window机制</span><br></pre></td></tr></table></figure>

<p>window中可以对input进行按照Event Time排序，使得完全按照Event Time发生的顺序去处理数据，以达到处理乱序数据的目的。</p>
<p>8.2、Flink何时触发window？</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">1、Event Time &lt; watermark时间（对于late element太多的数据而言）</span><br></pre></td></tr></table></figure>

<p>或者</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">1. watermark时间 &gt;&#x3D; window_end_time（对于out-of-order以及正常的数据而言）</span><br><span class="line">2. 在[window_start_time,window_end_time)中有数据存在</span><br></pre></td></tr></table></figure>

<p>8.3、Flink应该如何设置最大乱序时间？</p>
<p>这个要结合自己的业务以及数据情况去设置。如果maxOutOfOrderness设置的太小，而自身数据发送时由于网络等原因导致乱序或者late太多，那么最终的结果就是会有很多单条的数据在window中被触发，数据的正确性影响太大。</p>
<p>最后，我们通过一张图来看看watermark、Event Time和window的关系：<br><img src="/2018/08/25/Flinkwatermarks/w29.png" alt="avatar"></p>
<p>文章转自：<a href="https://blog.csdn.net/lmalds/article/details/52704170" target="_blank" rel="noopener">https://blog.csdn.net/lmalds/article/details/52704170</a></p>

      
      <!-- reward -->
      
    </div>
    
    
      <!-- copyright -->
      
    <footer class="article-footer">
      
      
  <ul class="article-tag-list" itemprop="keywords"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/Flink/" rel="tag">Flink</a></li></ul>


    </footer>

  </div>

  

  
  
  

  

</article>
    
    <article id="post-YARN的架构及提交流程" class="article article-type-post" itemscope
  itemprop="blogPost" data-scroll-reveal>

  <div class="article-inner">
    
    <header class="article-header">
       
<h2 itemprop="name">
  <a class="article-title" href="/2018/07/18/YARN%E7%9A%84%E6%9E%B6%E6%9E%84%E5%8F%8A%E6%8F%90%E4%BA%A4%E6%B5%81%E7%A8%8B/"
    >YARN 的架构及提交流程</a
  >
</h2>
  

    </header>
    

    
    <div class="article-meta">
      <a href="/2018/07/18/YARN%E7%9A%84%E6%9E%B6%E6%9E%84%E5%8F%8A%E6%8F%90%E4%BA%A4%E6%B5%81%E7%A8%8B/" class="article-date">
  <time datetime="2018-07-18T12:43:27.000Z" itemprop="datePublished">2018-07-18</time>
</a>
      
      
      
      
    </div>
    

    

    
    <div class="article-entry" itemprop="articleBody">
      


      

      
      <h1 id="YARN-的架构及提交流程"><a href="#YARN-的架构及提交流程" class="headerlink" title="YARN 的架构及提交流程"></a>YARN 的架构及提交流程</h1><h3 id="一-Yarn-架构"><a href="#一-Yarn-架构" class="headerlink" title="一. Yarn 架构"></a>一. Yarn 架构</h3><p>YARN 的全称是 Yet Another Resource Negotiator，YARN 整体上是 Master/Slave 结构，在整个框架中，ResourceManager 为 Master，NodeManager 为 Slave，如下图所示：<br><img src="/2018/07/18/YARN%E7%9A%84%E6%9E%B6%E6%9E%84%E5%8F%8A%E6%8F%90%E4%BA%A4%E6%B5%81%E7%A8%8B/yarn20.gif" alt="avatar"></p>
<p>YARN 基本架构</p>
<h4 id="ResourceManager（RM）"><a href="#ResourceManager（RM）" class="headerlink" title="ResourceManager（RM）"></a>ResourceManager（RM）</h4><p>RM 是一个全局的资源管理器，负责整个系统的资源管理和分配，它主要有两个组件构成：<br>调度器：Scheduler；<br>应用程序管理器：Applications Manager，ASM。<br>调度器<br>调度器根据容量、􏳴队列等限制条件（如某个队列分配一定的资源，最多执行一定数量的作业等），将系统中的资源分配给各个正在运行的应用程序。􏰣要注意的是，该调度器是一个纯调度器，它不再从事任何与应用程序有关的工作，比如不负责重新启动（因应用程序失败或者硬件故障导致的失败），这些均交由应用程序相关的 ApplicationMaster 完成。调度器仅根据各个应用程序的资源需求进行资源分配，而资源分配单位用一个抽象概念 资源容器(Resource Container，也即 Container)，Container 是一个动态资源分配单位，它将内存、CPU、磁盘、网络等资源封装在一起，从而限定每个任务使用的资源量。此外，该调度器是一个可插拔的组件，用户可根据自己的需求设计新的调度器，YARN 提供了多种直接可用的调度器，比如 Fair Scheduler 和 Capacity Schedule 等。<br>应用程序管理器<br>应用程序管理器负责管理整个系统中所有应用程序，包括应用程序提交、与调度器协商资源以 AM、监控 AM 运行状态并在失败时重新启动它等。</p>
<h4 id="NodeManager（NM）"><a href="#NodeManager（NM）" class="headerlink" title="NodeManager（NM）"></a>NodeManager（NM）</h4><p>NM 是每个节点上运行的资源和任务管理器，一方面，它会定时向 RM 汇报本节点上的资源使用情况和各个 Container 的运行状态；另一方面，它接收并处理来自 AM 的 Container 启动/停止等各种请求。</p>
<h4 id="ApplicationMaster（AM）"><a href="#ApplicationMaster（AM）" class="headerlink" title="ApplicationMaster（AM）"></a>ApplicationMaster（AM）</h4><p>提交的每个作业都会包含一个 AM，主要功能包括：<br>与 RM 协商以获取资源（用 container 表示）；<br>将得到的任务进一步分配给内部的任务；<br>与 NM 通信以启动/停止任务；<br>监控所有任务的运行状态，当任务有失败时，重新为任务申请资源并重启任务。<br>MapReduce 就是原生支持 ON YARN 的一种框架，可以在 YARN 上运行 MapReduce 作业。有很多分布式应用都开发了对应的应用程序框架，用于在 YARN 上运行任务，例如 Spark，Storm、Flink 等。<br>Container<br>Container 是 YARN 中的资源抽象，它封装了某个节点上的多维度资源，如内存、CPU、磁盘、网络等，当 AM 向 RM 申请资源时，RM 为 AM 返回的资源便是用 Container 表示的。 YARN 会为每个任务分配一个 Container 且该任务只能使用该 Container 中描述的资源。</p>
<h3 id="二-YARN-作业提交流程"><a href="#二-YARN-作业提交流程" class="headerlink" title="二. YARN 作业提交流程"></a>二. YARN 作业提交流程</h3><p>当用户向 YARN 中提交一个应用程序后，YARN 将分两个阶段运行该应用程序：第一个阶段是启动 ApplicationMaster；第二个阶段是由 ApplicationMaster 创建应用程序，为它申请资源，并监控它的整个运行过程，直到运行完成，如下图所示<br><img src="/2018/07/18/YARN%E7%9A%84%E6%9E%B6%E6%9E%84%E5%8F%8A%E6%8F%90%E4%BA%A4%E6%B5%81%E7%A8%8B/yarnflow.png" alt="avatar"></p>
<h5 id="YARN-工作流程"><a href="#YARN-工作流程" class="headerlink" title="YARN 工作流程"></a>YARN 工作流程</h5><p>上图所示的 YARN 工作流程分为以下几个步骤：</p>
<ul>
<li>用户向 YARN 提交应用程序，其中包括 ApplicationMaster 程序，启动 ApplicationMaster 命令、用户程序等;</li>
<li>RM 为该应用程序分配第一个 Container，并与对应的 NM 通信，要求它在这个 Container 中启动应用程序的 ApplicationMaster；</li>
<li>ApplicationMaster 首先向 RM 注册，这样用户可以直接通过 NM 查看应用程序的运行状态，然后它将为各个任务申请资源，并监控它的运行状态，直到运行结束，一直重复下面的 4-7 步；</li>
<li>ApplicationMaster 采用轮询的方式通过 RPC 协议向 RM 申请和领取资源；</li>
<li>一旦 ApplicationMaster 申请到资源后，便与对应的 NM 通信，要求它启动任务；</li>
<li>NM 为任务设置好运行环境（包括环境变量、jar 包等）后，将任务启动命令写到一个脚本中，并通过运行该脚本启动任务；</li>
<li>各个任务通过某个 RPC 协议向 ApplicationMaster 汇报自己的状态和进度，以让 ApplicationMaster 随时掌握各个任务的运行状态，从而可以在任务失败时重新启动任务；</li>
<li>应用程序运行完成后，ApplicationMaster 向 RM 注销并关闭自己（当然像 Storm、Flink 这种常驻应用程序列外）。</li>
</ul>

      
      <!-- reward -->
      
    </div>
    
    
      <!-- copyright -->
      
    <footer class="article-footer">
      
      
  <ul class="article-tag-list" itemprop="keywords"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/tedata/" rel="tag">tedata</a></li></ul>


    </footer>

  </div>

  

  
  
  

  

</article>
    
    <article id="post-kafka02" class="article article-type-post" itemscope
  itemprop="blogPost" data-scroll-reveal>

  <div class="article-inner">
    
    <header class="article-header">
       
<h2 itemprop="name">
  <a class="article-title" href="/2018/07/18/kafka02/"
    >kafka 第二课</a
  >
</h2>
  

    </header>
    

    
    <div class="article-meta">
      <a href="/2018/07/18/kafka02/" class="article-date">
  <time datetime="2018-07-18T12:43:27.000Z" itemprop="datePublished">2018-07-18</time>
</a>
      
      
      
      
    </div>
    

    

    
    <div class="article-entry" itemprop="articleBody">
      


      

      
      <h1 id="kafka02"><a href="#kafka02" class="headerlink" title="kafka02"></a>kafka02</h1><ul>
<li><input checked disabled type="checkbox"> 主要内容：</li>
<li><input checked disabled type="checkbox"> <ol>
<li>核心概念</li>
</ol>
</li>
<li><input checked disabled type="checkbox"> <ol start="2">
<li>交付语义</li>
</ol>
</li>
<li><input checked disabled type="checkbox"> <ol start="3">
<li>分区选择</li>
</ol>
</li>
<li><input checked disabled type="checkbox"> <ol start="4">
<li>全局有序</li>
</ol>
</li>
<li><input checked disabled type="checkbox"> <ol start="5">
<li>调优</li>
</ol>
</li>
<li><input checked disabled type="checkbox"> <ol start="6">
<li>监控</li>
</ol>
</li>
<li><input checked disabled type="checkbox"> <ol start="7">
<li>故障案例</li>
</ol>
</li>
</ul>
<h3 id="一-核心概念"><a href="#一-核心概念" class="headerlink" title="一. 核心概念"></a>一. 核心概念</h3><p>1.1 Consumers 消费者</p>
<p>容错性的消费机制；比如下图的 C1, C2 在一个消费组里，C1出故障，C2继续消费<br><img src="/2018/07/18/kafka02/customergroup.png" alt="avatar"></p>
<p>一个组内，共享一个公共的消费group id ；<br>组内的所有消费者协调在一起 去消费指定的topic的所有分区；<br>每个分区只能由一个消费组的一个消费者来消费，绝不会出现一个分区<br>被一个消费组的多个消费者进行重复消费；</p>
<p>1.2 Segment 分段<br>一个partition被切割成多个相同大小的segment<br>命名规则: partition的全局的第一个segment必然是从0开始，后续的segment的名称为上一个<br>segment文件的最后一个消息的offset值来标识。<br>参数 log.segment.bytes=102400</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">-rw-rw-r-- 1 hadoop hadoop  184 Oct 18 14:24 00000000000000000000.index</span><br><span class="line">-rw-rw-r-- 1 hadoop hadoop 100K Oct 18 14:24 00000000000000000000.log  offset&#x3D;0</span><br><span class="line">-rw-rw-r-- 1 hadoop hadoop  300 Oct 18 14:24 00000000000000000000.timeindex</span><br><span class="line"></span><br><span class="line">-rw-rw-r-- 1 hadoop hadoop  10M Oct 18 14:25 00000000000000001997.index</span><br><span class="line">-rw-rw-r-- 1 hadoop hadoop  16K Oct 18 14:25 00000000000000001997.log offset&#x3D;1997+1&#x3D;1998</span><br><span class="line">-rw-rw-r-- 1 hadoop hadoop   10 Oct 18 14:24 00000000000000001997.snapshot</span><br><span class="line"></span><br><span class="line">-rw-rw-r-- 1 hadoop hadoop  10M Oct 18 14:25 00000000000000003330.index</span><br><span class="line">-rw-rw-r-- 1 hadoop hadoop  16K Oct 18 14:25 00000000000000003330.log offset&#x3D;3330+1&#x3D;3331</span><br><span class="line">-rw-rw-r-- 1 hadoop hadoop   10 Oct 18 14:24 00000000000000003330.snapshot</span><br></pre></td></tr></table></figure>


<p>1.3 看图说话</p>
<p>index文件:</p>
<ul>
<li>相对offset,分区的每个segment的log的唯一</li>
<li>物理地址，消息在log文件的物理地址  byte</li>
<li>稀疏表维护的，并不是每一条的消息的相对offset和物理地址都维护</li>
</ul>
<p>log文件:</p>
<ul>
<li>存储message</li>
</ul>
<p>offset偏移量:</p>
<ul>
<li>绝对offset 是从0开始，分区唯一的<br>  相对offset     1      分区的每个segment的log的唯一</li>
</ul>
<p>1.4 如何查找offset为2002的消息？（*****）</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">a.二分查找到&lt;&#x3D;2002的最大的segment段的文件 1997；</span><br><span class="line">b.2002 - 文件名称的offset 1997 &#x3D;5,相对offset；</span><br><span class="line">c.二分查找index文件的&lt;&#x3D;5的最大相对offset为3，对应的物理偏移量为90；</span><br><span class="line">d.在log文件，从90位置按顺序查找，直到找到绝对offset为2002的消息为m2002。</span><br></pre></td></tr></table></figure>


<h3 id="二-交付语义"><a href="#二-交付语义" class="headerlink" title="二. 交付语义"></a>二. 交付语义</h3><p><a href="http://kafka.apache.org/documentation/#semantics" target="_blank" rel="noopener">http://kafka.apache.org/documentation/#semantics</a></p>
<ul>
<li>At most once—Messages may be lost but are never redelivered. 0 1</li>
<li>At least once—Messages are never lost but may be redelivered. &gt;=1</li>
<li>Exactly once—this is what people actually want, each message is delivered once and only once. =1</li>
</ul>
<p>0.11 Exactly once 不是是指消费者 正确的是指的 producer端的保障</p>
<p>消费段的消费语义？<br>kafka+SS<br><a href="http://spark.apache.org/docs/latest/streaming-kafka-0-10-integration.html#storing-offsets" target="_blank" rel="noopener">http://spark.apache.org/docs/latest/streaming-kafka-0-10-integration.html#storing-offsets</a></p>
<p>checkpoints 缺点：<br>小文件<br>代码不能变更 </p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">Kafka itself 官方&amp;J哥            至少一次消费语义</span><br><span class="line">stream.foreachRDD &#123; rdd &#x3D;&gt;</span><br><span class="line">  	val offsetRanges &#x3D; rdd.asInstanceOf[HasOffsetRanges].offsetRanges</span><br><span class="line"></span><br><span class="line">  	&#x2F;&#x2F; some time later, after outputs have completed</span><br><span class="line">  	stream.asInstanceOf[CanCommitOffsets].commitAsync(offsetRanges)</span><br><span class="line"></span><br><span class="line">&#125;</span><br><span class="line">缺点：As with HasOffsetRanges, the cast to CanCommitOffsets will only succeed if called on the result of createDirectStream, not after transformations. The commitAsync call is threadsafe, but must occur after outputs if you want meaningful semantics.</span><br></pre></td></tr></table></figure>


<p>Your own data store 业务真正需要 精准一次消费语义</p>
<p>心得:</p>
<ul>
<li>a.交付语义的前半段无需关心 ，关键在于后半段的消费者的offset如何存储</li>
<li>b.我司<br>MySQL–》maxwell–》kafka–&gt;SS+Phoenix–》HBASE<br>至少一次消费语义+断批还原+巧妙的使用upsert语法  幂等性<br>c.假如非要选择外部存储，精准一次消费语义，很难做到代码级别的事务性<br>offset可能维护了，但是数据没有写进去<br>要不一起成功 要不一起失败<br>解决方案：数据+offset–》新的数据，一次性输出 </li>
</ul>
<h3 id="三-分区选择"><a href="#三-分区选择" class="headerlink" title="三. 分区选择"></a>三. 分区选择</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line">https:&#x2F;&#x2F;github.com&#x2F;apache&#x2F;kafka&#x2F;blob&#x2F;2.2.1&#x2F;clients&#x2F;src&#x2F;main&#x2F;java&#x2F;org&#x2F;apache&#x2F;kafka&#x2F;clients&#x2F;producer&#x2F;internals&#x2F;DefaultPartitioner.java</span><br><span class="line"></span><br><span class="line">&#x2F;**</span><br><span class="line">     * Compute the partition for the given record. 消息发送到哪个partition</span><br><span class="line">     *</span><br><span class="line">     * @param topic The topic name</span><br><span class="line">     * @param key The key to partition on (or null if no key)</span><br><span class="line">     * @param keyBytes serialized key to partition on (or null if no key)</span><br><span class="line">     * @param value The value to partition on or null</span><br><span class="line">     * @param valueBytes serialized value to partition on or null</span><br><span class="line">     * @param cluster The current cluster metadata</span><br><span class="line">     *&#x2F;</span><br><span class="line">    public int partition(String topic, Object key, byte[] keyBytes, Object value, byte[] valueBytes, Cluster cluster) &#123;</span><br><span class="line">        List&lt;PartitionInfo&gt; partitions &#x3D; cluster.partitionsForTopic(topic);</span><br><span class="line">        int numPartitions &#x3D; partitions.size();</span><br><span class="line">&#x2F;&#x2F; key不传值就走这个逻辑 </span><br><span class="line">        if (keyBytes &#x3D;&#x3D; null) &#123;</span><br><span class="line">            int nextValue &#x3D; nextValue(topic);  &#x2F;&#x2F; 自增长的方法</span><br><span class="line">            List&lt;PartitionInfo&gt; availablePartitions &#x3D; cluster.availablePartitionsForTopic(topic);</span><br><span class="line">            if (availablePartitions.size() &gt; 0) &#123;</span><br><span class="line">                int part &#x3D; Utils.toPositive(nextValue) % availablePartitions.size();</span><br><span class="line">                return availablePartitions.get(part).partition();</span><br><span class="line">            &#125; else &#123;</span><br><span class="line">                &#x2F;&#x2F; no partitions are available, give a non-available partition</span><br><span class="line">                return Utils.toPositive(nextValue) % numPartitions;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125; else &#123;  &#x2F;&#x2F; key传值就走 该逻辑</span><br><span class="line">            &#x2F;&#x2F; hash the keyBytes to choose a partition</span><br><span class="line">            return Utils.toPositive(Utils.murmur2(keyBytes)) % numPartitions;</span><br><span class="line">				100  % 3&#x3D;33。。。1   p1					</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br></pre></td></tr></table></figure>


<p> 生产上produer.send消息到kafka，指定key，根据key hash求出一个固定的分区</p>
<h3 id="四-全局有序"><a href="#四-全局有序" class="headerlink" title="四. 全局有序"></a>四. 全局有序</h3><p> topic  ruozedata  3个分区</p>
<p> insert u1 u2 u3 u4 delete 1–&gt;0</p>
<ul>
<li><p>p0：u1,u4</p>
</li>
<li><p>p1: i,u2</p>
</li>
<li><p>p2: u3,d</p>
<p>u3,d,i,u2,u1,u4  不光光是错误的 且1–》1</p>
<p>只设置一个分区</p>
<p>后端消费者做，分组时间排序 性能差</p>
<p>特征数据@若泽数据  特征数据 </p>
</li>
</ul>
<p>是不是指比如对于一个库/表的操作，指定一个特别的key<br>这个key hash之后只会发到特定的 partition，在这个partition内部是有序的</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">producer.send(new ProducerRecord&lt;&gt;(topic,messageNo,messageStr)).</span><br><span class="line"></span><br><span class="line">key&#x3D;ruozedata.emp.100</span><br><span class="line"></span><br><span class="line">hash(&quot;ruozedata.emp.100&quot;) 69 % 3&#x3D;23...0  p0</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"> p0：insert u1 u2 u3 u4 delete    </span><br><span class="line"> p1: </span><br><span class="line"> p2:</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    retries: 100</span><br><span class="line">    max.in.flight.requests.per.connection &#x3D; 1</span><br></pre></td></tr></table></figure>


<h3 id="五-调优"><a href="#五-调优" class="headerlink" title="五. 调优"></a>五. 调优</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br></pre></td><td class="code"><pre><span class="line">Producer: </span><br><span class="line">    acks: all</span><br><span class="line">    buffer.memory: 536870912</span><br><span class="line">    compression.type :snappy</span><br><span class="line">    retries: 100</span><br><span class="line">    max.in.flight.requests.per.connection &#x3D; 1  &#x2F;&#x2F; 每次请求只发送一个消息</span><br><span class="line"></span><br><span class="line">    batch.size: 10240字节 不是条数 </span><br><span class="line">    max.request.size &#x3D; 2097152</span><br><span class="line">    request.timeout.ms &#x3D; 360000    大于 replica.lag.time.max.ms </span><br><span class="line">    metadata.fetch.timeout.ms&#x3D; 360000</span><br><span class="line">    timeout.ms &#x3D; 360000</span><br><span class="line"></span><br><span class="line">    linger.ms 5s (生产不用)</span><br><span class="line">    max.block.ms 1800000  &#x2F;&#x2F; 阻塞时间，要调大</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">Broker: CDH</span><br><span class="line">    message.max.bytes 2560KB  1条消息的大小</span><br><span class="line">    zookeeper.session.timeout.ms 180000</span><br><span class="line">    replica.fetch.max.bytes 5M   大于message.max.bytes</span><br><span class="line">    num.replica.fetchers 6</span><br><span class="line">    replica.lag.max.messages 6000</span><br><span class="line">    replica.lag.time.max.ms 15000</span><br><span class="line"></span><br><span class="line">    log.flush.interval.messages 10000</span><br><span class="line">    log.flush.interval.ms 5s</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">Consumer:</span><br><span class="line">https:&#x2F;&#x2F;issues.apache.org&#x2F;jira&#x2F;browse&#x2F;SPARK-22968</span><br><span class="line">    , &quot;max.partition.fetch.bytes&quot; -&gt; (5242880: java.lang.Integer) &#x2F;&#x2F;default: 1048576</span><br><span class="line">    , &quot;request.timeout.ms&quot; -&gt; (90000: java.lang.Integer) &#x2F;&#x2F;default: 60000</span><br><span class="line">    , &quot;session.timeout.ms&quot; -&gt; (60000: java.lang.Integer) &#x2F;&#x2F;default: 30000</span><br><span class="line">    , &quot;heartbeat.interval.ms&quot; -&gt; (5000: java.lang.Integer)</span><br><span class="line">    , &quot;receive.buffer.bytes&quot; -&gt; (10485760: java.lang.Integer)</span><br></pre></td></tr></table></figure>



<p>注意:</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">Minor changes required for Kafka 0.10 and the new consumer compared to laughing_man&#39;s answer:</span><br><span class="line"></span><br><span class="line">Broker:   No changes, you still need to increase properties message.max.bytes </span><br><span class="line">          and replica.fetch.max.bytes. message.max.bytes has to be equal or smaller(*) than </span><br><span class="line">      replica.fetch.max.bytes.</span><br><span class="line">Producer: Increase max.request.size to send the larger message.</span><br><span class="line">Consumer: Increase max.partition.fetch.bytes to receive larger messages.</span><br><span class="line">(*) Read the comments to learn more about message.max.bytes&lt;&#x3D;replica.fetch.max.bytes</span><br></pre></td></tr></table></figure>



<h3 id="六-监控-CDH"><a href="#六-监控-CDH" class="headerlink" title="六. 监控 CDH"></a>六. 监控 CDH</h3><ol>
<li>读写趋势一致 </li>
<li>且在同一个时间点吻合，数据条数吻合，但是数据量为什么不吻合</li>
<li>写 123 3字节</li>
</ol>
<ul>
<li>fetched data： 加了额外信息的数据<br>  比如  读topic p offset time  123 比如 20字节</li>
<li>received data： 接受的原始数据</li>
</ul>
<p>完全吻合说明： 消费及时，没有延时，kafka无压力</p>
<p><img src="/2018/07/18/kafka02/jk2.png" alt="avatar"></p>
<p>fetched data数据比 receive data 大 receive data？<br>也就是说 fetched data 是有读topic p offset time 这些额外信息，而receive 没有？</p>
<p>kafka 生产者 消息个数监控</p>
<h3 id="七-故障案例二"><a href="#七-故障案例二" class="headerlink" title="七. 故障案例二"></a>七. 故障案例二</h3><p>7.1 磁盘暴了<br>遇见错误不可怕 冷静分析<br>预警真的很重要<br>EOF 错误标识，文件不完整，一般是文件损坏</p>
<p>7.2 断电<br>kafka启动 都是绿色状态，其实不然，生产者 消费者无法work，抛异常</p>
<ul>
<li>a.服务down </li>
<li>b.删除kafka 重新安装</li>
<li>c.重新刷数据  5点  3点开始重新刷数据。</li>
</ul>

      
      <!-- reward -->
      
    </div>
    
    
      <!-- copyright -->
      
    <footer class="article-footer">
      
      
  <ul class="article-tag-list" itemprop="keywords"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/tedata/" rel="tag">tedata</a></li></ul>


    </footer>

  </div>

  

  
  
  

  

</article>
    
    <article id="post-kafka01" class="article article-type-post" itemscope
  itemprop="blogPost" data-scroll-reveal>

  <div class="article-inner">
    
    <header class="article-header">
       
<h2 itemprop="name">
  <a class="article-title" href="/2018/07/09/kafka01/"
    >kafka 第一课</a
  >
</h2>
  

    </header>
    

    
    <div class="article-meta">
      <a href="/2018/07/09/kafka01/" class="article-date">
  <time datetime="2018-07-09T12:43:27.000Z" itemprop="datePublished">2018-07-09</time>
</a>
      
      
      
      
    </div>
    

    

    
    <div class="article-entry" itemprop="articleBody">
      


      

      
      <h1 id="kafka-01"><a href="#kafka-01" class="headerlink" title="kafka 01"></a>kafka 01</h1><p>学习目标：</p>
<ol>
<li>kafka 架构</li>
<li>kafka 存储结构</li>
<li>kafka  LEO , HW</li>
<li>kafka ISR</li>
<li>kafka 如何保证数据不丢失</li>
<li>kafka 选举机制</li>
<li>kafka 消费语义</li>
</ol>
<h3 id="一-kafka-架构"><a href="#一-kafka-架构" class="headerlink" title="一. kafka 架构"></a>一. kafka 架构</h3><p><img src="/2018/07/09/kafka01/kafkajg.png" alt="avatar"></p>
<p>kafka 架构有四个角色：</p>
<ul>
<li>producer：生产数据</li>
<li>consumer：消费数据</li>
<li>kafka broker：中间件，用于接收producer数据和给 consumer传递数据</li>
<li>zookeeper：用于 kafka leader的选举</li>
</ul>
<p>二. kafka 存储结构<br>kafka存储设计多个概念：</p>
<ul>
<li>topic</li>
<li>partition</li>
<li>segment</li>
<li>replication</li>
</ul>
<h5 id="topics"><a href="#topics" class="headerlink" title="topics"></a>topics</h5><p>是一类数据的集合，比如把订单数据作为一个topic，产品数据作为另外一个topic，两个topic之间互补干扰</p>
<h5 id="partition"><a href="#partition" class="headerlink" title="partition"></a>partition</h5><p>partition 是 topic的分区，有了partition，就可以把一个topic的数据分散到多台机器上，避免了因单台机器IO造成的数据吞吐量瓶颈</p>
<h5 id="segment"><a href="#segment" class="headerlink" title="segment"></a>segment</h5><p>为什么有了partition，还需要segment呢？因为partition如果只是一个单一文件，那么会造成一个问题：即partition这个文件会随着数据增大而变得越来越大，成为一个巨型文件，对于清理数据来说是很不方便的，所以把partition再按照固定大小分割成 segment文件，该文件形如 </p>
<p>000000000000xxxx.log</p>
<p>000000000000xxxx.index</p>
<p>即包含索引文件和数据文件，每个log文件的数字就是上一个文件的最后一个LEO大小</p>
<h5 id="如何根据segment查找消息？"><a href="#如何根据segment查找消息？" class="headerlink" title="如何根据segment查找消息？"></a>如何根据segment查找消息？</h5><p>查找消息为 2002的位置<br>流程</p>
<ul>
<li>首先通过绝对offset，找到&lt;=2002 最大的索引，比如 0000001997.index</li>
<li>在 0000001997.index内部，查找 2002-1997=5 的消息，通过二分查找，找到 &lt;=5 最大的索引，比如3</li>
<li>通过 000000.1997.index 对应到 0000001997.log文件里面去顺序查找3之后的信息，从而找到5对应的信息，就找到了 2002的信息</li>
</ul>
<h3 id="三-kafka-LEO-HW"><a href="#三-kafka-LEO-HW" class="headerlink" title="三. kafka LEO, HW"></a>三. kafka LEO, HW</h3><p>LEO: Log End Offset 即日志最后的偏移量的位置</p>
<p>HW: High WaterMarker，高水位线，实际上是消费者能够看到的kafka的最后的偏移量</p>
<h3 id="四-kafka-ISR"><a href="#四-kafka-ISR" class="headerlink" title="四. kafka ISR"></a>四. kafka ISR</h3><p>ISR: In-Sync Replicas 指的是副本同步队列</p>
<p>它的组成是 leader + 满足一定条件的followers<br>满足的条件是在规定的时间内，同步leader的数据，这个规定的<br>时间是由 kafka.lag.time.max.ms 指定的，如果超过这个时间<br>followers没有同步leader的数据，leader会把followers剔除</p>
<p>由此我们可以知道，kafka的同步策略既不是要所有节点都commit才算数据提交，也不是只要发给leader就算数据提交，它是数据可靠性和数据吞吐量的折衷，只要满足 ISR中的数据提交就算一次成功写入</p>
<h3 id="五-如何保证数据不丢失"><a href="#五-如何保证数据不丢失" class="headerlink" title="五. 如何保证数据不丢失"></a>五. 如何保证数据不丢失</h3><p>是不是 有 了 ISR就会保证数据不丢失呢？<br>当然不是，因为 ISR 中有可能出现只有leader一个节点存在的情况，<br>这个时候如果 leader挂了，就会造成数据丢失<br>所以除了 </p>
<p>==request.required.acks = -1== ，</p>
<p>还需要保证</p>
<p>==min.isr.replicas &gt;= 2== </p>
<p>即ISR 中的节点个数至少要为2个，这样数据要么提交成功，要么抛异常，不会出现数据丢失</p>
<h3 id="六-kafka的选举机制"><a href="#六-kafka的选举机制" class="headerlink" title="六. kafka的选举机制"></a>六. kafka的选举机制</h3><p>kafka 的选举机制不同于 zookeeper的 “少数服从多数” 的机制<br>即对于 2n + 1 个节点，最多容忍n个节点挂，至少保证n+1 个节点才能<br>选举出leader<br>kafka 不是这样的，它n+1 个节点，可以容忍n个节点的挂</p>
<p>kafka的选举参数 unclean.leader.election.enable<br>当为false 时，表示必须是 ISR 里的节点才能被选为leader<br>当为true时，表示只要时AR里的节点就可以被选为leader<br>这就需要在数据可靠性和可用性之间进行平衡，<br>如果设为 false，那么数据是可靠的，但是如果ISR里的服务全部宕了<br>长时间不恢复，那么将忍受长时间的对外服务中断<br>如果设为true，恢复的时间将比上面要快，但有可能造成数据丢失</p>

      
      <!-- reward -->
      
    </div>
    
    
      <!-- copyright -->
      
    <footer class="article-footer">
      
      
  <ul class="article-tag-list" itemprop="keywords"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/tedata/" rel="tag">tedata</a></li></ul>


    </footer>

  </div>

  

  
  
  

  

</article>
    
    <article id="post-Spark 优化之加速启动" class="article article-type-post" itemscope
  itemprop="blogPost" data-scroll-reveal>

  <div class="article-inner">
    
    <header class="article-header">
       
<h2 itemprop="name">
  <a class="article-title" href="/2018/03/09/Spark%20%E4%BC%98%E5%8C%96%E4%B9%8B%E5%8A%A0%E9%80%9F%E5%90%AF%E5%8A%A8/"
    >Spark 优化之加速启动</a
  >
</h2>
  

    </header>
    

    
    <div class="article-meta">
      <a href="/2018/03/09/Spark%20%E4%BC%98%E5%8C%96%E4%B9%8B%E5%8A%A0%E9%80%9F%E5%90%AF%E5%8A%A8/" class="article-date">
  <time datetime="2018-03-09T04:00:00.000Z" itemprop="datePublished">2018-03-09</time>
</a>
      
      
      
      
    </div>
    

    

    
    <div class="article-entry" itemprop="articleBody">
      


      

      
      <h1 id="Spark-优化之加速启动"><a href="#Spark-优化之加速启动" class="headerlink" title="Spark 优化之加速启动"></a>Spark 优化之加速启动</h1><p>一. 问题背景</p>
<p>Spark on YARN 每次启动时会将本地的 spark jar 和 conf 上传到 HDFS，这样会消耗很长的时间</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">[hadoop@danner000 jars]$  spark-submit --class org.apache.spark.examples.SparkPi --master yarn --deploy-mode cluster spark-examples_2.11-2.4.4.jar 3</span><br><span class="line">...</span><br><span class="line">&#96;19&#x2F;10&#x2F;18 13:58:26 WARN yarn.Client: Neither spark.yarn.jars nor spark.yarn.archive is set, falling back to uploading libraries under SPARK_HOME.&#96;</span><br><span class="line">&#96;19&#x2F;10&#x2F;18 13:58:30 INFO yarn.Client: Uploading resource file:&#x2F;tmp&#x2F;spark-294ab9b7-97ff-4ffa-8e4f-ae44a89dd5da&#x2F;__spark_libs__1410305138065236635.zip -&gt; hdfs:&#x2F;&#x2F;192.168.22.147:9000&#x2F;user&#x2F;hadoop&#x2F;.sparkStaging&#x2F;application_1571146456067_0024&#x2F;__spark_libs__1410305138065236635.zip&#96;</span><br><span class="line">19&#x2F;10&#x2F;18 13:58:44 INFO yarn.Client: Uploading resource file:&#x2F;home&#x2F;hadoop&#x2F;app&#x2F;spark-2.4.4-bin-2.6.0-cdh5.15.1&#x2F;examples&#x2F;jars&#x2F;spark-examples_2.11-2.4.4.jar -&gt; hdfs:&#x2F;&#x2F;192.168.22.147:9000&#x2F;user&#x2F;hadoop&#x2F;.sparkStaging&#x2F;application_1571146456067_0024&#x2F;spark-examples_2.11-2.4.4.jar</span><br><span class="line">19&#x2F;10&#x2F;18 13:58:45 INFO yarn.Client: Uploading resource file:&#x2F;tmp&#x2F;spark-294ab9b7-97ff-4ffa-8e4f-ae44a89dd5da&#x2F;__spark_conf__5888474803491307773.zip -&gt; hdfs:&#x2F;&#x2F;192.168.22.147:9000&#x2F;user&#x2F;hadoop&#x2F;.sparkStaging&#x2F;application_1571146456067_0024&#x2F;__spark_conf__.zip</span><br><span class="line">...</span><br></pre></td></tr></table></figure>

<p>查看上面日志，是由于没有设置 ==spark.yarn.archive== 或 ==spark.yarn.jars==，所以每次启动的时候都会上传libs</p>
<h3 id="二-优化过程"><a href="#二-优化过程" class="headerlink" title="二. 优化过程"></a>二. 优化过程</h3><p>既然知道是哪个属性的原因，那我们就从源码里看看如何设置</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br></pre></td><td class="code"><pre><span class="line">&#x2F;&#x2F; org.apache.spark.deploy.yarn.Client private def createContainerLaunchContext &#123;</span><br><span class="line">    ... </span><br><span class="line">    val appStagingDirPath &#x3D; new Path(appStagingBaseDir, getAppStagingDir(appId))</span><br><span class="line">    ...</span><br><span class="line">    val localResources &#x3D; prepareLocalResources(appStagingDirPath, pySparkArchives)</span><br><span class="line">    ...</span><br><span class="line">&#125;</span><br><span class="line">&#x2F;&#x2F; org.apache.spark.deploy.yarn.Client def prepareLocalResources &#123;</span><br><span class="line">    ...</span><br><span class="line">    </span><br><span class="line">    def distribute &#123;</span><br><span class="line">      val trimmedPath &#x3D; path.trim()</span><br><span class="line">      val localURI &#x3D; Utils.resolveURI(trimmedPath)</span><br><span class="line">      if (localURI.getScheme !&#x3D; LOCAL_SCHEME) &#123;</span><br><span class="line">        if (addDistributedUri(localURI)) &#123;</span><br><span class="line">          val localPath &#x3D; getQualifiedLocalPath(localURI, hadoopConf)</span><br><span class="line">          val linkname &#x3D; targetDir.map(_ + &quot;&#x2F;&quot;).getOrElse(&quot;&quot;) +</span><br><span class="line">           destName.orElse(Option(localURI.getFragment())).getOrElse(localPath.getName())</span><br><span class="line">          val destPath &#x3D; copyFileToRemote(destDir, localPath, replication, symlinkCache)</span><br><span class="line">          val destFs &#x3D; FileSystem.get(destPath.toUri(), hadoopConf)</span><br><span class="line">          distCacheMgr.addResource(</span><br><span class="line">            destFs, hadoopConf, destPath, localResources, resType, linkname, statCache,</span><br><span class="line">            appMasterOnly &#x3D; appMasterOnly)</span><br><span class="line">          (false, linkname)</span><br><span class="line">        &#125; else &#123;</span><br><span class="line">          (false, null)</span><br><span class="line">        &#125;</span><br><span class="line">      &#125; else &#123;</span><br><span class="line">        (true, trimmedPath)</span><br><span class="line">      &#125;</span><br><span class="line">    &#125;</span><br><span class="line">    ...</span><br><span class="line">    &#x2F;** * Add Spark to the cache. There are two settings that control what files to add to the cache: * - if a Spark archive is defined, use the archive. The archive is expected to contain * jar files at its root directory. * - if a list of jars is provided, filter the non-local ones, resolve globs, and * add the found files to the cache. * * Note that the archive cannot be a &quot;local&quot; URI. If none of the above settings are found, * then upload all files found in $SPARK_HOME&#x2F;jars. *&#x2F;</span><br><span class="line">    val sparkArchive &#x3D; sparkConf.get(SPARK_ARCHIVE)</span><br><span class="line">    if (sparkArchive.isDefined) &#123;</span><br><span class="line">      val archive &#x3D; sparkArchive.get</span><br><span class="line">      require(!isLocalUri(archive), s&quot;$&#123;SPARK_ARCHIVE.key&#125; cannot be a local URI.&quot;)</span><br><span class="line">      distribute(Utils.resolveURI(archive).toString,</span><br><span class="line">        resType &#x3D; LocalResourceType.ARCHIVE,</span><br><span class="line">        destName &#x3D; Some(LOCALIZED_LIB_DIR))</span><br><span class="line">    &#125;else &#123;</span><br><span class="line">      sparkConf.get(SPARK_JARS) match &#123;</span><br><span class="line">      	case Some(jars) &#x3D;&gt;&#123;</span><br><span class="line">            &#x2F;&#x2F; 操作类似 SPARK_ARCHIVE，把 SPARK_JARS 上传；两者设置一个即可             ... </span><br><span class="line">        &#125;</span><br><span class="line">       case None &#x3D;&gt;</span><br><span class="line">          &#x2F;&#x2F; No configuration, so fall back to uploading local jar files.           logWarning(s&quot;Neither $&#123;SPARK_JARS.key&#125; nor $&#123;SPARK_ARCHIVE.key&#125; is set, falling back &quot; + &quot;to uploading libraries under SPARK_HOME.&quot;)</span><br><span class="line">          &#x2F;&#x2F; 把 spark&#x2F;jars 所有 jar 上传       &#125;</span><br><span class="line">       </span><br><span class="line">    ...</span><br><span class="line">&#125;</span><br><span class="line"> private[yarn] def copyFileToRemote &#123;</span><br><span class="line">        val destFs &#x3D; destDir.getFileSystem(hadoopConf)</span><br><span class="line">        val srcFs &#x3D; srcPath.getFileSystem(hadoopConf)</span><br><span class="line">        var destPath &#x3D; srcPath</span><br><span class="line">        if (force || !compareFs(srcFs, destFs) || &quot;file&quot;.equals(srcFs.getScheme)) &#123;</span><br><span class="line">            destPath &#x3D; new Path(destDir, destName.getOrElse(srcPath.getName()))</span><br><span class="line">            logInfo(s&quot;Uploading resource $srcPath -&gt; $destPath&quot;)</span><br><span class="line">            FileUtil.copy(srcFs, srcPath, destFs, destPath, false, hadoopConf)</span><br><span class="line">            destFs.setReplication(destPath, replication)</span><br><span class="line">            destFs.setPermission(destPath, new FsPermission(APP_FILE_PERMISSION))</span><br><span class="line">        &#125; else &#123;</span><br><span class="line">            logInfo(s&quot;Source and destination file systems are the same. Not copying $srcPath&quot;)</span><br><span class="line">        &#125;</span><br><span class="line">        ...</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>设置 SPARK_ARCHIVE 后，将 SPARK_ARCHIVE 目录分发 (distribute)<br>distribute 中判断是否为本地文件和是否已上传</p>
<p>copyFileToRemote 判断原文件和目标文件是否为同个文件系统，若相同则不上传</p>
<p>destPath 在 createContainerLaunchContext 函数被赋值 appStagingDirPath，根据 hadoop job 执行流程 可知 StagingDir 是 Yarn job 为执行任务存放文件而临时创建的目录；在本案例中就是 HDFS 目录<br>由以上分析可知，只需将 SPARK_ARCHIVE 设置为 hdfs 目录就可以避免每次上传的困扰。</p>
<p>将 spark/jars/*.jar 打包成 zip 并上传到 HDFS</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">spark-submit --class org.apache.spark.examples.SparkPi --master yarn --deploy-mode cluster &#96;--conf spark.yarn.archive&#x3D;hdfs:&#x2F;&#x2F;192.168.22.147:9000&#x2F;lib&#x2F;dep&#x2F;spark&#x2F;spark_jar.zip&#96; spark-examples_2.11-2.4.4.jar 3</span><br><span class="line">...</span><br></pre></td></tr></table></figure>


<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">19&#x2F;10&#x2F;18 12:39:16 INFO yarn.Client: Preparing resources for our AM container</span><br><span class="line">19&#x2F;10&#x2F;18 12:39:16 INFO yarn.Client: &#96;Source and destination file systems are the same. Not copying hdfs:&#x2F;&#x2F;192.168.22.147:9000&#x2F;lib&#x2F;dep&#x2F;spark&#x2F;spark_jar.tar&#96;</span><br><span class="line">...</span><br></pre></td></tr></table></figure>

<p>spark.yarn.archive 也可以设置在 spark-defaults.conf ;</p>
<p>spark.yarn.jars 相同操作，两者等效</p>
<p>Conf<br>看日志可知，每次启动也都上传，它会上传 SPARK_CONF_DIR 和 HADOOP_CONF_DIR目录下的文件。但此 Conf 无法优化，因为就是算是源文件和目标文件在同个文件系统，也会强制复制</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">&#x2F;&#x2F; This code forces the archive to be copied, so that unit tests pass (since in that case both     &#x2F;&#x2F; file systems are the same and the archive wouldn&#39;t normally be copied). In most (all?)     &#x2F;&#x2F; deployments, the archive would be copied anyway, since it&#39;s a temp file in the local file     &#x2F;&#x2F; system.     val remoteConfArchivePath &#x3D; new Path(destDir, LOCALIZED_CONF_ARCHIVE)</span><br><span class="line">    val remoteFs &#x3D; FileSystem.get(remoteConfArchivePath.toUri(), hadoopConf)</span><br><span class="line">    sparkConf.set(CACHED_CONF_ARCHIVE, remoteConfArchivePath.toString())</span><br><span class="line"></span><br><span class="line">    val localConfArchive &#x3D; new Path(createConfArchive().toURI())</span><br><span class="line">    copyFileToRemote(destDir, localConfArchive, replication, symlinkCache, force &#x3D; true,</span><br><span class="line">      destName &#x3D; Some(LOCALIZED_CONF_ARCHIVE))</span><br></pre></td></tr></table></figure>

      
      <!-- reward -->
      
    </div>
    
    
      <!-- copyright -->
      
    <footer class="article-footer">
      
      
  <ul class="article-tag-list" itemprop="keywords"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/tedata/" rel="tag">tedata</a></li></ul>


    </footer>

  </div>

  

  
  
  

  

</article>
    
    <article id="post-HBase02" class="article article-type-post" itemscope
  itemprop="blogPost" data-scroll-reveal>

  <div class="article-inner">
    
    <header class="article-header">
       
<h2 itemprop="name">
  <a class="article-title" href="/2018/02/10/HBase02/"
    >HBase Flush,合并及Rowkey设计</a
  >
</h2>
  

    </header>
    

    
    <div class="article-meta">
      <a href="/2018/02/10/HBase02/" class="article-date">
  <time datetime="2018-02-10T12:43:27.000Z" itemprop="datePublished">2018-02-10</time>
</a>
      
      
      
      
    </div>
    

    

    
    <div class="article-entry" itemprop="articleBody">
      


      

      
      <h1 id="HBase02"><a href="#HBase02" class="headerlink" title="HBase02"></a>HBase02</h1><p>学习目标：</p>
<ol>
<li>HBase memstore flush 调优</li>
<li>HBase compaction </li>
<li>Rowkey 设计</li>
</ol>
<h3 id="一-HBase-memstore-flush-调优"><a href="#一-HBase-memstore-flush-调优" class="headerlink" title="一. HBase memstore flush 调优"></a>一. HBase memstore flush 调优</h3><ol>
<li>memstore 级别调优</li>
</ol>
<p>habse.region.memstore.flush.size 默认是 128M，生产上建议调大<br>我们的经验值是 512M</p>
<p>这个值表示 某一个 memstore 的值超过 512M，就会进行 flush到storefile</p>
<ol start="2">
<li>region级别调优</li>
</ol>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">hbase.region.memstore.flush.size</span><br><span class="line">hbase.region.memstore.multipiler</span><br></pre></td></tr></table></figure>

<p>这个参数表示 一个 region 中的 所有 memstore 之和加起来超过</p>
<p>hbase.region.memstore.flush.size * hbase.region.memstore.multipiler </p>
<p>就会触发 flush，这个参数是为了防止 有很多个 memstore，但是每个memstore都没有查过 hbase.region.memstore.flush.size<br>但是生产上一般不会出现这种情况，因为一般 CF 不会超过3个，memstore数量也就小于3，不会产生上面情况</p>
<ol start="3">
<li>hregionserver 级别调优</li>
</ol>
<p>这个级别产生 flush，就是灾难的，因为 hregionserver 级别，它里面包含很多的 region，会影响其他表的工作<br>它受以下几个参数影响：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">Java Heap Size Of HBase：默认 50M</span><br><span class="line">hbase.regionserver.upperLimit:  默认 0.4</span><br><span class="line">hbase.regionserver.lowerLimit:  默认 0.95</span><br></pre></td></tr></table></figure>
<p><img src="/2018/02/10/HBase02/h1.png" alt="avatar"></p>
<p>如图，regionserver 规定了一个flush的上界 Upper和下界Lower，Upper 值为 Java Heap Size * 0.4， Lower 值为 Java Heap Size * 0.4 * 0.95<br>比如 Java Heap Size 为 1000M，<br>那么 Upper 为 400M，Lower 为 390M<br>如果 memstore 值 达到 390M，就会触发flush，如果继续增大到 400M，也会进行 flush，直到 memstore值小于 390M 以下才ok</p>
<p>生产上，一般会调整 Java Heap Size ，如果资源充足，建议调整到 32G，不要超过这个值，否则会出现指针压缩失效问题<br>hbase.regionserver.upperLimit 不用作调整<br>hbase.regionserver.lowerLimit 可以在 0.9 - 0.95 之间调整，小于0.9 会报错</p>
<h3 id="二-HBase-compaction"><a href="#二-HBase-compaction" class="headerlink" title="二. HBase compaction"></a>二. HBase compaction</h3><ol>
<li>什么是大合并和小合并</li>
</ol>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">minor compaction</span><br><span class="line">major compaction</span><br></pre></td></tr></table></figure>


<p>如图，相邻storefile 之间的合并称为小合并，小合并之后形成的大的Storefile 之间的合并成为大合并<br>大合并一般包括：<br>TTL 删除<br>put 多版本 合并<br>delete 操作</p>
<p><img src="/2018/02/10/HBase02/h2.png" alt="avatar"></p>
<ol start="2">
<li>何时触发合并</li>
</ol>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">minor compaction</span><br><span class="line">hbase.hstore.compaction.min：触发合并的最小值 </span><br><span class="line">hbase.hstore.compaction.min.size : 小于该值的storefile 一定会加到 合并队列中</span><br><span class="line">hbase.hstore.compaction.max：触发合并的最大值</span><br><span class="line">hbase.hstore.compaction.max.size ： 大于该值的storefile</span><br></pre></td></tr></table></figure>
<p> 一定会被合并队列排除，但这个最大值为 Longvalue.max ，一般不会达到</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">major compaction</span><br><span class="line">hbase.regionserver.majorcompaction  7</span><br><span class="line">hbase.regionserver.majorcompaction.jitter 0.5</span><br></pre></td></tr></table></figure>

<p>那么触发 大合并的时间区间为 [7 - 7 * 0.5 -  7 + 7  * 0.5]<br>生产上禁止此参数</p>
<p>定期检查</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hbase.server.thread.wakefrequency * hbase.server.compactchecker.interval.multipiler &#x3D; 10000ms * 1000 &#x3D; 10000s &#x3D; 2h 46m 40s</span><br></pre></td></tr></table></figure>

<p>每隔2小时46分40秒检查是否需要compaction</p>
<p>手动触发</p>
<p>执行 major_compact ‘ns1:t1’</p>
<h3 id="三-Rowkey-设计"><a href="#三-Rowkey-设计" class="headerlink" title="三. Rowkey 设计"></a>三. Rowkey 设计</h3><ol>
<li>“加盐”</li>
<li>即 newRK = 随机数 + RK<br>比如 原来的RK 为  1001，1002，1003，1004<br>现在变为 </li>
</ol>
<p>a-1001</p>
<p>b-1002</p>
<p>c-1003</p>
<p>d-1004</p>
<p>在建表的时候</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">create &#39;t1&#39;, &#39;f1&#39;, SPLITS &#x3D;&gt; [&#39;a&#39;, &#39;b&#39;, &#39;c&#39;, &#39;d&#39;]，</span><br></pre></td></tr></table></figure>
<p> 那么就会建一个表，包含</p>
<p>null, a</p>
<p>a,b</p>
<p>b,c</p>
<p>c,d</p>
<p>d,null </p>
<p>这样的表，这样设计的好处<br>原来容易产生热点问题的RK，现在形成了良好的随机性，可以分散到不同的节点上</p>
<p>这样设计的缺点：<br>a. 第一个分区，此处是null，a 这个分区没有值落在上面，造成了分区的浪费<br>b. 对于同一个RK，比如 1001，因为加前缀，很可能造成落在不同的regionserver，在查询时候要拉取各个不同regionserver上的数据，开销大</p>
<p>但是如何 配合 phoinex + 二级索引，可以解决上面的问题</p>
<ol start="2">
<li>hash</li>
</ol>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">RK 设计是 newRK &#x3D; hash(RK) + RK</span><br></pre></td></tr></table></figure>


<p>建表语句</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">create &#39;t1&#39;, &#39;f1&#39;, &#123;NUMREGIONS &#x3D;&gt; 15, SPLITALGO &#x3D;&gt; &#39;HexStringSplit&#39;&#125;</span><br></pre></td></tr></table></figure>

<p>其中 15表示分区的数量</p>
<p>这里相比于 盐表，因为 对于相同的RK，其Hash值是确定相同的，这样，对于同一个RK，<br>Hash(RK) + RK 的设计，其好处是：<br>就能保证同一个RK 能落在同一个 regionserver上，对于同RK 的查询<br>就不存在盐表中的多regionserver 查询问题，同时，也不会因为 随机值带来的第一个分区不能利用的问题<br>缺点是:<br>考虑一钟场景：Hash(1001) + 1001, Hash(1002) + 1002, Hash(1003) + 1003， 这样的数据很可能是落在<br>不同的server上的，如果想要查询它们，作聚合等操作，实际上，这种操作时分常常见的，那么就会产生<br>去不同server拉取数据的情况，造成很大的开销</p>
<ol start="3">
<li>反转<br>反转设计指的是把key值进行倒序设计<br>比如</li>
</ol>
<p>13812345670</p>
<p>13812345671</p>
<p>13812345672</p>
<p>13812345673</p>
<p>13812345674</p>
<p>13812345675</p>
<p>13812345676</p>
<p>13812345677</p>
<p>13812345678</p>
<p>13812345679</p>
<p>这样，反转后，就变成了0-9 开头的RK ，建表时，就可以按 数字进行split<br>create ‘t1’, ‘f1’, SPLITS =&gt; [‘0’,’1’, ‘2’,’3’,’4’,’5’,’6’,’7’,’8’,’9’]<br>形成了和盐表一样的设计<br>这种设计需要RK 在尾部形成良好的随机性</p>

      
      <!-- reward -->
      
    </div>
    
    
      <!-- copyright -->
      
    <footer class="article-footer">
      
      
  <ul class="article-tag-list" itemprop="keywords"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/tedata/" rel="tag">tedata</a></li></ul>


    </footer>

  </div>

  

  
  
  

  

</article>
    
  </article>
  

  
  <nav class="page-nav">
    
    <span class="page-number current">1</span><a class="page-number" href="/page/2/">2</a><a class="page-number" href="/page/3/">3</a><a class="extend next" rel="next" href="/page/2/">下一页</a>
  </nav>
  
</section>
</div>

      <footer class="footer">
  <div class="outer">
    <ul class="list-inline">
      <li>
        &copy;
        2015-2020
        tedwang0714
      </li>
      <li>
        
          Powered by
        
        
        <a href="https://hexo.io" target="_blank">Hexo</a> Theme <a href="https://github.com/Shen-Yu/hexo-theme-ayer" target="_blank">Ayer</a>
        
      </li>
    </ul>
    <ul class="list-inline">
      <li>
        
        
        <span>
  <i>PV:<span id="busuanzi_value_page_pv"></span></i>
  <i>UV:<span id="busuanzi_value_site_uv"></span></i>
</span>
        
      </li>
      <li>
        <!-- cnzz统计 -->
        
        <script type="text/javascript" src='https://s9.cnzz.com/z_stat.php?id=1278069914&amp;web_id=1278069914'></script>
        
      </li>
    </ul>
  </div>
</footer>
    <div class="to_top">
        <div class="totop" id="totop">
  <i class="ri-arrow-up-line"></i>
</div>
      </div>
    </main>
      <aside class="sidebar">
        <button class="navbar-toggle"></button>
<nav class="navbar">
  
  <div class="logo">
    <a href="/"><img src="/images/xingji.png" alt="TedWang 的大数据"></a>
  </div>
  
  <ul class="nav nav-main">
    
    <li class="nav-item">
      <a class="nav-item-link" href="/index.html">主页</a>
    </li>
    
    <li class="nav-item">
      <a class="nav-item-link" href="/archives">归档</a>
    </li>
    
    <li class="nav-item">
      <a class="nav-item-link" href="/categories">分类</a>
    </li>
    
    <li class="nav-item">
      <a class="nav-item-link" href="/tags/">标签</a>
    </li>
    
  </ul>
</nav>
<nav class="navbar navbar-bottom">
  <ul class="nav">
    <li class="nav-item">
      
      <a class="nav-item-link nav-item-search"  title="Search">
        <i class="ri-search-line"></i>
      </a>
      
      
      <a class="nav-item-link" target="_blank" href="/atom.xml" title="RSS Feed">
        <i class="ri-rss-line"></i>
      </a>
      
    </li>
  </ul>
</nav>
<div class="search-form-wrap">
  <div class="local-search local-search-plugin">
  <input type="search" id="local-search-input" class="local-search-input" placeholder="Search...">
  <div id="local-search-result" class="local-search-result"></div>
</div>
</div>
      </aside>
      <div id="mask"></div>

<!-- #reward -->
<div id="reward">
  <span class="close"><i class="ri-close-line"></i></span>
  <p class="reward-p"><i class="ri-cup-line"></i>请我喝杯咖啡吧~</p>
  <div class="reward-box">
    
    <div class="reward-item">
      <img class="reward-img" src="/images/alipay.jpg">
      <span class="reward-type">支付宝</span>
    </div>
    
    
    <div class="reward-item">
      <img class="reward-img" src="/images/wechat.jpg">
      <span class="reward-type">微信</span>
    </div>
    
  </div>
</div>
      
<script src="/js/jquery-2.0.3.min.js"></script>


<script src="/js/jquery.justifiedGallery.min.js"></script>


<script src="/js/lazyload.min.js"></script>


<script src="/js/busuanzi-2.3.pure.min.js"></script>


<script src="/js/share.js"></script>



<script src="/fancybox/jquery.fancybox.min.js"></script>




<script>
  try {
    var typed = new Typed("#subtitle", {
    strings: ['人类精神必须置于技术之上','',''],
    startDelay: 0,
    typeSpeed: 200,
    loop: true,
    backSpeed: 100,
    showCursor: true
    });
  } catch (err) {
  }
  
</script>




<script>
  var ayerConfig = {
    mathjax: false
  }
</script>


<script src="/js/ayer.js"></script>


<script src="https://cdn.jsdelivr.net/npm/jquery-modal@0.9.2/jquery.modal.min.js"></script>
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/jquery-modal@0.9.2/jquery.modal.min.css">


<!-- Root element of PhotoSwipe. Must have class pswp. -->
<div class="pswp" tabindex="-1" role="dialog" aria-hidden="true">

    <!-- Background of PhotoSwipe. 
         It's a separate element as animating opacity is faster than rgba(). -->
    <div class="pswp__bg"></div>

    <!-- Slides wrapper with overflow:hidden. -->
    <div class="pswp__scroll-wrap">

        <!-- Container that holds slides. 
            PhotoSwipe keeps only 3 of them in the DOM to save memory.
            Don't modify these 3 pswp__item elements, data is added later on. -->
        <div class="pswp__container">
            <div class="pswp__item"></div>
            <div class="pswp__item"></div>
            <div class="pswp__item"></div>
        </div>

        <!-- Default (PhotoSwipeUI_Default) interface on top of sliding area. Can be changed. -->
        <div class="pswp__ui pswp__ui--hidden">

            <div class="pswp__top-bar">

                <!--  Controls are self-explanatory. Order can be changed. -->

                <div class="pswp__counter"></div>

                <button class="pswp__button pswp__button--close" title="Close (Esc)"></button>

                <button class="pswp__button pswp__button--share" style="display:none" title="Share"></button>

                <button class="pswp__button pswp__button--fs" title="Toggle fullscreen"></button>

                <button class="pswp__button pswp__button--zoom" title="Zoom in/out"></button>

                <!-- Preloader demo http://codepen.io/dimsemenov/pen/yyBWoR -->
                <!-- element will get class pswp__preloader--active when preloader is running -->
                <div class="pswp__preloader">
                    <div class="pswp__preloader__icn">
                        <div class="pswp__preloader__cut">
                            <div class="pswp__preloader__donut"></div>
                        </div>
                    </div>
                </div>
            </div>

            <div class="pswp__share-modal pswp__share-modal--hidden pswp__single-tap">
                <div class="pswp__share-tooltip"></div>
            </div>

            <button class="pswp__button pswp__button--arrow--left" title="Previous (arrow left)">
            </button>

            <button class="pswp__button pswp__button--arrow--right" title="Next (arrow right)">
            </button>

            <div class="pswp__caption">
                <div class="pswp__caption__center"></div>
            </div>

        </div>

    </div>

</div>

<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/photoswipe@4.1.3/dist/photoswipe.min.css">
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/photoswipe@4.1.3/dist/default-skin/default-skin.css">
<script src="https://cdn.jsdelivr.net/npm/photoswipe@4.1.3/dist/photoswipe.min.js"></script>
<script src="https://cdn.jsdelivr.net/npm/photoswipe@4.1.3/dist/photoswipe-ui-default.min.js"></script>

<script>
    function viewer_init() {
        let pswpElement = document.querySelectorAll('.pswp')[0];
        let $imgArr = document.querySelectorAll(('.article-entry img:not(.reward-img)'))

        $imgArr.forEach(($em, i) => {
            $em.onclick = () => {
                // slider展开状态
                // todo: 这样不好，后面改成状态
                if (document.querySelector('.left-col.show')) return
                let items = []
                $imgArr.forEach(($em2, i2) => {
                    let img = $em2.getAttribute('data-idx', i2)
                    let src = $em2.getAttribute('data-target') || $em2.getAttribute('src')
                    let title = $em2.getAttribute('alt')
                    // 获得原图尺寸
                    const image = new Image()
                    image.src = src
                    items.push({
                        src: src,
                        w: image.width || $em2.width,
                        h: image.height || $em2.height,
                        title: title
                    })
                })
                var gallery = new PhotoSwipe(pswpElement, PhotoSwipeUI_Default, items, {
                    index: parseInt(i)
                });
                gallery.init()
            }
        })
    }
    viewer_init()
</script>




<script type="text/javascript" src="https://js.users.51.la/20544303.js"></script>
  </div>
</body>

</html>