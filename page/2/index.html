<!DOCTYPE html>
<html lang="en">

<head>
  <meta charset="utf-8" />
    
  <meta name="description" content="大数据学习" />
  
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1" />
  <title>
     TedWang 的大数据
  </title>
  <meta name="generator" content="hexo-theme-yilia-plus">
  
  <link rel="shortcut icon" href="/images/xingji.png" />
  
  
<link rel="stylesheet" href="/css/style.css">

  
<script src="/js/pace.min.js"></script>


  

  

</head>

</html>

<body>
  <div id="app">
    <main class="content">
      
<section class="cover">
    
  <div class="cover-frame">
    <div class="bg-box">
      <img src="/images/xk.jpg" alt="image frame" />
    </div>
    <div class="cover-inner text-center text-white">
      <h1><a href="/">TedWang 的大数据</a></h1>
      <div id="subtitle-box">
        
        <span id="subtitle"></span>
        
      </div>
      <div>
        
      </div>
    </div>
  </div>
  <div class="cover-learn-more">
    <a href="javascript:void(0)" class="anchor"><i class="ri-arrow-down-line"></i></a>
  </div>
</section>



<script src="https://cdn.jsdelivr.net/npm/typed.js@2.0.11/lib/typed.min.js"></script>

<div id="main">
  <section class="outer">
  <article class="articles">
    
    
    
    
    <article id="post-HBase01" class="article article-type-post" itemscope
  itemprop="blogPost" data-scroll-reveal>

  <div class="article-inner">
    
    <header class="article-header">
       
<h2 itemprop="name">
  <a class="article-title" href="/2018/01/24/HBase01/"
    >HBase 第一课</a
  >
</h2>
  

    </header>
    

    
    <div class="article-meta">
      <a href="/2018/01/24/HBase01/" class="article-date">
  <time datetime="2018-01-24T12:43:27.000Z" itemprop="datePublished">2018-01-24</time>
</a>
      
      
      
      
    </div>
    

    

    
    <div class="article-entry" itemprop="articleBody">
      


      

      
      <h1 id="HBase01"><a href="#HBase01" class="headerlink" title="HBase01"></a>HBase01</h1><p>学习目标：</p>
<ol>
<li>hbase 架构</li>
<li>hbase 写流程</li>
<li>hbase 读流程</li>
<li>hbase web页面</li>
</ol>
<h3 id="一-hbase架构"><a href="#一-hbase架构" class="headerlink" title="一. hbase架构"></a>一. hbase架构</h3><ol>
<li>hbase 逻辑图<br><img src="/2018/01/24/HBase01/RegionLogic.png" alt="avatar"><br><img src="/2018/01/24/HBase01/datalogic.png" alt="avatar"></li>
</ol>
<p>一个表包含多个region，如果不指定 splitKey，则默认是一个region，当数据达到一定量时，开始分裂成多个region</p>
<p>一个region包含 一个或多个ColumnFamily，即列族，它是一些有相近概念的字段的集合，比如 SKU族， Order族，一般列族不超过3个</p>
<p>一个列族对应一个store，一个store包含 一个 memstore和0到多个storefile<br>数据写是先写到memstore，如何超过一定数据量，就flush到磁盘写道storefile中</p>
<ol start="2">
<li>HBase 的数据模型<br>我们以 下图为例，它有两个 CF, 即 SKU 和 Order<br>左边 五列实际上是一个 store，右边 5列是另一个 store<br>row1的数据就是 SKUName，SKUNum，SKUSum，因为SKUPrice是空值<br>它不会被记录<br><img src="/2018/01/24/HBase01/datamate1.png" alt="avatar"></li>
</ol>
<ol start="3">
<li>HBase 数据模型多版本<br>看下这个图，对于row2，可以看到SKUName 有两个值<br>但是它们的Timestamp 不一样，这就是 HBase的特点，它会把<br>更新的值也insert 进去，只不过时间戳不一样，这就决定了它<br>的写很快的特点</li>
</ol>
<p><img src="/2018/01/24/HBase01/datamate2.png" alt="avatar"></p>
<h3 id="二-HBase-写流程"><a href="#二-HBase-写流程" class="headerlink" title="二. HBase 写流程"></a>二. HBase 写流程</h3><ul>
<li>第一步：和zookeeper通信，get /hasbe/meta-region-server<br>作用是找到 存储 hbase:meta 表的server</li>
</ul>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line">[zk: localhost:2181(CONNECTED) 2] get &#x2F;hbase&#x2F;meta-region-server</span><br><span class="line">�regionserver:60020���I�&#39;_PBUF</span><br><span class="line"></span><br><span class="line">cZxid &#x3D; 0x300000576</span><br><span class="line"></span><br><span class="line">ctime &#x3D; Sun Feb 16 10:34:37 CST 2020</span><br><span class="line"></span><br><span class="line">mZxid &#x3D; 0x300000576</span><br><span class="line"></span><br><span class="line">mtime &#x3D; Sun Feb 16 10:34:37 CST 2020</span><br><span class="line"></span><br><span class="line">pZxid &#x3D; 0x300000576</span><br><span class="line"></span><br><span class="line">cversion &#x3D; 0</span><br><span class="line"></span><br><span class="line">dataVersion &#x3D; 0</span><br><span class="line"></span><br><span class="line">aclVersion &#x3D; 0</span><br><span class="line"></span><br><span class="line">ephemeralOwner &#x3D; 0x0</span><br><span class="line"></span><br><span class="line">dataLength &#x3D; 62</span><br><span class="line"></span><br><span class="line">numChildren &#x3D; 0</span><br></pre></td></tr></table></figure>

<ul>
<li>第二步 读取 hbase:meta 表，主要获取如下信息</li>
</ul>
<p>表名称/startkey/endkey/字段名称</p>
<p>这样就知道了哪个表在哪台server上，要写入哪个rowkey</p>
<p>这个是 hasbe:meta 的信息：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">tedata:orderinfo,,1576370203149.63c59b5a54 column&#x3D;info:regioninfo, timestamp&#x3D;1581820480055, value&#x3D;&#123;ENCODED &#x3D;&gt; 63c59b5a54b02a27d8969ac352cb6e81, NAME &#x3D;&gt; &#39;tedata:orderinfo,</span><br><span class="line"> b02a27d8969ac352cb6e81.                    ,1576370203149.63c59b5a54b02a27d8969ac352cb6e81.&#39;, STARTKEY &#x3D;&gt; &#39;&#39;, ENDKEY &#x3D;&gt; &#39;&#39;&#125;</span><br></pre></td></tr></table></figure>

<ul>
<li>第三步 写HLOG 文件</li>
</ul>
<p>HBase先会写入 HLOG 日志文件，也成为预写日志文件，只有HLOG 写入成功了，才会写入 去写memstore，这样就保证了数据不会丢失</p>
<ul>
<li>第四步 写store </li>
</ul>
<p>HLOG写入成功后，开始把数据写入 memstore，当memstore写满，flush到storefile中<br>storefile 和 DFS Client 通信，写入到 HDFS 中</p>
<h3 id="三-HBase-读流程"><a href="#三-HBase-读流程" class="headerlink" title="三. HBase 读流程"></a>三. HBase 读流程</h3><ul>
<li>第一步：和zookeeper通信，get /hasbe/meta-region-server<br>作用是找到 存储 hbase:meta 表的server</li>
</ul>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line">[zk: localhost:2181(CONNECTED) 2] get &#x2F;hbase&#x2F;meta-region-server</span><br><span class="line">�regionserver:60020���I�&#39;_PBUF</span><br><span class="line"></span><br><span class="line">cZxid &#x3D; 0x300000576</span><br><span class="line"></span><br><span class="line">ctime &#x3D; Sun Feb 16 10:34:37 CST 2020</span><br><span class="line"></span><br><span class="line">mZxid &#x3D; 0x300000576</span><br><span class="line"></span><br><span class="line">mtime &#x3D; Sun Feb 16 10:34:37 CST 2020</span><br><span class="line"></span><br><span class="line">pZxid &#x3D; 0x300000576</span><br><span class="line"></span><br><span class="line">cversion &#x3D; 0</span><br><span class="line"></span><br><span class="line">dataVersion &#x3D; 0</span><br><span class="line"></span><br><span class="line">aclVersion &#x3D; 0</span><br><span class="line"></span><br><span class="line">ephemeralOwner &#x3D; 0x0</span><br><span class="line"></span><br><span class="line">dataLength &#x3D; 62</span><br><span class="line"></span><br><span class="line">numChildren &#x3D; 0</span><br></pre></td></tr></table></figure>

<ul>
<li>第二步 读取 hbase:meta 表，主要获取如下信息</li>
</ul>
<p>表名称/startkey/endkey/字段名称</p>
<p>这样就知道了哪个表在哪台server上，要写入哪个rowkey</p>
<p>这个是 hasbe:meta 的信息：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">tedata:orderinfo,,1576370203149.63c59b5a54 column&#x3D;info:regioninfo, timestamp&#x3D;1581820480055, value&#x3D;&#123;ENCODED &#x3D;&gt; 63c59b5a54b02a27d8969ac352cb6e81, NAME &#x3D;&gt; &#39;tedata:orderinfo,</span><br><span class="line"> b02a27d8969ac352cb6e81.                    ,1576370203149.63c59b5a54b02a27d8969ac352cb6e81.&#39;, STARTKEY &#x3D;&gt; &#39;&#39;, ENDKEY &#x3D;&gt; &#39;&#39;&#125;</span><br></pre></td></tr></table></figure>

<ul>
<li>第三步</li>
</ul>
<p>先去memstore读取数据，如果没有，从Blockcache 读取，如果没有，<br>去HDFS 读取数据</p>

      
      <!-- reward -->
      
    </div>
    
    
      <!-- copyright -->
      
    <footer class="article-footer">
      
      
  <ul class="article-tag-list" itemprop="keywords"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/tedata/" rel="tag">tedata</a></li></ul>


    </footer>

  </div>

  

  
  
  

  

</article>
    
    <article id="post-CDH完美卸载" class="article article-type-post" itemscope
  itemprop="blogPost" data-scroll-reveal>

  <div class="article-inner">
    
    <header class="article-header">
       
<h2 itemprop="name">
  <a class="article-title" href="/2017/07/20/CDH%E5%AE%8C%E7%BE%8E%E5%8D%B8%E8%BD%BD/"
    >CDH 完美卸载</a
  >
</h2>
  

    </header>
    

    
    <div class="article-meta">
      <a href="/2017/07/20/CDH%E5%AE%8C%E7%BE%8E%E5%8D%B8%E8%BD%BD/" class="article-date">
  <time datetime="2017-07-20T12:43:27.000Z" itemprop="datePublished">2017-07-20</time>
</a>
      
      
      
      
    </div>
    

    

    
    <div class="article-entry" itemprop="articleBody">
      


      

      
      <h1 id="CDH完美卸载"><a href="#CDH完美卸载" class="headerlink" title="CDH完美卸载"></a>CDH完美卸载</h1><ul>
<li><input checked disabled type="checkbox"> 学习目标：</li>
<li><input checked disabled type="checkbox"> CDH 卸载</li>
</ul>
<h3 id="一-卸载前的规划"><a href="#一-卸载前的规划" class="headerlink" title="一. 卸载前的规划"></a>一. 卸载前的规划</h3><ul>
<li>关闭集群 及 MySQL服务</li>
<li>删除部署文件夹 /opt/cloudera*</li>
<li>删除数据文件夹</li>
</ul>
<h3 id="二-如何完美卸载集群"><a href="#二-如何完美卸载集群" class="headerlink" title="二.如何完美卸载集群"></a>二.如何完美卸载集群</h3><ol>
<li>HDFS YARN ZK存储数据目录</li>
</ol>
<ul>
<li>/dfs/nn</li>
<li>/dfs/dn</li>
<li>/dfs/snn</li>
<li>/yarn/nm</li>
<li>/var/lib/zookeeper</li>
</ul>
<ol start="2">
<li><p>关闭集群 及 MySQL服务</p>
</li>
<li><p>杀进程(执行2次)</p>
</li>
</ol>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">kill -9 $(pgrep -f cloudera)</span><br><span class="line">pgrep -f cloudera</span><br></pre></td></tr></table></figure>


<p>2.4 卸载</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">umount &#x2F;opt&#x2F;cloudera-manager&#x2F;cm-5.16.2&#x2F;run&#x2F;cloudera-scm-agent&#x2F;process</span><br></pre></td></tr></table></figure>


<p>假如无法卸载，夯住了 就强制kill</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">yum install -y lsof</span><br><span class="line">lsof &#x2F;opt&#x2F;cloudera-manager&#x2F;cm-5.16.2&#x2F;run&#x2F;cloudera-scm-agent&#x2F;process</span><br><span class="line">kill -9 $(lsof &#x2F;opt&#x2F;cloudera-manager&#x2F;cm-5.16.2&#x2F;run&#x2F;cloudera-scm-agent&#x2F;process | awk &#39;&#123;print $2&#125;&#39;)</span><br></pre></td></tr></table></figure>



<p>一定要再df -h 校验一下</p>
<p>2.5 删除cloudera部署文件夹</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">rm -rf &#x2F;opt&#x2F;cloudera*</span><br><span class="line">rm -rf &#x2F;dfs &#x2F;yarn &#x2F;var&#x2F;lib&#x2F;zookeeper</span><br><span class="line"></span><br><span class="line">rm -rf &#x2F;usr&#x2F;share&#x2F;cmf</span><br><span class="line">rm -rf &#x2F;var&#x2F;lib&#x2F;cloudera*</span><br><span class="line">rm -rf &#x2F;var&#x2F;log&#x2F;cloudera*</span><br><span class="line">rm -rf &#x2F;run&#x2F;cloudera-scm-agent</span><br><span class="line">rm -rf &#x2F;etc&#x2F;security&#x2F;limits.d&#x2F;cloudera-scm.conf </span><br><span class="line">rm -rf &#x2F;etc&#x2F;cloudera*</span><br><span class="line">rm -rf &#x2F;etc&#x2F;hadoop* &#x2F;etc&#x2F;zookeeper &#x2F;etc&#x2F;hive* &#x2F;etc&#x2F;hbase* &#x2F;etc&#x2F;impala &#x2F;etc&#x2F;spark &#x2F;etc&#x2F;solr &#x2F;etc&#x2F;sqoop*</span><br><span class="line">rm -rf &#x2F;tmp&#x2F;scm_*   &#x2F;tmp&#x2F;.scm_prepare_node.lock</span><br></pre></td></tr></table></figure>


<p>2.6 全局搜索 删除</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">find &#x2F; -name &#39;*cloudera*&#39; | while read line; do rm -rf $&#123;line&#125;; done</span><br></pre></td></tr></table></figure>


<p>2.7 MySQL的数据库</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line">cmf db--》cm server</span><br><span class="line">amon db--》amon</span><br><span class="line"></span><br><span class="line">mysql&gt; drop database cmf;</span><br><span class="line">Query OK, 47 rows affected (0.60 sec)</span><br><span class="line"></span><br><span class="line">mysql&gt; drop database amon;</span><br><span class="line">Query OK, 64 rows affected (0.23 sec)</span><br><span class="line"></span><br><span class="line">mysql&gt; drop user cmf;</span><br><span class="line">Query OK, 0 rows affected (0.00 sec)</span><br><span class="line"></span><br><span class="line">mysql&gt; drop user amon;</span><br><span class="line">Query OK, 0 rows affected (0.00 sec)</span><br><span class="line"></span><br><span class="line">mysql&gt; use mysql;</span><br><span class="line">Database changed</span><br><span class="line">mysql&gt; select user from user;</span><br><span class="line">+-----------+</span><br><span class="line">| user      |</span><br><span class="line">+-----------+</span><br><span class="line">| root      |</span><br><span class="line">| mysql.sys |</span><br><span class="line">| root      |</span><br><span class="line">+-----------+</span><br><span class="line">3 rows in set (0.00 sec)</span><br></pre></td></tr></table></figure>


<h3 id="三-有个坑需要踩"><a href="#三-有个坑需要踩" class="headerlink" title="三. 有个坑需要踩"></a>三. 有个坑需要踩</h3><p>alternatives 一个组件多版本 动态管理</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">[root@ruozedata001 alternatives]# alternatives --config hadoop</span><br><span class="line"></span><br><span class="line">There is 1 program that provides &#39;hadoop&#39;.</span><br><span class="line"></span><br><span class="line">  Selection    Command</span><br><span class="line">-----------------------------------------------</span><br><span class="line">*+ 1           &#x2F;opt&#x2F;cloudera&#x2F;parcels&#x2F;CDH-5.16.2-1.cdh5.16.2.p0.8&#x2F;bin&#x2F;hadoop</span><br><span class="line"> + 2 	       &#x2F;opt&#x2F;cloudera&#x2F;parcels&#x2F;CDH-5.16.3-1.cdh5.16.3.p0.8&#x2F;bin&#x2F;hadoop</span><br><span class="line"></span><br><span class="line">Enter to keep the current selection[+], or type selection number: 1</span><br><span class="line">[root@ruozedata001 alternatives]#</span><br></pre></td></tr></table></figure>



<p>未来，本套机器需要升级 假如CDH5.16.3,就是hadoop命令找不到<br>修复这个版本切换：  alternatives –config hadoop<br><a href="http://blog.itpub.net/30089851/viewspace-2128683/" target="_blank" rel="noopener">http://blog.itpub.net/30089851/viewspace-2128683/</a></p>

      
      <!-- reward -->
      
    </div>
    
    
      <!-- copyright -->
      
    <footer class="article-footer">
      
      
  <ul class="article-tag-list" itemprop="keywords"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/tedata/" rel="tag">tedata</a></li></ul>


    </footer>

  </div>

  

  
  
  

  

</article>
    
    <article id="post-CDH通过实例恢复集群" class="article article-type-post" itemscope
  itemprop="blogPost" data-scroll-reveal>

  <div class="article-inner">
    
    <header class="article-header">
       
<h2 itemprop="name">
  <a class="article-title" href="/2017/07/12/CDH%E9%80%9A%E8%BF%87%E5%AE%9E%E4%BE%8B%E6%81%A2%E5%A4%8D%E9%9B%86%E7%BE%A4/"
    >CDH 通过实例恢复集群</a
  >
</h2>
  

    </header>
    

    
    <div class="article-meta">
      <a href="/2017/07/12/CDH%E9%80%9A%E8%BF%87%E5%AE%9E%E4%BE%8B%E6%81%A2%E5%A4%8D%E9%9B%86%E7%BE%A4/" class="article-date">
  <time datetime="2017-07-12T12:43:27.000Z" itemprop="datePublished">2017-07-12</time>
</a>
      
      
      
      
    </div>
    

    

    
    <div class="article-entry" itemprop="articleBody">
      


      

      
      <h1 id="CDH通过实例恢复集群"><a href="#CDH通过实例恢复集群" class="headerlink" title="CDH通过实例恢复集群"></a>CDH通过实例恢复集群</h1><ol>
<li><p>修改新的 内网IP</p>
</li>
<li><p>启动 mysql</p>
</li>
<li><p>server<br>修改 db<br>cat db.properties<br><img src="/2017/07/12/CDH%E9%80%9A%E8%BF%87%E5%AE%9E%E4%BE%8B%E6%81%A2%E5%A4%8D%E9%9B%86%E7%BE%A4/cdh0301.png" alt="avatar"><br><img src="/2017/07/12/CDH%E9%80%9A%E8%BF%87%E5%AE%9E%E4%BE%8B%E6%81%A2%E5%A4%8D%E9%9B%86%E7%BE%A4/cdh0302.png" alt="avatar"></p>
</li>
</ol>
<ol start="4">
<li><p>agent  (有agent的机器都要检查)<br>config.ini<br><img src="/2017/07/12/CDH%E9%80%9A%E8%BF%87%E5%AE%9E%E4%BE%8B%E6%81%A2%E5%A4%8D%E9%9B%86%E7%BE%A4/cdh0303.png" alt="avatar"></p>
</li>
<li><p>mysql 表<br>use cmf;<br>desc hosts;<br>update hosts set ip = xxx where hosts = xxx<br><img src="/2017/07/12/CDH%E9%80%9A%E8%BF%87%E5%AE%9E%E4%BE%8B%E6%81%A2%E5%A4%8D%E9%9B%86%E7%BE%A4/cdh0304.png" alt="avatar"></p>
</li>
</ol>
<p>tail -f /-F 的区别是什么？</p>
<ol start="6">
<li><p>启动 server<br>./cloudera-scm-server start</p>
</li>
<li><p>启动 所以 agent</p>
</li>
<li><p>web 界面启动 cms<br>可以看到，ip发生了变化<br><img src="/2017/07/12/CDH%E9%80%9A%E8%BF%87%E5%AE%9E%E4%BE%8B%E6%81%A2%E5%A4%8D%E9%9B%86%E7%BE%A4/cdh0305.png" alt="avatar"></p>
</li>
</ol>
<p>需要重新部署下 客户端</p>

      
      <!-- reward -->
      
    </div>
    
    
      <!-- copyright -->
      
    <footer class="article-footer">
      
      
  <ul class="article-tag-list" itemprop="keywords"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/tedata/" rel="tag">tedata</a></li></ul>


    </footer>

  </div>

  

  
  
  

  

</article>
    
    <article id="post-CDH01" class="article article-type-post" itemscope
  itemprop="blogPost" data-scroll-reveal>

  <div class="article-inner">
    
    <header class="article-header">
       
<h2 itemprop="name">
  <a class="article-title" href="/2017/07/09/CDH01/"
    >CDH 常规使用</a
  >
</h2>
  

    </header>
    

    
    <div class="article-meta">
      <a href="/2017/07/09/CDH01/" class="article-date">
  <time datetime="2017-07-09T12:43:27.000Z" itemprop="datePublished">2017-07-09</time>
</a>
      
      
      
      
    </div>
    

    

    
    <div class="article-entry" itemprop="articleBody">
      


      

      
      <h1 id="CDH-常规使用"><a href="#CDH-常规使用" class="headerlink" title="CDH 常规使用"></a>CDH 常规使用</h1><p>学习目标：</p>
<ol>
<li>cdh官网解读</li>
<li>cdh启动/停止</li>
<li>cdh架构</li>
<li>日志解读</li>
<li>web页面分析</li>
<li>怎样添加服务</li>
<li>监控</li>
</ol>
<h3 id="一-cloudera官网"><a href="#一-cloudera官网" class="headerlink" title="一. cloudera官网"></a>一. cloudera官网</h3><p><a href="http://www.cloudera.com" target="_blank" rel="noopener">www.cloudera.com</a>  CM不开源  CDH: CM+apache组件和cloudera公司的组件</p>
<p>CDH 6.x 6.3.1   HDFS3.0 HBase2.0<br>    5.x 5.16.1  HDFS2.6 HBase1.2</p>
<p><a href="https://www.cloudera.com/downloads/manager/5-16-2.html" target="_blank" rel="noopener">https://www.cloudera.com/downloads/manager/5-16-2.html</a><br><a href="https://docs.cloudera.com/documentation/index.html" target="_blank" rel="noopener">https://docs.cloudera.com/documentation/index.html</a><br><a href="https://docs.cloudera.com/documentation/enterprise/5-16-x.html" target="_blank" rel="noopener">https://docs.cloudera.com/documentation/enterprise/5-16-x.html</a></p>
<h3 id="二-正常启动停止顺序"><a href="#二-正常启动停止顺序" class="headerlink" title="二. 正常启动停止顺序"></a>二. 正常启动停止顺序</h3><p>su - mysqladmin</p>
<p>service mysql start</p>
<p>/opt/cloudera-manager/cm-5.16.2/etc/init.d/cloudera-scm-server start 1个节点<br>/opt/cloudera-manager/cm-5.16.2/etc/init.d/cloudera-scm-agent start 所有节点</p>
<p>进入web7180，先启动 CMS 5个进程服务</p>
<p>启动Cluster1服务: HDFS YARN ZK KAFKA HBASE</p>
<p>停止顺序?</p>
<p>坑: mysql 单点 cm metadata </p>
<p>cm挂了 启动 初始化  cm metadata + hive 表数据 重新初始化</p>
<p>1.mysql没有开启binlog<br>2.mysql没有 定期备份 1天<br>mysqldump命令 cmf &gt;cmf.sql</p>
<h3 id="三-架构"><a href="#三-架构" class="headerlink" title="三. 架构"></a>三. 架构</h3><p><img src="/2017/07/09/CDH01/cma.png" alt="avatar"></p>
<p>假如 CM web界面server服务挂了，HDFS YARN这些服务正常吗？</p>
<p>配置:<br>服务端   /opt/cloudera-manager/cm-5.16.2/run/cloudera-scm-agent/process/366-hdfs-NAMENODE<br>客户端   /etc/hadoop/conf</p>
<p>1.cmf.config表<br>2.服务端 带序号的<br>3.客户端 不带序号 默认的</p>
<p>务必从web界面修改参数值 </p>
<p>应用开发 配置 /etc/hadoop/conf</p>
<p>客户端  gateway<br>    在web上点击 添加gateway服务，添加后，不需要重启服务，只需要重新部署客户端即可<br><img src="/2017/07/09/CDH01/gateway.png" alt="avatar"></p>
<h3 id="四-日志"><a href="#四-日志" class="headerlink" title="四. 日志"></a>四. 日志</h3><p><a href="http://106.14.180.252:7180/cmf/config2?task=ALL_LOG_DIRECTORIES" target="_blank" rel="noopener">http://106.14.180.252:7180/cmf/config2?task=ALL_LOG_DIRECTORIES</a></p>
<p>Configruation –&gt; log service 查看日志目录</p>
<p>组件服务的日志： /var/log/xxx</p>
<p>TAR CM的 /opt/cloudera-manager/cm-5.16.2/log/cloudera-scm-server<br>     /opt/cloudera-manager/cm-5.16.2/log/cloudera-scm-agent</p>
<p>RPM部署CM ：</p>
<pre><code>/var/log/cloudera-scm-server 
/var/log/cloudera-scm-agent</code></pre><p>xxxxx..log.out  进程的日志  出现error  优先排查<br>Will not attempt to authenticate using SASL (unknown error)</p>
<p>stdout 和 stderr 相当于 shell 脚本的debug的输出</p>
<p>xxxx.stdout</p>
<p>xxxx.stderr</p>
<p><a href="http://blog.itpub.net/30089851/viewspace-2136372/" target="_blank" rel="noopener">http://blog.itpub.net/30089851/viewspace-2136372/</a></p>
<h3 id="五-界面解读"><a href="#五-界面解读" class="headerlink" title="五. 界面解读"></a>五. 界面解读</h3><p>进程 process  instance  role</p>
<p>NameNode Advanced Configuration Snippet (Safety Valve) for hdfs-site.xml</p>
<p>ranger HDP</p>
<h3 id="六-添加服务-amp-HOST"><a href="#六-添加服务-amp-HOST" class="headerlink" title="六. 添加服务&amp;HOST"></a>六. 添加服务&amp;HOST</h3><p>先手工的部署agent 启动/opt/cloudera-manager/cm-5.16.2/etc/init.d/cloudera-scm-agent start</p>
<h3 id="七-监控"><a href="#七-监控" class="headerlink" title="七. 监控"></a>七. 监控</h3><p>TS query language  TSQL</p>
<p>生产者<br>SELECT total_kafka_bytes_received_rate_across_kafka_broker_topics<br>WHERE entityName = “kafka:DSHS” AND category = KAFKA_TOPIC</p>
<p>消费者<br>SELECT total_kafka_bytes_fetched_rate_across_kafka_broker_topics<br>WHERE entityName = “kafka:DSHS” AND category = KAFKA_TOPIC</p>
<p>SELECT<br>total_kafka_bytes_received_rate_across_kafka_broker_topics,<br>total_kafka_bytes_fetched_rate_across_kafka_broker_topics<br>WHERE entityName = “kafka:DSHS” AND category = KAFKA_TOPIC</p>
<p>charts –》 Chart Builder ，粘贴SQL，出图，给 title 命个名，save –》 HOME PAGE</p>
<p>如果 对监控指标 不是很明确，以HDFS 为例，可以点击 “HDFS” –》 “Charts Library” 去里面选择想要的图的SQL</p>
<p>丢: 配置邮件预警 +….+Kafka+Spark2</p>

      
      <!-- reward -->
      
    </div>
    
    
      <!-- copyright -->
      
    <footer class="article-footer">
      
      
  <ul class="article-tag-list" itemprop="keywords"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/tedata/" rel="tag">tedata</a></li></ul>


    </footer>

  </div>

  

  
  
  

  

</article>
    
    <article id="post-SparkSQL02" class="article article-type-post" itemscope
  itemprop="blogPost" data-scroll-reveal>

  <div class="article-inner">
    
    <header class="article-header">
       
<h2 itemprop="name">
  <a class="article-title" href="/2017/04/12/SparkSQL02/"
    >Spark SQL 第二课</a
  >
</h2>
  

    </header>
    

    
    <div class="article-meta">
      <a href="/2017/04/12/SparkSQL02/" class="article-date">
  <time datetime="2017-04-12T12:43:27.000Z" itemprop="datePublished">2017-04-12</time>
</a>
      
      
      
      
    </div>
    

    

    
    <div class="article-entry" itemprop="articleBody">
      


      

      
      <h1 id="SparkSQL02"><a href="#SparkSQL02" class="headerlink" title="SparkSQL02"></a>SparkSQL02</h1><p>学习目标：</p>
<ol>
<li>saveAsTable 和 insertInto 的区别</li>
<li>怎么创建视图，即DF 创建表</li>
<li>Catalog 使用</li>
<li>DF/DS/RDD 相互转换</li>
<li>UDF 函数</li>
</ol>
<h3 id="一-saveAsTable-和-insertInto-的区别"><a href="#一-saveAsTable-和-insertInto-的区别" class="headerlink" title="一. saveAsTable 和 insertInto 的区别"></a>一. saveAsTable 和 insertInto 的区别</h3><p>sql 写出的数据 保存成一张表, 操作hive<br>方式一</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">spark.sql(&quot;&quot;).write.saveAsTable(&quot;&quot;)</span><br></pre></td></tr></table></figure>


<p>方式二</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">spark.sql(&quot;&quot;).write.insertInto(&quot;&quot;)</span><br></pre></td></tr></table></figure>


<p>saveAsTable 和 insertInto 的区别:</p>
<p>insertInto 忽略字段名称, 而是基于位置插入的, 即按顺序插入</p>
<p>看下下面的测试:</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">scala&gt; Seq((1,2)).toDF(&quot;i&quot;,&quot;j&quot;).write.mode(&quot;overwrite&quot;).saveAsTable(&quot;t1&quot;)</span><br><span class="line">scala&gt; sql(&quot;select * from t1&quot;).show</span><br><span class="line">+---+---+</span><br><span class="line">|  i|  j|</span><br><span class="line">+---+---+</span><br><span class="line">|  1|  2|</span><br><span class="line">+---+---+</span><br></pre></td></tr></table></figure>

<p><img src="/2017/04/12/SparkSQL02/insertinto1.png" alt="avatar"></p>
<p>可以看到, saveAsTable 是根据字段插入的, 字段i,j 和 值是对应的</p>
<p>可以看到, insertInto 是根据顺序来的, i, j 并没有和 值对应<br><img src="/2017/04/12/SparkSQL02/saveastable.png" alt="avatar"></p>
<p>测试表存在和不存在上面两种方式的区别?</p>
<h3 id="二-怎么创建视图，即DF-创建表"><a href="#二-怎么创建视图，即DF-创建表" class="headerlink" title="二. 怎么创建视图，即DF 创建表"></a>二. 怎么创建视图，即DF 创建表</h3><p>spark.sql(“create table…”) //这种方式不推荐, 因为 创建表是需要权限的, 提前创建好最好</p>
<p>临时视图和 全局视图<br>创建临时视图</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">df.createOrReplaceTempView(&quot;people&quot;)</span><br></pre></td></tr></table></figure>

<p>创建全局视图</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">df.createGlobalTempView(&quot;people&quot;)</span><br></pre></td></tr></table></figure>

<p>全局视图必须在 视图前加 “ global_temp”</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">spark.sql(&quot;SELECT * FROM global_temp.people&quot;).show()</span><br></pre></td></tr></table></figure>


<p>platform 组内 province访问次数最多的 TOPN</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">select platform, province, count(1) from  log group  by  platform, province</span><br></pre></td></tr></table></figure>



<h3 id="三-Catalog-使用"><a href="#三-Catalog-使用" class="headerlink" title="三. Catalog 使用"></a>三. Catalog 使用</h3><p>catalog</p>
<p>创建catalog<br>scala&gt; val catalog = spark.catalog</p>
<p>查看数据库</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">scala&gt; catalog.listDatabases.show</span><br><span class="line">+-------+--------------------+--------------------+</span><br><span class="line">|   name|         description|         locationUri|</span><br><span class="line">+-------+--------------------+--------------------+</span><br><span class="line">|    bdp|                    |hdfs:&#x2F;&#x2F;hdcluster&#x2F;...|</span><br><span class="line">|default|Default Hive data...|hdfs:&#x2F;&#x2F;hdcluster&#x2F;...|</span><br><span class="line">|   test|                    |hdfs:&#x2F;&#x2F;hdcluster&#x2F;...|</span><br><span class="line">+-------+--------------------+--------------------+</span><br></pre></td></tr></table></figure>




<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">scala&gt; catalog.listDatabases.show(false)</span><br><span class="line">+-------+---------------------+--------------------------------------------+</span><br><span class="line">|name   |description          |locationUri                                 |</span><br><span class="line">+-------+---------------------+--------------------------------------------+</span><br><span class="line">|bdp    |                     |hdfs:&#x2F;&#x2F;hdcluster&#x2F;user&#x2F;hive&#x2F;warehouse&#x2F;bdp.db |</span><br><span class="line">|default|Default Hive database|hdfs:&#x2F;&#x2F;hdcluster&#x2F;user&#x2F;hive&#x2F;warehouse        |</span><br><span class="line">|test   |                     |hdfs:&#x2F;&#x2F;hdcluster&#x2F;user&#x2F;hive&#x2F;warehouse&#x2F;test.db|</span><br><span class="line">+-------+---------------------+--------------------------------------------+</span><br></pre></td></tr></table></figure>

<p>查看表</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">scala&gt; catalog.listTables(&quot;test&quot;).show</span><br><span class="line">+-------+--------+-----------+---------+-----------+</span><br><span class="line">|   name|database|description|tableType|isTemporary|</span><br><span class="line">+-------+--------+-----------+---------+-----------+</span><br><span class="line">|  test2|    test|       null|  MANAGED|      false|</span><br><span class="line">| test22|    test|       null|  MANAGED|      false|</span><br><span class="line">|tratest|    test|       null|  MANAGED|      false|</span><br><span class="line">+-------+--------+-----------+---------+-----------+</span><br></pre></td></tr></table></figure>


<p>查看所有函数</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">scala&gt; catalog.listFunctions.show</span><br><span class="line">+----------+--------+-----------+--------------------+-----------+</span><br><span class="line">|      name|database|description|           className|isTemporary|</span><br><span class="line">+----------+--------+-----------+--------------------+-----------+</span><br><span class="line">|         !|    null|       null|org.apache.spark....|       true|</span><br><span class="line">|         %|    null|       null|org.apache.spark....|       true|</span><br><span class="line">|         &amp;|    null|       null|org.apache.spark....|       true|</span><br><span class="line">|         *|    null|       null|org.apache.spark....|       true|</span><br><span class="line">|         +|    null|       null|org.apache.spark....|       true|</span><br><span class="line">|         -|    null|       null|org.apache.spark....|       true|</span><br></pre></td></tr></table></figure>


<p>查看字段</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">scala&gt; catalog.listColumns(&quot;test.test22&quot;).show</span><br><span class="line">+----+-----------+--------+--------+-----------+--------+</span><br><span class="line">|name|description|dataType|nullable|isPartition|isBucket|</span><br><span class="line">+----+-----------+--------+--------+-----------+--------+</span><br><span class="line">| uid|       null|  string|    true|      false|   false|</span><br><span class="line">| pid|       null|  string|    true|      false|   false|</span><br></pre></td></tr></table></figure>


<h3 id="四-DF-DS-RDD-相互转换"><a href="#四-DF-DS-RDD-相互转换" class="headerlink" title="四. DF/DS/RDD 相互转换"></a>四. DF/DS/RDD 相互转换</h3><p>DF/DF/RDD</p>
<p>ROW DF 弱类型</p>
<ul>
<li>df 转ds</li>
</ul>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">import spark.implicits._</span><br><span class="line">val df &#x3D; spark.read.option(&quot;header&quot;,&quot;true&quot;)</span><br><span class="line">.option(&quot;inferSchema&quot;,&quot;true&quot;).csv(&quot;data&#x2F;sales.csv&quot;)</span><br><span class="line">val ds &#x3D; df.as[Sales]</span><br><span class="line">&#x2F;&#x2F; Sales 是一个case class</span><br><span class="line">case class Sales(transactionId:Int,customerId:Int,itemId:Int,amountPaid:Double)</span><br></pre></td></tr></table></figure>


<ul>
<li>ds 转df</li>
</ul>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">import spark.implicits._</span><br><span class="line"></span><br><span class="line">val ds &#x3D; spark.read.textFile(&quot;data&#x2F;access.log&quot;)</span><br><span class="line">      .map(x &#x3D;&gt; &#123;</span><br><span class="line">        val splits &#x3D; x.split(&quot;\t&quot;)</span><br><span class="line">        val platform &#x3D; splits(1)</span><br><span class="line">        val traffic &#x3D; splits(6).toLong</span><br><span class="line">        val province &#x3D; splits(8)</span><br><span class="line">        val city &#x3D; splits(9)</span><br><span class="line">        val isp &#x3D; splits(10)</span><br><span class="line">        (platform, traffic, province, city, isp)</span><br><span class="line">      &#125;)</span><br><span class="line">val df &#x3D; ds.toDF(&quot;platform&quot;, &quot;traffic&quot;, &quot;province&quot;, &quot;city&quot;, &quot;isp&quot;)</span><br></pre></td></tr></table></figure>


<pre><code>  // 如果你想使用SQL来进行处理，那么就是将df注册成一个临时视图
df.createOrReplaceTempView(&quot;log&quot;)</code></pre><p>row_number<br>rank<br>dense_rank<br>的区别</p>
<ul>
<li>RDD =&gt; DF</li>
</ul>
<ol>
<li>反射</li>
</ol>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">import spark.implicits._</span><br><span class="line">  val peopleDF &#x3D; spark.sparkContext</span><br><span class="line">     .textFile(&quot;data&#x2F;people.txt&quot;)</span><br><span class="line">     .map(_.split(&quot;,&quot;))</span><br><span class="line">     .map(x &#x3D;&gt; Person(x(0), x(1).trim.toInt))</span><br><span class="line">     .toDF()</span><br><span class="line"></span><br><span class="line">   peopleDF.show(false)</span><br><span class="line"></span><br><span class="line">case class Person(name:String,age:Int)</span><br></pre></td></tr></table></figure>


<p>分解:</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">val peopleDF &#x3D; spark.sparkContext</span><br><span class="line">     .textFile(&quot;data&#x2F;people.txt&quot;) </span><br><span class="line">得到一个 RDD</span><br></pre></td></tr></table></figure>



<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">.map(_.split(&quot;,&quot;))</span><br><span class="line">&#x2F;&#x2F; 这一步是一个反射, Person是一个case class</span><br><span class="line">&#x2F;&#x2F;已经有schema信息了, 而 RDD 相对于 DF</span><br><span class="line">&#x2F;&#x2F;缺少的就是schema</span><br><span class="line">     .map(x &#x3D;&gt; Person(x(0), x(1).trim.toInt))</span><br><span class="line">     .toDF()</span><br><span class="line">转成一个DF</span><br></pre></td></tr></table></figure>


<ol start="2">
<li>编程自定义<br>// step1: Create an RDD</li>
</ol>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">val peopleRDD &#x3D; spark.sparkContext.textFile(&quot;data&#x2F;people.txt&quot;)</span><br></pre></td></tr></table></figure>



<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">&#x2F;&#x2F; step2: The schema is encoded in a string</span><br><span class="line">    &#x2F;&#x2F; 和官网提供的不同, 因为字段类型多样, 官网只给了字符串类型写法</span><br><span class="line">    &#x2F;&#x2F; 下面是更适用的写法</span><br><span class="line"></span><br><span class="line">    val schema &#x3D; StructType(Array(</span><br><span class="line">      StructField(&quot;name&quot;,StringType),</span><br><span class="line">      StructField(&quot;age&quot;,IntegerType)</span><br><span class="line">    ))</span><br><span class="line"></span><br><span class="line">    &#x2F;&#x2F; Convert records of the RDD (people) to Rows</span><br><span class="line">    val rowRDD &#x3D; peopleRDD</span><br><span class="line">      .map(_.split(&quot;,&quot;))</span><br><span class="line">      .map(attributes &#x3D;&gt; Row(attributes(0), attributes(1).trim.toInt)) &#x2F;&#x2F; 要写toInt</span><br><span class="line"></span><br><span class="line">    &#x2F;&#x2F; Apply the schema to the RDD</span><br><span class="line">    val peopleDF &#x3D; spark.createDataFrame(rowRDD, schema)</span><br><span class="line"></span><br><span class="line">    peopleDF.show()</span><br></pre></td></tr></table></figure>



<p>注意下这段代码, 起了两个 spark context, 会报spark context 异常</p>
<p><img src="/2017/04/12/SparkSQL02/sperror1.png" alt="avatar"><br><img src="/2017/04/12/SparkSQL02/sperror2.png" alt="avatar"></p>
<p>正确的做法是:<br>用 spark session 起 spark context<br><img src="/2017/04/12/SparkSQL02/spok.png" alt="avatar"></p>
<h3 id="五-UDF-函数"><a href="#五-UDF-函数" class="headerlink" title="五. UDF 函数"></a>五. UDF 函数</h3><p>spark sql 注册 udf 的两种方式:</p>
<ol>
<li>sqlContext.udf.register()<br>只对 sql 有效</li>
<li>spark.sql.function.udf()<br>此时注册的方法，对外部可见, 可以使用api</li>
</ol>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line">select AD.* ,</span><br><span class="line">ROUND(AD.adbidseccesscounts&#x2F;AD.adbidcounts * 100, 2) bidrate,</span><br><span class="line">ROUND(AD.adclickcounts&#x2F;AD.addispalycounts * 100, 2) clickrate</span><br><span class="line">from</span><br><span class="line">(</span><br><span class="line">select</span><br><span class="line">province,</span><br><span class="line">city,</span><br><span class="line">sum (case when requestmode&#x3D;1 and processnode&gt;&#x3D;1 then 1 else 0 end) requestmodecounts,</span><br><span class="line">sum (case when requestmode&#x3D;1 and processnode&gt;&#x3D;2 then 1 else 0 end) processnodecounts,</span><br><span class="line">sum (case when requestmode&#x3D;1 and processnode&#x3D;3 then 1 else 0 end) adrequestcounts,</span><br><span class="line">sum (case when adplatformproviderid&gt;&#x3D;100000 and iseffective&#x3D;1</span><br><span class="line">     and isbilling&#x3D;1 and isbid&#x3D;1 and adorderid!&#x3D;0 then 1 else 0 end) adbidcounts,</span><br><span class="line">sum (case when adplatformproviderid&gt;&#x3D;100000 and iseffective&#x3D;1</span><br><span class="line">     and isbilling&#x3D;1 and iswin&#x3D;1 then 1 else 0 end) adbidseccesscounts,</span><br><span class="line">sum (case when requestmode&#x3D;2 and iseffective&#x3D;1 then 1 else 0 end) addispalycounts,</span><br><span class="line">sum (case when requestmode&#x3D;3 and iseffective&#x3D;1 then 1 else 0 end) adclickcounts,</span><br><span class="line">sum (case when requestmode&#x3D;2 and iseffective&#x3D;1 and isbilling&#x3D;1 then 1 else 0 end) mediadispalycounts,</span><br><span class="line">sum (case when requestmode&#x3D;3 and iseffective&#x3D;1 and isbilling&#x3D;1 then 1 else 0 end) mediaclickcounts,</span><br><span class="line">sum (case when adplatformproviderid&gt;&#x3D;100000 and iseffective&#x3D;1</span><br><span class="line">     and isbilling&#x3D;1 and iswin&#x3D;1 and adorderid&gt;200000 then 1 else 0 end) adconsumecounts,</span><br><span class="line">sum (case when adplatformproviderid&gt;&#x3D;100000 and iseffective&#x3D;1</span><br><span class="line">     and isbilling&#x3D;1 and iswin&#x3D;1 and adorderid&gt;200000 then 1 else 0 end) adcostcounts</span><br><span class="line">from SYS_AD  GROUP BY province,city) AD</span><br></pre></td></tr></table></figure>


      
      <!-- reward -->
      
    </div>
    
    
      <!-- copyright -->
      
    <footer class="article-footer">
      
      
  <ul class="article-tag-list" itemprop="keywords"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/tedata/" rel="tag">tedata</a></li></ul>


    </footer>

  </div>

  

  
  
  

  

</article>
    
    <article id="post-SparkSQL01" class="article article-type-post" itemscope
  itemprop="blogPost" data-scroll-reveal>

  <div class="article-inner">
    
    <header class="article-header">
       
<h2 itemprop="name">
  <a class="article-title" href="/2017/04/10/SparkSQL01/"
    >Spark SQL 第一课</a
  >
</h2>
  

    </header>
    

    
    <div class="article-meta">
      <a href="/2017/04/10/SparkSQL01/" class="article-date">
  <time datetime="2017-04-10T12:43:27.000Z" itemprop="datePublished">2017-04-10</time>
</a>
      
      
      
      
    </div>
    

    

    
    <div class="article-entry" itemprop="articleBody">
      


      

      
      <h1 id="SparkSQL01"><a href="#SparkSQL01" class="headerlink" title="SparkSQL01"></a>SparkSQL01</h1><p>学习目标：</p>
<ol>
<li>SparkSQL 介绍及与 hive on spark 区别</li>
<li>SparkSQL 编程 POM 依赖</li>
<li>SparkSQL 编程规范</li>
<li>SparkSQL 连接 数据源演示</li>
</ol>
<h3 id="一-SparkSQL-介绍及与-hive-on-spark-区别"><a href="#一-SparkSQL-介绍及与-hive-on-spark-区别" class="headerlink" title="一. SparkSQL 介绍及与 hive on spark 区别"></a>一. SparkSQL 介绍及与 hive on spark 区别</h3><p>SparkSQL 支持hive 语法, 同时支持 hive 序列化/反序列化 , UDF 等<br>支持 访问存在的 hive 仓库</p>
<p>ExtDS : 外部数据源</p>
<p>Hive是进程级别的<br>Spark是线程级别的， 如果用Shark，会存在线程安全的问题</p>
<p>Hive On Spark:  hive 跑在 spark 引擎之上(原来是跑在MR之上)<br>和 Spark SQL 不是一回事<br>set hive.execution.engine = spark 即可</p>
<p>Spark SQL 是在 Spark 里面的<br>Hive On Spark 是在 hive 里面</p>
<p>Spark SQL<br>1.0<br>称为<br>SchemaRdd ==&gt; Table<br>==&gt; DataFrame<br>==&gt; DataSets  1.6  complie-time type safety, 编译时的类型检查, 提前抛出异常</p>
<p>Dataset:<br>dataset 是一个分布式的数据集<br>A Dataset is a distributed collection of data.<br>named columns , 带名字的列, 可以理解为一个表<br>In scala API, DataFrame is simply a type alias of DataSet[ROW]</p>
<p>Dataset API 仅支持 scala, java, 不支持 python</p>
<p>Spark SQL 添加依赖</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">&lt;dependency&gt;</span><br><span class="line">    &lt;groupId&gt;org.apache.spark&lt;&#x2F;groupId&gt;</span><br><span class="line">    &lt;artifactId&gt;spark-sql_2.11&lt;&#x2F;artifactId&gt;</span><br><span class="line">    &lt;version&gt;$&#123;spark.version&#125;&lt;&#x2F;version&gt;</span><br><span class="line">&lt;&#x2F;dependency&gt;</span><br></pre></td></tr></table></figure>



<p>DataFrame 和 Dataset的关系在 源码中的体现</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">def toDF(): DataFrame &#x3D; new Dataset[Row](sparkSession, queryExecution, RowEncoder(schema))</span><br></pre></td></tr></table></figure>

<p>==A DataFrame is a DataSet organized into named columns==</p>
<p>DataSet API 只在 Scala 和 Java中能用, python 中用不了</p>
<p>Spark Session 是 spark DF/DS 编程的入口点<br>The entry point to programming Spark with the Dataset and DataFrame API</p>
<h3 id="二-SparkSQL-编程-POM-依赖"><a href="#二-SparkSQL-编程-POM-依赖" class="headerlink" title="二. SparkSQL 编程 POM 依赖"></a>二. SparkSQL 编程 POM 依赖</h3><p>IDEA 运行SparkSQL 需要加的依赖</p>
<ol>
<li></li>
</ol>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">&lt;dependency&gt;</span><br><span class="line">  &lt;groupId&gt;org.codehaus.janino&lt;&#x2F;groupId&gt;</span><br><span class="line">  &lt;artifactId&gt;janino&lt;&#x2F;artifactId&gt;</span><br><span class="line">  &lt;version&gt;3.0.8&lt;&#x2F;version&gt;</span><br><span class="line">&lt;&#x2F;dependency&gt;</span><br></pre></td></tr></table></figure>

<p>否则会报错:</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">Exception in thread &quot;main&quot; java.lang.NoClassDefFoundError: org&#x2F;codehaus&#x2F;janino&#x2F;InternalCompilerException</span><br></pre></td></tr></table></figure>



<ol start="2">
<li></li>
</ol>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">&lt;dependency&gt;</span><br><span class="line">  &lt;groupId&gt;org.apache.hive&lt;&#x2F;groupId&gt;</span><br><span class="line">  &lt;artifactId&gt;hive-exec&lt;&#x2F;artifactId&gt;</span><br><span class="line">  &lt;version&gt;$&#123;hive.version&#125;&lt;&#x2F;version&gt;</span><br><span class="line">&lt;&#x2F;dependency&gt;</span><br></pre></td></tr></table></figure>

<p>注意这个 hive.version 要写 ==1.2.1==, 如果选择 cdh 版本的hive 会报错</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">Exception in thread &quot;main&quot; java.lang.NoSuchFieldError: METASTORE_CLIENT_SOCKET_LIFETIME</span><br></pre></td></tr></table></figure>


<p>SparkSQL</p>
<p>show : 展示前20条数据</p>
<p>def show(): Unit = show(20)</p>
<p>如果执行中, 报 mysql 驱动找不到的 错误, 需要指定 mysql-connect 包</p>
<p>./spark-shell –jars mysql-connect-xxx.jar</p>
<p>show(3) //表示展示3条<br><img src="/2017/04/10/SparkSQL01/show1.png" alt="avatar"></p>
<p>def show(truncate: Boolean): Unit = show(20, truncate)<br>此处的truncate 表示 一个数据长度大于 20, 就把它截断, 展示的不完整<br>测试如下<br><img src="/2017/04/10/SparkSQL01/show2.png" alt="avatar"></p>
<p>由于 spark-shell 每次使用比较麻烦, spark 提供了 sql 接口<br>./spark-sql    </p>
<p>如果 报 mysql 连接不上的问题, 可按如下解决:<br>./spark-sql  <br>–jars  mysql-connect-xx.jar \   //虽然官方说 –jars 会在 driver和 executor端都加上驱动, 但是实际上driver端并没有加上, 需要通过下面参数指定<br>–driver-class-path  mysql-connect-xx.jar</p>
<p>进入 spark-sql 后, 就能像sql客户端一样使用了<br><img src="/2017/04/10/SparkSQL01/sparksql.png" alt="avatar"></p>
<h3 id="三-SparkSQL-编程规范"><a href="#三-SparkSQL-编程规范" class="headerlink" title="三. SparkSQL 编程规范"></a>三. SparkSQL 编程规范</h3><p>编程:<br>spark 入口及参数设置</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">val spark &#x3D; SparkSession.builder()</span><br><span class="line">  .master(&quot;local&quot;)</span><br><span class="line">  .appName(&quot;SparkSessionApp&quot;)</span><br><span class="line">  .getOrCreate()</span><br></pre></td></tr></table></figure>

<p>可以通过 .config 设置各种参数</p>
<p>读文本操作<br>第1中写法</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">val df: DataFrame &#x3D; spark.read.format(&quot;text&quot;).load(&quot;data&#x2F;people.txt&quot;)</span><br></pre></td></tr></table></figure>


<p>第2种写法</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">spark.read.text(&quot;data&#x2F;people.txt&quot;).show() &#x2F;&#x2F;read.text &#x3D;&#x3D; read.format(&quot;text&quot;).load(&quot;&quot;)</span><br><span class="line"></span><br><span class="line">def text(paths: String*): DataFrame &#x3D; format(&quot;text&quot;).load(paths : _*)</span><br></pre></td></tr></table></figure>


<p>第3种写法</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">val ds: Dataset[String] &#x3D; spark.read.textFile(&quot;data&#x2F;people.txt&quot;)</span><br><span class="line">    ds.show()</span><br></pre></td></tr></table></figure>

<p>可以传多个路径进去</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">def text(path: String): DataFrame &#x3D; &#123;</span><br><span class="line">  &#x2F;&#x2F; This method ensures that calls that explicit need single argument works, see SPARK-16009</span><br><span class="line">  text(Seq(path): _*)</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">spark.read.format(&quot;text&quot;)</span><br><span class="line">spark.read.textFile</span><br></pre></td></tr></table></figure>

<p>这两个的返回值是不同的，前者是 DataFrame，后面的是 DataSet，所以<br>前面是不能map的，比如加上rdd才可以，后面可以直接map</p>
<p>实例：读取 csv格式数据</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">val df &#x3D; spark.read.format(&quot;csv&quot;)</span><br><span class="line">  .option(&quot;timestampFormat&quot;, &quot;yyyy&#x2F;MM&#x2F;dd HH:mm:ss ZZ&quot;) &#x2F;&#x2F;指定 时间戳格式</span><br><span class="line">  .option(&quot;inferSchema&quot;, &quot;true&quot;)  &#x2F;&#x2F;内部推导Schema开启</span><br><span class="line">  .option(&quot;sep&quot;, &quot;,&quot;)  &#x2F;&#x2F;分隔符</span><br><span class="line">  .option(&quot;header&quot;, &quot;true&quot;) &#x2F;&#x2F; 把行首作为字段</span><br><span class="line">  .load(&quot;data&#x2F;user.csv&quot;)</span><br></pre></td></tr></table></figure>


<p>写数据, 如果路径存在报错<br><img src="/2017/04/10/SparkSQL01/error1.png" alt="avatar"></p>
<p>需要追加一个 写的模式</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">resultDS</span><br><span class="line">.write</span><br><span class="line">.mode(&quot;overwrite&quot;)</span><br><span class="line">.format(&quot;text&quot;)</span><br><span class="line">.save(&quot;out&quot;)</span><br><span class="line"></span><br><span class="line">resultDS.write.mode(SaveMode.Overwrite).format(&quot;text&quot;).save(&quot;out&quot;)</span><br></pre></td></tr></table></figure>


<p>此时执行还是报错<br><img src="/2017/04/10/SparkSQL01/error2.png" alt="avatar"></p>
<p>因为此时有两列,<br>(splits(0), splits(1))<br>改成一列, 输出正常,<br>怎么样可以输出多列?</p>
<p>压缩</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">option(&quot;compression&quot;,&quot;gzip&quot;)</span><br><span class="line">resultDS</span><br><span class="line">.write</span><br><span class="line">.option(&quot;compression&quot;,&quot;gzip&quot;).mode(&quot;overwrite&quot;)</span><br><span class="line">.format(&quot;text&quot;).save(&quot;out&quot;)</span><br></pre></td></tr></table></figure>

<p>如果是 lzo压缩会报错<br><img src="/2017/04/10/SparkSQL01/lzoerror.png" alt="avatar"></p>
<p>源码中 压缩格式如下<br><img src="/2017/04/10/SparkSQL01/compress1.png" alt="avatar"></p>
<p>setCodecConfiguration 的设置, 底层和 MR 一样的<br><img src="/2017/04/10/SparkSQL01/compression2.png" alt="avatar"></p>
<p>导入json 数据, 读取会变成如下错误</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">Exception in thread &quot;main&quot; java.lang.IllegalArgumentException: Illegal pattern component: XXX</span><br></pre></td></tr></table></figure>


<p>原因:<br>maven升级的时候，没有自动加载完整依赖包，jsonAPI对于timeStampFormat有特殊需求，默认为下面这个格式这种格式，是无法被scala-lang包识别的。我们看报错的源码可以看出。</p>
<p>解决<br>加上: option(“timestampFormat”, “yyyy/MM/dd HH:mm:ss ZZ”)</p>
<p>即</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">spark.read</span><br><span class="line">.option(&quot;timestampFormat&quot;, &quot;yyyy&#x2F;MM&#x2F;dd HH:mm:ss ZZ&quot;)</span><br><span class="line">.format(&quot;json&quot;)</span><br><span class="line">.load(&quot;data&#x2F;people.json&quot;)</span><br></pre></td></tr></table></figure>


<p>json/csv 的报错以上方法 都适用</p>

      
      <!-- reward -->
      
    </div>
    
    
      <!-- copyright -->
      
    <footer class="article-footer">
      
      
  <ul class="article-tag-list" itemprop="keywords"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/tedata/" rel="tag">tedata</a></li></ul>


    </footer>

  </div>

  

  
  
  

  

</article>
    
    <article id="post-HiveSQL1" class="article article-type-post" itemscope
  itemprop="blogPost" data-scroll-reveal>

  <div class="article-inner">
    
    <header class="article-header">
       
<h2 itemprop="name">
  <a class="article-title" href="/2017/03/25/HiveSQL1/"
    >Hive SQL 记录1</a
  >
</h2>
  

    </header>
    

    
    <div class="article-meta">
      <a href="/2017/03/25/HiveSQL1/" class="article-date">
  <time datetime="2017-03-25T12:43:27.000Z" itemprop="datePublished">2017-03-25</time>
</a>
      
      
      
      
    </div>
    

    

    
    <div class="article-entry" itemprop="articleBody">
      


      

      
      <h1 id="Hive-SQL-记录"><a href="#Hive-SQL-记录" class="headerlink" title="Hive SQL 记录"></a>Hive SQL 记录</h1><h3 id="案例一"><a href="#案例一" class="headerlink" title="案例一"></a>案例一</h3><p>表中有如下字段：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">domain           time     traffic(T)</span><br><span class="line">gifshow.com   2019&#x2F;01&#x2F;01    5</span><br><span class="line">yy.com        2019&#x2F;01&#x2F;01    4</span><br><span class="line">huya.com      2019&#x2F;01&#x2F;01    1</span><br><span class="line">gifshow.com   2019&#x2F;01&#x2F;20    6</span><br><span class="line">gifshow.com   2019&#x2F;02&#x2F;01    8</span><br><span class="line">yy.com        2019&#x2F;01&#x2F;20    5</span><br><span class="line">gifshow.com   2019&#x2F;02&#x2F;02    7</span><br></pre></td></tr></table></figure>

<p>需求是按月统计每个用户的累计访问量(只能用一个 SQL)，结果如下</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">domain          month     traffics   totals</span><br><span class="line">gifshow.com     2019-01      11         11</span><br><span class="line">gifshow.com     2019-02      15         26</span><br><span class="line">yy.com          2019-01       9         9</span><br><span class="line">huya.com        2019-01       1         1</span><br></pre></td></tr></table></figure>

<p>需求分析：</p>
<p>每个用户每月的访问量：group by 用户，月；然后再 sum<br>相同每月数据累加：<br>第一步的按月和用户统计：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">select domain,substr(time,1,7) ,sum(traffic) from domain_traffic group by domain,substr(time,1,7);</span><br></pre></td></tr></table></figure>


<p>结果输出如下：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">gifshow.com     2019&#x2F;01 11</span><br><span class="line">gifshow.com     2019&#x2F;02 15</span><br><span class="line">huya.com        2019&#x2F;01 1</span><br><span class="line">yy.com  2019&#x2F;01 9</span><br></pre></td></tr></table></figure>

<p>第二步，相同用户每月累加</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">select ms.domain,ms.m,ms.s,sum(ms.s)over(partition by ms.domain order by ms.m) from </span><br><span class="line">(select domain,substr(time,1,7) m ,sum(traffic) s from domain_traffic group by domain,substr(time,1,7)) ms;</span><br><span class="line">使用 sum()over() 函数实现累加功能，over 实现分组排序，sum 实现就有点意思：将本组内当前行以及之前的行全部相加(01之前没有最终只有01，02之前是01最终是01+02)。</span><br></pre></td></tr></table></figure>


<h3 id="案例二："><a href="#案例二：" class="headerlink" title="案例二："></a>案例二：</h3><p>表数据如下：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line">uid		pid</span><br><span class="line">li  	a</span><br><span class="line">zhang   b</span><br><span class="line">li  	a</span><br><span class="line">zhang   a</span><br><span class="line">wang    a</span><br><span class="line">john    a</span><br><span class="line">zhang   a</span><br><span class="line">wang    a</span><br><span class="line">tom 	b</span><br><span class="line">ao  	b</span><br><span class="line">wang    b</span><br><span class="line">tom 	b</span><br><span class="line">wang    b</span><br><span class="line">wang    b</span><br><span class="line">wang    b</span><br><span class="line">zhang   b</span><br><span class="line">zhang   b</span><br><span class="line">ao  	a</span><br></pre></td></tr></table></figure>

<ol>
<li>统计产品 UV</li>
<li>统计每个产品 top3 用户</li>
</ol>
<p>UV：按产品分组，count (uid 排重 )</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">select pid,count(distinct uid) from uid_pid group by pid;</span><br></pre></td></tr></table></figure>

<p>结果：</p>
<p>a       5<br>b       4<br>top3：</p>
<p>按产品、用户分组，count(1) 排序，limit 取 top (整体取 top)<br>select uid,pid,count(1) m from uid_pid group by uid,pid order by m desc limit 3</p>
<p>产品 top ：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">select t.uid,t.pid,t.m from (select s.uid,s.pid,s.m,row_number()over(partition by pid order by s.m desc) as rank from (select uid,pid,count(1) m from uid_pid group by uid,pid) s ) t where t.rank &lt;&#x3D; 3;</span><br></pre></td></tr></table></figure>


<p>输出</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">zhang   a       2</span><br><span class="line">wang    a       2</span><br><span class="line">li      a       2</span><br><span class="line">wang    b       4</span><br><span class="line">zhang   b       3</span><br><span class="line">tom     b       2</span><br></pre></td></tr></table></figure>

<p>使用 row_number()over 函数实现分组 Top。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">row_number()over() 作用就是分组排序并加上序号标记：over 中按 pid 分组，并按 次数 m 降序排列，row_numbe()</span><br></pre></td></tr></table></figure>
<p> 记录排序相当于增加了一列序号 rank。</p>
<p>总结<br>以上两个案例都用到 over 这个函数，我们从 MR 角度来讲解 over 作用。</p>
<h3 id="行列转换"><a href="#行列转换" class="headerlink" title="行列转换"></a>行列转换</h3><h3 id="行转列"><a href="#行转列" class="headerlink" title="行转列"></a>行转列</h3><p>将多行数据合并成某列</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">user_id	order_id</span><br><span class="line">104399	2105395</span><br><span class="line">104399	1715131</span><br><span class="line">104400	1609001</span><br><span class="line">104400	2986088</span><br><span class="line">104400	1795054</span><br><span class="line">select user_id,concat_ws(&#39;,&#39;,collect_set(order_id)) as order_value</span><br><span class="line">from table</span><br><span class="line">group by user_id ;</span><br></pre></td></tr></table></figure>

<p>结果输出</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">user_id	order_value</span><br><span class="line">104399	2105395,1715131</span><br><span class="line">104400	1609001,2986088,1795054</span><br></pre></td></tr></table></figure>

<p>按 user_id 分组后，每个user_id 都对应多个order_id；接着 collect_set 收集多个 order_id 并去重；最后由 concat_ws 指定分隔符将数组中的 order_id 合并成一个字符串</p>
<p>collect_list 不去重，collect_set 去重，但类型要求是 string</p>
<p>将多行数据转成多列</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">name	subject	score</span><br><span class="line">张三	语文	80</span><br><span class="line">张三	数学	90</span><br><span class="line">张三	英语	60</span><br></pre></td></tr></table></figure>


<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">select name,</span><br><span class="line">	max(case when subject &#x3D;&#39;语文&#39; then score else 0 end) as Chinese,</span><br><span class="line">	max(case when subject &#x3D;&#39;数学&#39; then score else 0 end) as Math,</span><br><span class="line">	max(case when subject &#x3D;&#39;英语&#39; then score else 0 end) as English</span><br><span class="line">from table</span><br><span class="line">group by name</span><br></pre></td></tr></table></figure>

<p>name    Chinese    Math    English<br>张三    80    90    60</p>
<h3 id="列转行"><a href="#列转行" class="headerlink" title="列转行"></a>列转行</h3><p>将某列数据扩展成多行</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">user_id	order_value</span><br><span class="line">104399	2105395,1715131</span><br><span class="line">104400	1609001,2986088,1795054</span><br><span class="line">select user_id,order_id</span><br><span class="line">from table</span><br><span class="line">lateral view explode(split(order_value,&#39;,&#39;)) t as order_id;</span><br></pre></td></tr></table></figure>

<p>结果输出</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">user_id	order_id</span><br><span class="line">104399	2105395</span><br><span class="line">104399	1715131</span><br><span class="line">104400	1609001</span><br><span class="line">104400	2986088</span><br><span class="line">104400	1795054</span><br><span class="line">lateral view explode(数组)</span><br></pre></td></tr></table></figure>
<p> 将数组中每个值都扩展成一行中列值。上面例子中，由 split 将 order_value 变成一个 order 数组，然后再由 explode 扩展成列。</p>
<p>将多列数据转成多行</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">name	Chinese	Math	English</span><br><span class="line">张三	80	90	60</span><br><span class="line"> select a.name,b.label,b.value</span><br><span class="line"> from (select * from table )a</span><br><span class="line">    lateral view explode(map(</span><br><span class="line">        &#39;语文&#39;,Chinese,</span><br><span class="line">        &#39;数学&#39;,Math,</span><br><span class="line">        &#39;英语&#39;,English</span><br><span class="line">    )) b as label ,value</span><br><span class="line">name	subject	score</span><br><span class="line">张三	语文	80</span><br><span class="line">张三	数学	90</span><br><span class="line">张三	英语	60</span><br></pre></td></tr></table></figure>

<p>留存<br>用户留存率一般是面向新增用户的概念，指某一天注册后的几天还是否活跃,是以每天为单位进行计算的。一般收到的需求都是一个时间段内的新增用户的几天留存。</p>
<p>根据留存的定义可知，我们需要求两个数<br>：新增和某日留存数，两者相除可得留存率。新增数很简单一般都会有标识，留存数需要有条件限定：假设求2018-05-18日的3日留存，先获取2018-05-18的新增 user_ids，然后判断当前活跃用户的 id 是否包含在 user_ids 且 ( 当前活跃的日期 - 2018-05-18 ) = 2。</p>
<p>/<em>计算某日新增登录设备的次日、3日、7日、14日、30日、90日留存率</em>/</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br></pre></td><td class="code"><pre><span class="line">SELECT</span><br><span class="line">	log_day &#39;日期&#39;,</span><br><span class="line">	count(user_id_d0) &#39;新增数量&#39;,</span><br><span class="line">	count(user_id_d1) &#x2F; count(user_id_d0) &#39;次日留存&#39;,</span><br><span class="line">	count(user_id_d3) &#x2F; count(user_id_d0) &#39;3日留存&#39;,</span><br><span class="line">	count(user_id_d7) &#x2F; count(user_id_d0) &#39;7日留存&#39;,</span><br><span class="line">	count(user_id_d14) &#x2F; count(user_id_d0) &#39;14日留存&#39;,</span><br><span class="line">	count(user_id_d30) &#x2F; count(user_id_d0) &#39;30日留存&#39;,</span><br><span class="line">	count(user_id_d90) &#x2F; count(user_id_d0) &#39;90日留存&#39;</span><br><span class="line">FROM</span><br><span class="line">	(</span><br><span class="line">		SELECT DISTINCT</span><br><span class="line">			log_day,</span><br><span class="line">			a.user_id_d0,</span><br><span class="line">			b.device_id AS user_id_d1,</span><br><span class="line">			c.device_id AS user_id_d3,</span><br><span class="line">			d.device_id AS user_id_d7,</span><br><span class="line">			e.device_id AS user_id_d14,</span><br><span class="line">			f.device_id AS user_id_d30,</span><br><span class="line">			g.device_id AS user_id_d90</span><br><span class="line">		FROM</span><br><span class="line">			(</span><br><span class="line">				SELECT DISTINCT</span><br><span class="line">					Date(event_time) AS log_day,</span><br><span class="line">					device_id AS user_id_d0</span><br><span class="line">				FROM</span><br><span class="line">					role_login_back</span><br><span class="line">				GROUP BY</span><br><span class="line">					device_id</span><br><span class="line">				ORDER BY</span><br><span class="line">					log_day</span><br><span class="line">			) a</span><br><span class="line">		LEFT JOIN role_login_back b ON DATEDIFF(DATE(b.event_time),a.log_day) &#x3D; 1</span><br><span class="line">		AND a.user_id_d0 &#x3D; b.device_id</span><br><span class="line">		LEFT JOIN role_login_back c ON DATEDIFF(DATE(c.event_time),a.log_day) &#x3D; 2</span><br><span class="line">		AND a.user_id_d0 &#x3D; c.device_id</span><br><span class="line">		LEFT JOIN role_login_back d ON DATEDIFF(DATE(d.event_time),a.log_day) &#x3D; 6</span><br><span class="line">		AND a.user_id_d0 &#x3D; d.device_id</span><br><span class="line">		LEFT JOIN role_login_back e ON DATEDIFF(DATE(e.event_time),a.log_day) &#x3D; 13</span><br><span class="line">		AND a.user_id_d0 &#x3D; e.device_id</span><br><span class="line">		LEFT JOIN role_login_back f ON DATEDIFF(DATE(f.event_time),a.log_day) &#x3D; 29</span><br><span class="line">		AND a.user_id_d0 &#x3D; f.device_id</span><br><span class="line">		LEFT JOIN role_login_back g ON DATEDIFF(DATE(g.event_time),a.log_day) &#x3D; 89</span><br><span class="line">		AND a.user_id_d0 &#x3D; g.device_id</span><br><span class="line">	) AS temp</span><br><span class="line">GROUP BY</span><br><span class="line">log_day</span><br></pre></td></tr></table></figure>

      
      <!-- reward -->
      
    </div>
    
    
      <!-- copyright -->
      
    <footer class="article-footer">
      
      
  <ul class="article-tag-list" itemprop="keywords"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/tedata/" rel="tag">tedata</a></li></ul>


    </footer>

  </div>

  

  
  
  

  

</article>
    
    <article id="post-hadooplzo" class="article article-type-post" itemscope
  itemprop="blogPost" data-scroll-reveal>

  <div class="article-inner">
    
    <header class="article-header">
       
<h2 itemprop="name">
  <a class="article-title" href="/2017/03/12/hadooplzo/"
    >Hadoop 支持 lzo压缩算法</a
  >
</h2>
  

    </header>
    

    
    <div class="article-meta">
      <a href="/2017/03/12/hadooplzo/" class="article-date">
  <time datetime="2017-03-12T04:00:00.000Z" itemprop="datePublished">2017-03-12</time>
</a>
      
      
      
      
    </div>
    

    

    
    <div class="article-entry" itemprop="articleBody">
      


      

      
      <h1 id="Hadoop-支持-lzo压缩算法"><a href="#Hadoop-支持-lzo压缩算法" class="headerlink" title="Hadoop 支持 lzo压缩算法"></a>Hadoop 支持 lzo压缩算法</h1><p>学习目标：</p>
<ol>
<li>安装lzo相关依赖</li>
<li>编译lzo</li>
<li>编译Hadoop-lzo</li>
<li>修改hadoop配置</li>
<li>准备数据</li>
<li>wordcount</li>
<li>文件添加index</li>
<li>安装lzo相关依赖</li>
</ol>
<h3 id="一-lzo-概念和优点"><a href="#一-lzo-概念和优点" class="headerlink" title="一. lzo 概念和优点"></a>一. lzo 概念和优点</h3><p>Hadoop经常用于处理大量的数据，如果期间的输出数据、中间数据能压缩存储，对系统的I/O性能会有提升。综合考虑压缩、解压速度、是否支持split，目前lzo是最好的选择。LZO（LZO是Lempel-Ziv-Oberhumer的缩写）是一种高压缩比和解压速度极快的编码，它的特点是解压缩速度非常快，无损压缩，压缩后的数据能准确还原，lzo是基于block分块的，允许数据被分解成chunk，能够被并行的解压。LZO库实现了许多有下述特点的算法：</p>
<p>　　（1）、解压简单，速度非常快。</p>
<p>　　（2）、解压不需要内存。</p>
<p>　　（3）、压缩相当地快。</p>
<p>　　（4）、压缩需要64 kB的内存。</p>
<p>　　（5）、允许在压缩部分以损失压缩速度为代价提高压缩率，解压速度不会降低。</p>
<p>　　（6）、包括生成预先压缩数据的压缩级别，这样可以得到相当有竞争力的压缩比。</p>
<p>　　（7）、另外还有一个只需要8 kB内存的压缩级别。</p>
<p>　　（8）、算法是线程安全的。</p>
<p>　　（9）、算法是无损的。</p>
<h3 id="二-安装lzo相关依赖"><a href="#二-安装lzo相关依赖" class="headerlink" title="二. 安装lzo相关依赖"></a>二. 安装lzo相关依赖</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">[root@JD ~]# yum install -y svn ncurses-devel</span><br><span class="line">[root@JD ~]# yum install -y gcc gcc-c++ make cmake</span><br><span class="line">[root@JD ~]# yum install -y openssl openssl-devel svn ncurses-devel zlib-devel libtool </span><br><span class="line">[root@JD ~]# yum install -y lzo lzo-devel lzop autoconf automake cmake</span><br></pre></td></tr></table></figure>


<h3 id="三-编译lzo"><a href="#三-编译lzo" class="headerlink" title="三. 编译lzo"></a>三. 编译lzo</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">[hadoop@JD ~]$ wget http:&#x2F;&#x2F;www.oberhumer.com&#x2F;opensource&#x2F;lzo&#x2F;download&#x2F;lzo-2.06.tar.gz</span><br><span class="line">[hadoop@JD ~]$ tar -zxvf lzo-2.06.tar.gz</span><br><span class="line">[hadoop@JD ~]$ cd lzo-2.06</span><br><span class="line">[hadoop@JD ~]$ export CFLAGS&#x3D;-m64</span><br><span class="line">[hadoop@JD ~]$ mkdir compile</span><br><span class="line">[hadoop@JD ~]$ .&#x2F;configure -enable-shared -prefix&#x3D;&#x2F;home&#x2F;hadoop&#x2F;app&#x2F;lzo-2.06&#x2F;compile</span><br><span class="line">[hadoop@JD ~]$ make &amp;&amp;  make install</span><br></pre></td></tr></table></figure>


<h3 id="三-编译Hadoop-lzo"><a href="#三-编译Hadoop-lzo" class="headerlink" title="三 编译Hadoop-lzo"></a>三 编译Hadoop-lzo</h3><p>下载源码</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">wget https:&#x2F;&#x2F;github.com&#x2F;twitter&#x2F;hadoop-lzo&#x2F;archive&#x2F;master.zip</span><br><span class="line">解压</span><br><span class="line"></span><br><span class="line">[hadoop@JD software]$ unzip -d ~&#x2F;app&#x2F; hadoop-lzo-master.zip</span><br></pre></td></tr></table></figure>


<p>进入解压后的目录</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[hadoop@JD app]$ cd hadoop-lzo-master&#x2F;</span><br><span class="line">[hadoop@JD hadoop-lzo-master]$</span><br></pre></td></tr></table></figure>


<p>修改此目录下pom.xml文件</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">&lt;repositories&gt;</span><br><span class="line">	#添加cloudera仓库</span><br><span class="line">    &lt;repository&gt;</span><br><span class="line">     &lt;id&gt;cloudera&lt;&#x2F;id&gt;</span><br><span class="line">     &lt;url&gt;https:&#x2F;&#x2F;repository.cloudera.com&#x2F;artifactory&#x2F;cloudera-repos&lt;&#x2F;url&gt;</span><br><span class="line">    &lt;&#x2F;repository&gt;</span><br><span class="line">  &lt;&#x2F;repositories&gt;</span><br><span class="line">&lt;properties&gt;</span><br><span class="line">    &lt;project.build.sourceEncoding&gt;UTF-8&lt;&#x2F;project.build.sourceEncoding&gt;</span><br><span class="line">    #因为用的是cdh的</span><br><span class="line">    &lt;hadoop.current.version&gt;2.6.0-cdh5.15.1&lt;&#x2F;hadoop.current.version&gt;</span><br><span class="line">    &lt;hadoop.old.version&gt;1.0.4&lt;&#x2F;hadoop.old.version&gt;</span><br><span class="line">&lt;&#x2F;properties&gt;</span><br></pre></td></tr></table></figure>


<p>添加环境变量</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[hadoop@JD hadoop-lzo-master]$ export CFLAGS&#x3D;-m64</span><br><span class="line">[hadoop@JD hadoop-lzo-master]$ export CXXFLAGS&#x3D;-m64</span><br></pre></td></tr></table></figure>



<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">#Modify the actual path for your hadoop</span><br><span class="line">[hadoop@JD hadoop-lzo-master]$ export C_INCLUDE_PATH&#x3D;&#x2F;home&#x2F;hadoop&#x2F;app&#x2F;lzo-2.06&#x2F;compile&#x2F;include</span><br><span class="line">[hadoop@JD hadoop-lzo-master]$ export LIBRARY_PATH&#x3D;&#x2F;home&#x2F;hadoop&#x2F;app&#x2F;lzo-2.06&#x2F;compile&#x2F;lib</span><br></pre></td></tr></table></figure>


<p>编译源码：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">mvn clean package -DskipTests</span><br></pre></td></tr></table></figure>
<p><img src="/2017/03/12/hadooplzo/hl1.png" alt="avatar"></p>
<p>进入target/native/Linux-amd64-64</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">[hadoop@JD hadoop-lzo-master]$ cd target&#x2F;native&#x2F;Linux-amd64-64&#x2F;</span><br><span class="line">[hadoop@JD Linux-amd64-64]$ mkdir ~&#x2F;app&#x2F;hadoop-lzo-files</span><br><span class="line">[hadoop@JD Linux-amd64-64]$ tar -cBf - -C lib . | tar -xBvf - -C ~&#x2F;app&#x2F;hadoop-lzo-files</span><br></pre></td></tr></table></figure>

<p>拷贝文件</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[hadoop@hadoop-01 hadoop-lzo-files]$ cp ~&#x2F;app&#x2F;hadoop-lzo-files&#x2F;libgplcompression* $HADOOP_HOME&#x2F;lib&#x2F;native&#x2F;</span><br></pre></td></tr></table></figure>


<h3 id="四-修改hadoop配置"><a href="#四-修改hadoop配置" class="headerlink" title="四 修改hadoop配置"></a>四 修改hadoop配置</h3><p>vi core-site.xml</p>
<p>修改core-site.xml的配置文件</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">    &lt;property&gt;</span><br><span class="line">        &lt;name&gt;io.compression.codecs&lt;&#x2F;name&gt;</span><br><span class="line">        &lt;value&gt;org.apache.hadoop.io.compress.GzipCodec,</span><br><span class="line">               org.apache.hadoop.io.compress.DefaultCodec,</span><br><span class="line">               org.apache.hadoop.io.compress.BZip2Codec,</span><br><span class="line">               org.apache.hadoop.io.compress.SnappyCodec,</span><br><span class="line">               com.hadoop.compression.lzo.LzoCodec,</span><br><span class="line">               com.hadoop.compression.lzo.LzopCodec</span><br><span class="line">        &lt;&#x2F;value&gt;</span><br><span class="line">    &lt;&#x2F;property&gt;</span><br><span class="line"></span><br><span class="line">  </span><br><span class="line"></span><br><span class="line">&lt;property&gt;</span><br><span class="line">      &lt;name&gt;io.compression.codec.lzo.class&lt;&#x2F;name&gt;</span><br><span class="line">      &lt;value&gt;com.hadoop.compression.lzo.LzoCodec&lt;&#x2F;value&gt;</span><br><span class="line">  &lt;&#x2F;property&gt;</span><br></pre></td></tr></table></figure>

<p>修改mapred-site.xml配置文件</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">&lt;property&gt;</span><br><span class="line">  &lt;name&gt;mapred.compress.map.output&lt;&#x2F;name&gt;</span><br><span class="line">  &lt;value&gt;true&lt;&#x2F;value&gt;</span><br><span class="line">&lt;&#x2F;property&gt;</span><br><span class="line"> </span><br><span class="line">&lt;property&gt;</span><br><span class="line">    &lt;name&gt;mapred.map.output.compression.codec&lt;&#x2F;name&gt;</span><br><span class="line">    &lt;value&gt;com.hadoop.compression.lzo.LzoCodec&lt;&#x2F;value&gt;</span><br><span class="line">&lt;&#x2F;property&gt;</span><br><span class="line"> </span><br><span class="line">&lt;property&gt;</span><br><span class="line">    &lt;name&gt;mapred.child.env&lt;&#x2F;name&gt;</span><br><span class="line">    &lt;value&gt;LD_LIBRARY_PATH&#x3D;&#x2F;usr&#x2F;local&#x2F;hadoop&#x2F;lzo&#x2F;lib&lt;&#x2F;value&gt;</span><br><span class="line">&lt;&#x2F;property&gt;</span><br></pre></td></tr></table></figure>

<h3 id="五-准备数据"><a href="#五-准备数据" class="headerlink" title="五 准备数据"></a>五 准备数据</h3><p>准备一个753M的数据<br><img src="/2017/03/12/hadooplzo/hl2.png" alt="avatar"></p>
<p>然后压缩此文件<br><img src="/2017/03/12/hadooplzo/hl3.png" alt="avatar"></p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">lzop -f access.txt</span><br></pre></td></tr></table></figure>



<h3 id="六-wordcount"><a href="#六-wordcount" class="headerlink" title="六 wordcount"></a>六 wordcount</h3><p>首先把数据上传到hdfs</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hdfs dfs -put access.txt.lzo &#x2F;lzo-data&#x2F;input</span><br></pre></td></tr></table></figure>


<p>计算wordcount</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">hadoop jar \</span><br><span class="line">&#x2F;home&#x2F;hadoop&#x2F;app&#x2F;hadoop&#x2F;share&#x2F;hadoop&#x2F;mapreduce&#x2F;hadoop-mapreduce-examples-2.6.0-cdh5.15.1.jar \</span><br><span class="line">wordcount \</span><br><span class="line">-Dmapreduce.job.inputformat.class&#x3D;com.hadoop.mapreduce.LzoTextInputFormat \</span><br><span class="line">&#x2F;lzo-data&#x2F;input&#x2F;access.txt.lzo \</span><br><span class="line">&#x2F;lzo-data&#x2F;output3</span><br></pre></td></tr></table></figure>

<p>从下图可以看出，没有分片<br><img src="/2017/03/12/hadooplzo/hl5.png" alt="avatar"></p>
<h3 id="七-文件添加index"><a href="#七-文件添加index" class="headerlink" title="七 文件添加index"></a>七 文件添加index</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">hadoop jar \</span><br><span class="line">&#x2F;home&#x2F;hadoop&#x2F;app&#x2F;hadoop-lzo-master&#x2F;target&#x2F;hadoop-lzo-0.4.21-SNAPSHOT.jar \</span><br><span class="line">com.hadoop.compression.lzo.DistributedLzoIndexer  \</span><br><span class="line">&#x2F;lzo-data&#x2F;input&#x2F;access.txt.lzo</span><br></pre></td></tr></table></figure>



<p>再次计算wordcount</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">hadoop jar \</span><br><span class="line">&#x2F;home&#x2F;hadoop&#x2F;app&#x2F;hadoop&#x2F;share&#x2F;hadoop&#x2F;mapreduce&#x2F;hadoop-mapreduce-examples-2.6.0-cdh5.15.1.jar \</span><br><span class="line">wordcount \</span><br><span class="line">-Dmapreduce.job.inputformat.class&#x3D;com.hadoop.mapreduce.LzoTextInputFormat \</span><br><span class="line">&#x2F;lzo-data&#x2F;input&#x2F;access.txt.lzo \</span><br><span class="line">&#x2F;lzo-data&#x2F;output4</span><br></pre></td></tr></table></figure>


<p>从下图，我们可以看出分成3个<br><img src="/2017/03/12/hadooplzo/hl7.png" alt="avatar"></p>
<p>参考文章：</p>
<p><a href="https://www.iteblog.com/archives/992.html" target="_blank" rel="noopener">https://www.iteblog.com/archives/992.html</a></p>

      
      <!-- reward -->
      
    </div>
    
    
      <!-- copyright -->
      
    <footer class="article-footer">
      
      
  <ul class="article-tag-list" itemprop="keywords"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/tedata/" rel="tag">tedata</a></li></ul>


    </footer>

  </div>

  

  
  
  

  

</article>
    
    <article id="post-SparkCore03" class="article article-type-post" itemscope
  itemprop="blogPost" data-scroll-reveal>

  <div class="article-inner">
    
    <header class="article-header">
       
<h2 itemprop="name">
  <a class="article-title" href="/2017/03/10/SparkCore03/"
    >SparkCore 03</a
  >
</h2>
  

    </header>
    

    
    <div class="article-meta">
      <a href="/2017/03/10/SparkCore03/" class="article-date">
  <time datetime="2017-03-10T04:00:00.000Z" itemprop="datePublished">2017-03-10</time>
</a>
      
      
      
      
    </div>
    

    

    
    <div class="article-entry" itemprop="articleBody">
      


      

      
      <h1 id="Spark-Core-03"><a href="#Spark-Core-03" class="headerlink" title="Spark Core 03"></a>Spark Core 03</h1><p>学习目标：</p>
<ol>
<li>RDD 的依赖</li>
<li>Persist/Cache</li>
<li>repartition/coalesce</li>
</ol>
<p>一.  RDD 的依赖</p>
<ol>
<li>先看一个例子</li>
</ol>
<p>执行一个 wc</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">def main(args: Array[String]): Unit &#x3D; &#123;</span><br><span class="line">  val sc &#x3D; ContextUtils.getContext(this.getClass.getSimpleName)</span><br><span class="line"></span><br><span class="line">  val data &#x3D; Array(&quot;hadoop hbase scala&quot;, &quot;hadoop hive scala&quot;, &quot;hadoop spark hive&quot;)</span><br><span class="line">  val input &#x3D; sc.parallelize(data)</span><br><span class="line">  input.flatMap(_.split(&quot; &quot;)).map((_,1)).reduceByKey(_+_).foreach(println)</span><br><span class="line"></span><br><span class="line">  sc.stop()</span><br><span class="line"></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>看下 spark ui 的DAG 图<br>![avatar](/Spark Core03/stageDAG.png)</p>
<p>可以看到，此时产生了两个 Stage：Stage0，Stage1</p>
<p>stage0 产生的是 reduceByKey 之前的DAG<br>![avatar](/Spark Core03/stage0.png)</p>
<p>stage1 产生的是<br>![avatar](/Spark Core03/stage1.png)</p>
<p>可以看到，在reduceByKey 处产生了 ShuffleRDD，stage 一分为二</p>
<ol start="2">
<li><p>什么是ShuffleRDD？<br>所谓 shuffle，指的是相同key的 元素聚合到一起的过程，这个过程因为可能存在跨主机，跨机架，所以是一个开销非常大的操作</p>
</li>
<li><p>图解 RDD 的依赖<br>先根据上面的例子，画一张图<br>![avatar](/Spark Core03/wcDep.png)</p>
</li>
</ol>
<p>看上图，从 flatMap –》Map，Map –》 Combiner，每个父RDD仅被子RDD 使用一次<br>这种依赖称为窄依赖</p>
<p>ReduceByKey中，父RDD 被 子RDD 使用多次，这种依赖称为 宽依赖</p>
<p>一般：map，fliter，union 这些操作都是窄依赖<br>    reduceByKey，groupByKey，countByKey这些操作都是宽依赖</p>
<p>二.  Persist/ Cache<br>persist/cache 指的是把数据缓存起来，下次调用的时候可以直接使用，而不用去再计算生成</p>
<p>persist 有四个参数</p>
<p>UseDisk：是否使用磁盘</p>
<p>UseMemory：是否使用内存</p>
<p>UseOffHeap：是否使用对外内存</p>
<p>Deserialization：是否不使用序列化</p>
<p>比如：<br>MEMORY_ONLY 的设置就是：(false, true, false, true)， 即只使用内存<br>MEMORY_AND_SER 就是：(fasle, true, false, false) 即使用内存并且序列化</p>
<p>persist/cache 的区别？<br>cache 是 persist 的特列，即是 MEMORY_ONLY 这种情况</p>
<p>怎么选择persist？</p>
<ol>
<li>优先选择 MEMORY_ONLY, 这也是spark的默认设置</li>
<li>如果1 不能满足，就选择 MEMORY_AND_SER，但要注意，序列化会增加 CPU的开销</li>
<li>不要选择 disk的方式，这种方式还不如重新算一遍</li>
</ol>
<p>三. repartition/coalease</p>
<p>repartition: 重新分区</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">scala&gt; inputre.mapPartitionsWithIndex((index, partition) &#x3D;&gt; &#123;</span><br><span class="line">     |       partition.map( x &#x3D;&gt; &#123;</span><br><span class="line">     |         println(s&quot;$index, $x&quot;)</span><br><span class="line">     |       &#125;)</span><br><span class="line">     |     &#125;).collect</span><br><span class="line">0, hadoop spark hive</span><br><span class="line">1, hadoop hbase scala</span><br><span class="line">2, hadoop hive scala</span><br><span class="line">res7: Array[Unit] &#x3D; Array((), (), ())</span><br></pre></td></tr></table></figure>



<p>colaease：减小分区</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">scala&gt; inputce.partition</span><br><span class="line">partitioner   partitions</span><br><span class="line"></span><br><span class="line">scala&gt; inputce.partitions.size</span><br><span class="line">res10: Int &#x3D; 2</span><br></pre></td></tr></table></figure>


<p>默认情况下，coalesce 是不能增大分区的，除非在引用方法时在分区数后面加上 true</p>
<p>不加 true，partition 最多增大到本来的分区数</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">scala&gt; val inputce &#x3D; inputre.coalesce(4)</span><br><span class="line">inputce: org.apache.spark.rdd.RDD[String] &#x3D; CoalescedRDD[15] at coalesce at &lt;console&gt;:25</span><br><span class="line"></span><br><span class="line">scala&gt; inputce.partitions.size</span><br><span class="line">res11: Int &#x3D; 3</span><br></pre></td></tr></table></figure>

<p>加上true，分区数可以增加到指定数目</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">scala&gt; val inputce &#x3D; inputre.coalesce(4, true)</span><br><span class="line">inputce: org.apache.spark.rdd.RDD[String] &#x3D; MapPartitionsRDD[20] at coalesce at &lt;console&gt;:25</span><br><span class="line"></span><br><span class="line">scala&gt; inputce.partitions.size</span><br><span class="line">res13: Int &#x3D; 4</span><br></pre></td></tr></table></figure>


      
      <!-- reward -->
      
    </div>
    
    
      <!-- copyright -->
      
    <footer class="article-footer">
      
      
  <ul class="article-tag-list" itemprop="keywords"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/tedata/" rel="tag">tedata</a></li></ul>


    </footer>

  </div>

  

  
  
  

  

</article>
    
    <article id="post-SparkCore 02" class="article article-type-post" itemscope
  itemprop="blogPost" data-scroll-reveal>

  <div class="article-inner">
    
    <header class="article-header">
       
<h2 itemprop="name">
  <a class="article-title" href="/2017/03/07/SparkCore%2002/"
    >Spark Core 第二课</a
  >
</h2>
  

    </header>
    

    
    <div class="article-meta">
      <a href="/2017/03/07/SparkCore%2002/" class="article-date">
  <time datetime="2017-03-07T12:43:27.000Z" itemprop="datePublished">2017-03-07</time>
</a>
      
      
      
      
    </div>
    

    

    
    <div class="article-entry" itemprop="articleBody">
      


      

      
      <p>Spark Core02</p>
<p>学习目标：</p>
<ol>
<li>RDD 的 Action 算子</li>
<li>关于RDD 的一些操作</li>
<li>排序的实现</li>
<li>Spark 关键术语</li>
</ol>
<p>一. RDD 的Action 算子</p>
<ol>
<li><p>foreach</p>
</li>
<li><p>foreachPartition</p>
</li>
<li><p>countByKey</p>
</li>
</ol>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">scala&gt; rdd1.map((_,1)).countByKey</span><br><span class="line">res10: scala.collection.Map[Int,Long] &#x3D; Map(5 -&gt; 1, 1 -&gt; 1, 6 -&gt; 1, 2 -&gt; 1, 3 -&gt; 1, 4 -&gt; 1)</span><br></pre></td></tr></table></figure>


<ol start="4">
<li>collectAsMap</li>
</ol>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">scala&gt; rdd1.map((_,1)).collectAsMap</span><br><span class="line">res14: scala.collection.Map[Int,Int] &#x3D; Map(2 -&gt; 1, 5 -&gt; 1, 4 -&gt; 1, 1 -&gt; 1, 3 -&gt; 1, 6 -&gt; 1)</span><br></pre></td></tr></table></figure>



<p>二. 关于RDD 的一些操作</p>
<ol>
<li>rdd.count</li>
<li>rdd.partitions.size</li>
<li>rdd.first</li>
<li>rdd.top(n)</li>
<li>rdd.takeOrder(n)</li>
</ol>
<p>三.  排序实现</p>
<ol>
<li>算子实现</li>
</ol>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">def main(args: Array[String]): Unit &#x3D; &#123;</span><br><span class="line"></span><br><span class="line">    val sc &#x3D; ContextUtils.getContext(this.getClass.getSimpleName)</span><br><span class="line"></span><br><span class="line">    val rdd1 &#x3D; sc.parallelize(List(&quot;iphone11 7000 20&quot;,&quot;hwpro30 5000 100&quot;,&quot;xiaomi 3000 200&quot;,&quot;sumsung 6000 1000&quot;))</span><br><span class="line"></span><br><span class="line">rdd1.map( x &#x3D;&gt; &#123;</span><br><span class="line">val splits &#x3D; x.split(&quot; &quot;)</span><br><span class="line">  (splits(0), splits(1).trim.toInt, splits(2).trim.toInt)</span><br><span class="line">&#125;).sortBy(_._2).foreach(println)</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>


<ol start="2">
<li>继承 Ordered 类实现</li>
</ol>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">  rdd1.map( x &#x3D;&gt; &#123;</span><br><span class="line">      val splits &#x3D; x.split(&quot; &quot;)</span><br><span class="line">      val product &#x3D; splits(0)</span><br><span class="line">      val price &#x3D; splits(1).toInt</span><br><span class="line">      val amount &#x3D; splits(2).toInt</span><br><span class="line">      MyProduct(product, price, amount)</span><br><span class="line">    &#125;).sortBy(x &#x3D;&gt; x ).foreach(println)</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">case class MyProduct(product: String, price: Int, amount: Int) extends Ordered[MyProduct] &#123;</span><br><span class="line">  override def compare(that: MyProduct): Int &#x3D; &#123;</span><br><span class="line">    -(this.amount - that.amount)</span><br><span class="line">  &#125;</span><br></pre></td></tr></table></figure>


<ol start="3">
<li>隐式转换</li>
</ol>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">implicit def myproduct2orderproduct(myproduct: MyProduct2) &#x3D; new Ordered[MyProduct2] &#123;</span><br><span class="line">      override def compare(that: MyProduct2): Int &#x3D; &#123;</span><br><span class="line">        myproduct.amount - that.amount</span><br><span class="line">      &#125;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    rdd1.map( x &#x3D;&gt; &#123;</span><br><span class="line">      val splits &#x3D; x.split(&quot; &quot;)</span><br><span class="line">      MyProduct2(splits(0).trim, splits(1).trim.toInt, splits(2).trim.toInt)</span><br><span class="line">    &#125;).sortBy(y &#x3D;&gt; y)</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">case class MyProduct2(product: String, price: Int, amount: Int)</span><br></pre></td></tr></table></figure>


<p>四. 关键术语</p>
<h5 id="Application"><a href="#Application" class="headerlink" title="Application:"></a>Application:</h5><p>构建在spark上的应用程序, 包含一个 driver + 多个 executor</p>
<h5 id="Application-jar"><a href="#Application-jar" class="headerlink" title="Application jar:"></a>Application jar:</h5><p>包含 user的Spark Application, . User Jar 不允许包含 Hadoop/Spark的 lib包,然而, 他们可以在运行时候添加进去</p>
<h5 id="Driver-Program"><a href="#Driver-Program" class="headerlink" title="Driver Program:"></a>Driver Program:</h5><p>main 方法, 包含一个 sc, 一个应用程序里面有driver</p>
<h5 id="Cluster-Manager"><a href="#Cluster-Manager" class="headerlink" title="Cluster Manager:"></a>Cluster Manager:</h5><h6 id="Deploy-Mode"><a href="#Deploy-Mode" class="headerlink" title="Deploy Mode:"></a>Deploy Mode:</h6><p>区分driver模式跑在哪里</p>
<p>YARN: RM NM(container)</p>
<p>cluster: Driver 跑在container</p>
<p>client: Driver就运行在你提交的机器的本地</p>
<p>Executor:</p>
<p>process</p>
<p>run tasks</p>
<p>keep data in memory or disk storage across them Each application has its own executors 对应于YARN 上的 container </p>
<h5 id="Task"><a href="#Task" class="headerlink" title="Task"></a>Task</h5><p>发送到 executor上 运行  基本单元</p>
<h5 id="RDD"><a href="#RDD" class="headerlink" title="RDD:"></a>RDD:</h5><p>partitions , 每一个 partition 对应一个 Task</p>
<h5 id="Job"><a href="#Job" class="headerlink" title="Job:"></a>Job:</h5><p>只要遇到 action, 就产生 job</p>
<h5 id="Stage"><a href="#Stage" class="headerlink" title="Stage:"></a>Stage:</h5><p>一组 tasks的集合</p>
<ul>
<li>一个 application: 1到n个 job</li>
<li>一个 Job: 1到 n 个 stage</li>
<li>一个 Stage: 1到n个 task, task与 partition 一一对应 </li>
</ul>
<p>Executor<br>是一个较大的概念<br>对应于YARN 是 Container<br>对应 Master-Slave 就是 Worker Node</p>
<p>Spark application run as independent sets of process on cluster: 指的就是 executor</p>

      
      <!-- reward -->
      
    </div>
    
    
      <!-- copyright -->
      
    <footer class="article-footer">
      
      
  <ul class="article-tag-list" itemprop="keywords"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/tedata/" rel="tag">tedata</a></li></ul>


    </footer>

  </div>

  

  
  
  

  

</article>
    
  </article>
  

  
  <nav class="page-nav">
    
    <a class="extend prev" rel="prev" href="/">上一页</a><a class="page-number" href="/">1</a><span class="page-number current">2</span><a class="page-number" href="/page/3/">3</a><a class="extend next" rel="next" href="/page/3/">下一页</a>
  </nav>
  
</section>
</div>

      <footer class="footer">
  <div class="outer">
    <ul class="list-inline">
      <li>
        &copy;
        2015-2020
        tedwang0714
      </li>
      <li>
        
          Powered by
        
        
        <a href="https://hexo.io" target="_blank">Hexo</a> Theme <a href="https://github.com/Shen-Yu/hexo-theme-ayer" target="_blank">Ayer</a>
        
      </li>
    </ul>
    <ul class="list-inline">
      <li>
        
        
        <span>
  <i>PV:<span id="busuanzi_value_page_pv"></span></i>
  <i>UV:<span id="busuanzi_value_site_uv"></span></i>
</span>
        
      </li>
      <li>
        <!-- cnzz统计 -->
        
        <script type="text/javascript" src='https://s9.cnzz.com/z_stat.php?id=1278069914&amp;web_id=1278069914'></script>
        
      </li>
    </ul>
  </div>
</footer>
    <div class="to_top">
        <div class="totop" id="totop">
  <i class="ri-arrow-up-line"></i>
</div>
      </div>
    </main>
      <aside class="sidebar">
        <button class="navbar-toggle"></button>
<nav class="navbar">
  
  <div class="logo">
    <a href="/"><img src="/images/xingji.png" alt="TedWang 的大数据"></a>
  </div>
  
  <ul class="nav nav-main">
    
    <li class="nav-item">
      <a class="nav-item-link" href="/index.html">主页</a>
    </li>
    
    <li class="nav-item">
      <a class="nav-item-link" href="/archives">归档</a>
    </li>
    
    <li class="nav-item">
      <a class="nav-item-link" href="/categories">分类</a>
    </li>
    
    <li class="nav-item">
      <a class="nav-item-link" href="/tags/">标签</a>
    </li>
    
  </ul>
</nav>
<nav class="navbar navbar-bottom">
  <ul class="nav">
    <li class="nav-item">
      
      <a class="nav-item-link nav-item-search"  title="Search">
        <i class="ri-search-line"></i>
      </a>
      
      
      <a class="nav-item-link" target="_blank" href="/atom.xml" title="RSS Feed">
        <i class="ri-rss-line"></i>
      </a>
      
    </li>
  </ul>
</nav>
<div class="search-form-wrap">
  <div class="local-search local-search-plugin">
  <input type="search" id="local-search-input" class="local-search-input" placeholder="Search...">
  <div id="local-search-result" class="local-search-result"></div>
</div>
</div>
      </aside>
      <div id="mask"></div>

<!-- #reward -->
<div id="reward">
  <span class="close"><i class="ri-close-line"></i></span>
  <p class="reward-p"><i class="ri-cup-line"></i>请我喝杯咖啡吧~</p>
  <div class="reward-box">
    
    <div class="reward-item">
      <img class="reward-img" src="/images/alipay.jpg">
      <span class="reward-type">支付宝</span>
    </div>
    
    
    <div class="reward-item">
      <img class="reward-img" src="/images/wechat.jpg">
      <span class="reward-type">微信</span>
    </div>
    
  </div>
</div>
      
<script src="/js/jquery-2.0.3.min.js"></script>


<script src="/js/jquery.justifiedGallery.min.js"></script>


<script src="/js/lazyload.min.js"></script>


<script src="/js/busuanzi-2.3.pure.min.js"></script>


<script src="/js/share.js"></script>



<script src="/fancybox/jquery.fancybox.min.js"></script>




<script>
  try {
    var typed = new Typed("#subtitle", {
    strings: ['人类精神必须置于技术之上','',''],
    startDelay: 0,
    typeSpeed: 200,
    loop: true,
    backSpeed: 100,
    showCursor: true
    });
  } catch (err) {
  }
  
</script>




<script>
  var ayerConfig = {
    mathjax: false
  }
</script>


<script src="/js/ayer.js"></script>


<script src="https://cdn.jsdelivr.net/npm/jquery-modal@0.9.2/jquery.modal.min.js"></script>
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/jquery-modal@0.9.2/jquery.modal.min.css">


<!-- Root element of PhotoSwipe. Must have class pswp. -->
<div class="pswp" tabindex="-1" role="dialog" aria-hidden="true">

    <!-- Background of PhotoSwipe. 
         It's a separate element as animating opacity is faster than rgba(). -->
    <div class="pswp__bg"></div>

    <!-- Slides wrapper with overflow:hidden. -->
    <div class="pswp__scroll-wrap">

        <!-- Container that holds slides. 
            PhotoSwipe keeps only 3 of them in the DOM to save memory.
            Don't modify these 3 pswp__item elements, data is added later on. -->
        <div class="pswp__container">
            <div class="pswp__item"></div>
            <div class="pswp__item"></div>
            <div class="pswp__item"></div>
        </div>

        <!-- Default (PhotoSwipeUI_Default) interface on top of sliding area. Can be changed. -->
        <div class="pswp__ui pswp__ui--hidden">

            <div class="pswp__top-bar">

                <!--  Controls are self-explanatory. Order can be changed. -->

                <div class="pswp__counter"></div>

                <button class="pswp__button pswp__button--close" title="Close (Esc)"></button>

                <button class="pswp__button pswp__button--share" style="display:none" title="Share"></button>

                <button class="pswp__button pswp__button--fs" title="Toggle fullscreen"></button>

                <button class="pswp__button pswp__button--zoom" title="Zoom in/out"></button>

                <!-- Preloader demo http://codepen.io/dimsemenov/pen/yyBWoR -->
                <!-- element will get class pswp__preloader--active when preloader is running -->
                <div class="pswp__preloader">
                    <div class="pswp__preloader__icn">
                        <div class="pswp__preloader__cut">
                            <div class="pswp__preloader__donut"></div>
                        </div>
                    </div>
                </div>
            </div>

            <div class="pswp__share-modal pswp__share-modal--hidden pswp__single-tap">
                <div class="pswp__share-tooltip"></div>
            </div>

            <button class="pswp__button pswp__button--arrow--left" title="Previous (arrow left)">
            </button>

            <button class="pswp__button pswp__button--arrow--right" title="Next (arrow right)">
            </button>

            <div class="pswp__caption">
                <div class="pswp__caption__center"></div>
            </div>

        </div>

    </div>

</div>

<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/photoswipe@4.1.3/dist/photoswipe.min.css">
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/photoswipe@4.1.3/dist/default-skin/default-skin.css">
<script src="https://cdn.jsdelivr.net/npm/photoswipe@4.1.3/dist/photoswipe.min.js"></script>
<script src="https://cdn.jsdelivr.net/npm/photoswipe@4.1.3/dist/photoswipe-ui-default.min.js"></script>

<script>
    function viewer_init() {
        let pswpElement = document.querySelectorAll('.pswp')[0];
        let $imgArr = document.querySelectorAll(('.article-entry img:not(.reward-img)'))

        $imgArr.forEach(($em, i) => {
            $em.onclick = () => {
                // slider展开状态
                // todo: 这样不好，后面改成状态
                if (document.querySelector('.left-col.show')) return
                let items = []
                $imgArr.forEach(($em2, i2) => {
                    let img = $em2.getAttribute('data-idx', i2)
                    let src = $em2.getAttribute('data-target') || $em2.getAttribute('src')
                    let title = $em2.getAttribute('alt')
                    // 获得原图尺寸
                    const image = new Image()
                    image.src = src
                    items.push({
                        src: src,
                        w: image.width || $em2.width,
                        h: image.height || $em2.height,
                        title: title
                    })
                })
                var gallery = new PhotoSwipe(pswpElement, PhotoSwipeUI_Default, items, {
                    index: parseInt(i)
                });
                gallery.init()
            }
        })
    }
    viewer_init()
</script>




<script type="text/javascript" src="https://js.users.51.la/20544303.js"></script>
  </div>
</body>

</html>