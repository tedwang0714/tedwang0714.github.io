<!DOCTYPE html>
<html lang="en">

<head>
  <meta charset="utf-8" />
    
  <meta name="description" content="大数据学习" />
  
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1" />
  <title>
     TedWang 的大数据
  </title>
  <meta name="generator" content="hexo-theme-yilia-plus">
  
  <link rel="shortcut icon" href="/favicon.ico" />
  
  
<link rel="stylesheet" href="/css/style.css">

  
<script src="/js/pace.min.js"></script>


  

  

</head>

</html>

<body>
  <div id="app">
    <main class="content">
      
<section class="cover">
    
      
      <a class="forkMe" href="https://github.com/Shen-Yu/hexo-theme-ayer"
        target="_blank"><img width="149" height="149" src="/images/forkme.png"
          class="attachment-full size-full" alt="Fork me on GitHub" data-recalc-dims="1"></a>
    
  <div class="cover-frame">
    <div class="bg-box">
      <img src="/images/xk.jpg" alt="image frame" />
    </div>
    <div class="cover-inner text-center text-white">
      <h1><a href="/">TedWang 的大数据</a></h1>
      <div id="subtitle-box">
        
        <span id="subtitle"></span>
        
      </div>
      <div>
        
      </div>
    </div>
  </div>
  <div class="cover-learn-more">
    <a href="javascript:void(0)" class="anchor"><i class="ri-arrow-down-line"></i></a>
  </div>
</section>



<script src="https://cdn.jsdelivr.net/npm/typed.js@2.0.11/lib/typed.min.js"></script>

<div id="main">
  <section class="outer">
  <article class="articles">
    
    
    
    
    <article id="post-CDH通过实例恢复集群" class="article article-type-post" itemscope
  itemprop="blogPost" data-scroll-reveal>

  <div class="article-inner">
    
    <header class="article-header">
       
<h2 itemprop="name">
  <a class="article-title" href="/2017/07/12/CDH%E9%80%9A%E8%BF%87%E5%AE%9E%E4%BE%8B%E6%81%A2%E5%A4%8D%E9%9B%86%E7%BE%A4/"
    >CDH 通过实例恢复集群</a
  >
</h2>
  

    </header>
    

    
    <div class="article-meta">
      <a href="/2017/07/12/CDH%E9%80%9A%E8%BF%87%E5%AE%9E%E4%BE%8B%E6%81%A2%E5%A4%8D%E9%9B%86%E7%BE%A4/" class="article-date">
  <time datetime="2017-07-12T12:43:27.000Z" itemprop="datePublished">2017-07-12</time>
</a>
      
      
      
      
    </div>
    

    

    
    <div class="article-entry" itemprop="articleBody">
      


      

      
      <h1 id="CDH通过实例恢复集群"><a href="#CDH通过实例恢复集群" class="headerlink" title="CDH通过实例恢复集群"></a>CDH通过实例恢复集群</h1><ol>
<li><p>修改新的 内网IP</p>
</li>
<li><p>启动 mysql</p>
</li>
<li><p>server<br>修改 db<br>cat db.properties<br><img src="/2017/07/12/CDH%E9%80%9A%E8%BF%87%E5%AE%9E%E4%BE%8B%E6%81%A2%E5%A4%8D%E9%9B%86%E7%BE%A4/cdh0301.png" alt="avatar"><br><img src="/2017/07/12/CDH%E9%80%9A%E8%BF%87%E5%AE%9E%E4%BE%8B%E6%81%A2%E5%A4%8D%E9%9B%86%E7%BE%A4/cdh0302.png" alt="avatar"></p>
</li>
</ol>
<ol start="4">
<li><p>agent  (有agent的机器都要检查)<br>config.ini<br><img src="/2017/07/12/CDH%E9%80%9A%E8%BF%87%E5%AE%9E%E4%BE%8B%E6%81%A2%E5%A4%8D%E9%9B%86%E7%BE%A4/cdh0303.png" alt="avatar"></p>
</li>
<li><p>mysql 表<br>use cmf;<br>desc hosts;<br>update hosts set ip = xxx where hosts = xxx<br><img src="/2017/07/12/CDH%E9%80%9A%E8%BF%87%E5%AE%9E%E4%BE%8B%E6%81%A2%E5%A4%8D%E9%9B%86%E7%BE%A4/cdh0304.png" alt="avatar"></p>
</li>
</ol>
<p>tail -f /-F 的区别是什么？</p>
<ol start="6">
<li><p>启动 server<br>./cloudera-scm-server start</p>
</li>
<li><p>启动 所以 agent</p>
</li>
<li><p>web 界面启动 cms<br>可以看到，ip发生了变化<br><img src="/2017/07/12/CDH%E9%80%9A%E8%BF%87%E5%AE%9E%E4%BE%8B%E6%81%A2%E5%A4%8D%E9%9B%86%E7%BE%A4/cdh0305.png" alt="avatar"></p>
</li>
</ol>
<p>需要重新部署下 客户端</p>

      
      <!-- reward -->
      
    </div>
    
    
      <!-- copyright -->
      
    <footer class="article-footer">
      
      
  <ul class="article-tag-list" itemprop="keywords"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/tedata/" rel="tag">tedata</a></li></ul>


    </footer>

  </div>

  

  
  
  

  

</article>
    
    <article id="post-CDH01" class="article article-type-post" itemscope
  itemprop="blogPost" data-scroll-reveal>

  <div class="article-inner">
    
    <header class="article-header">
       
<h2 itemprop="name">
  <a class="article-title" href="/2017/07/09/CDH01/"
    >CDH 常规使用</a
  >
</h2>
  

    </header>
    

    
    <div class="article-meta">
      <a href="/2017/07/09/CDH01/" class="article-date">
  <time datetime="2017-07-09T12:43:27.000Z" itemprop="datePublished">2017-07-09</time>
</a>
      
      
      
      
    </div>
    

    

    
    <div class="article-entry" itemprop="articleBody">
      


      

      
      <h1 id="CDH-常规使用"><a href="#CDH-常规使用" class="headerlink" title="CDH 常规使用"></a>CDH 常规使用</h1><p>学习目标：</p>
<ol>
<li>cdh官网解读</li>
<li>cdh启动/停止</li>
<li>cdh架构</li>
<li>日志解读</li>
<li>web页面分析</li>
<li>怎样添加服务</li>
<li>监控</li>
</ol>
<h3 id="一-cloudera官网"><a href="#一-cloudera官网" class="headerlink" title="一. cloudera官网"></a>一. cloudera官网</h3><p><a href="http://www.cloudera.com" target="_blank" rel="noopener">www.cloudera.com</a>  CM不开源  CDH: CM+apache组件和cloudera公司的组件</p>
<p>CDH 6.x 6.3.1   HDFS3.0 HBase2.0<br>    5.x 5.16.1  HDFS2.6 HBase1.2</p>
<p><a href="https://www.cloudera.com/downloads/manager/5-16-2.html" target="_blank" rel="noopener">https://www.cloudera.com/downloads/manager/5-16-2.html</a><br><a href="https://docs.cloudera.com/documentation/index.html" target="_blank" rel="noopener">https://docs.cloudera.com/documentation/index.html</a><br><a href="https://docs.cloudera.com/documentation/enterprise/5-16-x.html" target="_blank" rel="noopener">https://docs.cloudera.com/documentation/enterprise/5-16-x.html</a></p>
<h3 id="二-正常启动停止顺序"><a href="#二-正常启动停止顺序" class="headerlink" title="二. 正常启动停止顺序"></a>二. 正常启动停止顺序</h3><p>su - mysqladmin</p>
<p>service mysql start</p>
<p>/opt/cloudera-manager/cm-5.16.2/etc/init.d/cloudera-scm-server start 1个节点<br>/opt/cloudera-manager/cm-5.16.2/etc/init.d/cloudera-scm-agent start 所有节点</p>
<p>进入web7180，先启动 CMS 5个进程服务</p>
<p>启动Cluster1服务: HDFS YARN ZK KAFKA HBASE</p>
<p>停止顺序?</p>
<p>坑: mysql 单点 cm metadata </p>
<p>cm挂了 启动 初始化  cm metadata + hive 表数据 重新初始化</p>
<p>1.mysql没有开启binlog<br>2.mysql没有 定期备份 1天<br>mysqldump命令 cmf &gt;cmf.sql</p>
<h3 id="三-架构"><a href="#三-架构" class="headerlink" title="三. 架构"></a>三. 架构</h3><p><img src="/2017/07/09/CDH01/cma.png" alt="avatar"></p>
<p>假如 CM web界面server服务挂了，HDFS YARN这些服务正常吗？</p>
<p>配置:<br>服务端   /opt/cloudera-manager/cm-5.16.2/run/cloudera-scm-agent/process/366-hdfs-NAMENODE<br>客户端   /etc/hadoop/conf</p>
<p>1.cmf.config表<br>2.服务端 带序号的<br>3.客户端 不带序号 默认的</p>
<p>务必从web界面修改参数值 </p>
<p>应用开发 配置 /etc/hadoop/conf</p>
<p>客户端  gateway<br>    在web上点击 添加gateway服务，添加后，不需要重启服务，只需要重新部署客户端即可<br><img src="/2017/07/09/CDH01/gateway.png" alt="avatar"></p>
<h3 id="四-日志"><a href="#四-日志" class="headerlink" title="四. 日志"></a>四. 日志</h3><p><a href="http://106.14.180.252:7180/cmf/config2?task=ALL_LOG_DIRECTORIES" target="_blank" rel="noopener">http://106.14.180.252:7180/cmf/config2?task=ALL_LOG_DIRECTORIES</a></p>
<p>Configruation –&gt; log service 查看日志目录</p>
<p>组件服务的日志： /var/log/xxx</p>
<p>TAR CM的 /opt/cloudera-manager/cm-5.16.2/log/cloudera-scm-server<br>     /opt/cloudera-manager/cm-5.16.2/log/cloudera-scm-agent</p>
<p>RPM部署CM ：</p>
<pre><code>/var/log/cloudera-scm-server 
/var/log/cloudera-scm-agent</code></pre><p>xxxxx..log.out  进程的日志  出现error  优先排查<br>Will not attempt to authenticate using SASL (unknown error)</p>
<p>stdout 和 stderr 相当于 shell 脚本的debug的输出</p>
<p>xxxx.stdout</p>
<p>xxxx.stderr</p>
<p><a href="http://blog.itpub.net/30089851/viewspace-2136372/" target="_blank" rel="noopener">http://blog.itpub.net/30089851/viewspace-2136372/</a></p>
<h3 id="五-界面解读"><a href="#五-界面解读" class="headerlink" title="五. 界面解读"></a>五. 界面解读</h3><p>进程 process  instance  role</p>
<p>NameNode Advanced Configuration Snippet (Safety Valve) for hdfs-site.xml</p>
<p>ranger HDP</p>
<h3 id="六-添加服务-amp-HOST"><a href="#六-添加服务-amp-HOST" class="headerlink" title="六. 添加服务&amp;HOST"></a>六. 添加服务&amp;HOST</h3><p>先手工的部署agent 启动/opt/cloudera-manager/cm-5.16.2/etc/init.d/cloudera-scm-agent start</p>
<h3 id="七-监控"><a href="#七-监控" class="headerlink" title="七. 监控"></a>七. 监控</h3><p>TS query language  TSQL</p>
<p>生产者<br>SELECT total_kafka_bytes_received_rate_across_kafka_broker_topics<br>WHERE entityName = “kafka:DSHS” AND category = KAFKA_TOPIC</p>
<p>消费者<br>SELECT total_kafka_bytes_fetched_rate_across_kafka_broker_topics<br>WHERE entityName = “kafka:DSHS” AND category = KAFKA_TOPIC</p>
<p>SELECT<br>total_kafka_bytes_received_rate_across_kafka_broker_topics,<br>total_kafka_bytes_fetched_rate_across_kafka_broker_topics<br>WHERE entityName = “kafka:DSHS” AND category = KAFKA_TOPIC</p>
<p>charts –》 Chart Builder ，粘贴SQL，出图，给 title 命个名，save –》 HOME PAGE</p>
<p>如果 对监控指标 不是很明确，以HDFS 为例，可以点击 “HDFS” –》 “Charts Library” 去里面选择想要的图的SQL</p>
<p>丢: 配置邮件预警 +….+Kafka+Spark2</p>

      
      <!-- reward -->
      
    </div>
    
    
      <!-- copyright -->
      
    <footer class="article-footer">
      
      
  <ul class="article-tag-list" itemprop="keywords"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/tedata/" rel="tag">tedata</a></li></ul>


    </footer>

  </div>

  

  
  
  

  

</article>
    
    <article id="post-SparkSQL02" class="article article-type-post" itemscope
  itemprop="blogPost" data-scroll-reveal>

  <div class="article-inner">
    
    <header class="article-header">
       
<h2 itemprop="name">
  <a class="article-title" href="/2017/04/12/SparkSQL02/"
    >Spark SQL 第二课</a
  >
</h2>
  

    </header>
    

    
    <div class="article-meta">
      <a href="/2017/04/12/SparkSQL02/" class="article-date">
  <time datetime="2017-04-12T12:43:27.000Z" itemprop="datePublished">2017-04-12</time>
</a>
      
      
      
      
    </div>
    

    

    
    <div class="article-entry" itemprop="articleBody">
      


      

      
      <h1 id="SparkSQL02"><a href="#SparkSQL02" class="headerlink" title="SparkSQL02"></a>SparkSQL02</h1><p>学习目标：</p>
<ol>
<li>saveAsTable 和 insertInto 的区别</li>
<li>怎么创建视图，即DF 创建表</li>
<li>Catalog 使用</li>
<li>DF/DS/RDD 相互转换</li>
<li>UDF 函数</li>
</ol>
<h3 id="一-saveAsTable-和-insertInto-的区别"><a href="#一-saveAsTable-和-insertInto-的区别" class="headerlink" title="一. saveAsTable 和 insertInto 的区别"></a>一. saveAsTable 和 insertInto 的区别</h3><p>sql 写出的数据 保存成一张表, 操作hive<br>方式一</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">spark.sql(&quot;&quot;).write.saveAsTable(&quot;&quot;)</span><br></pre></td></tr></table></figure>


<p>方式二</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">spark.sql(&quot;&quot;).write.insertInto(&quot;&quot;)</span><br></pre></td></tr></table></figure>


<p>saveAsTable 和 insertInto 的区别:</p>
<p>insertInto 忽略字段名称, 而是基于位置插入的, 即按顺序插入</p>
<p>看下下面的测试:</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">scala&gt; Seq((1,2)).toDF(&quot;i&quot;,&quot;j&quot;).write.mode(&quot;overwrite&quot;).saveAsTable(&quot;t1&quot;)</span><br><span class="line">scala&gt; sql(&quot;select * from t1&quot;).show</span><br><span class="line">+---+---+</span><br><span class="line">|  i|  j|</span><br><span class="line">+---+---+</span><br><span class="line">|  1|  2|</span><br><span class="line">+---+---+</span><br></pre></td></tr></table></figure>

<p><img src="/2017/04/12/SparkSQL02/insertinto1.png" alt="avatar"></p>
<p>可以看到, saveAsTable 是根据字段插入的, 字段i,j 和 值是对应的</p>
<p>可以看到, insertInto 是根据顺序来的, i, j 并没有和 值对应<br><img src="/2017/04/12/SparkSQL02/saveastable.png" alt="avatar"></p>
<p>测试表存在和不存在上面两种方式的区别?</p>
<h3 id="二-怎么创建视图，即DF-创建表"><a href="#二-怎么创建视图，即DF-创建表" class="headerlink" title="二. 怎么创建视图，即DF 创建表"></a>二. 怎么创建视图，即DF 创建表</h3><p>spark.sql(“create table…”) //这种方式不推荐, 因为 创建表是需要权限的, 提前创建好最好</p>
<p>临时视图和 全局视图<br>创建临时视图</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">df.createOrReplaceTempView(&quot;people&quot;)</span><br></pre></td></tr></table></figure>

<p>创建全局视图</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">df.createGlobalTempView(&quot;people&quot;)</span><br></pre></td></tr></table></figure>

<p>全局视图必须在 视图前加 “ global_temp”</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">spark.sql(&quot;SELECT * FROM global_temp.people&quot;).show()</span><br></pre></td></tr></table></figure>


<p>platform 组内 province访问次数最多的 TOPN</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">select platform, province, count(1) from  log group  by  platform, province</span><br></pre></td></tr></table></figure>



<h3 id="三-Catalog-使用"><a href="#三-Catalog-使用" class="headerlink" title="三. Catalog 使用"></a>三. Catalog 使用</h3><p>catalog</p>
<p>创建catalog<br>scala&gt; val catalog = spark.catalog</p>
<p>查看数据库</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">scala&gt; catalog.listDatabases.show</span><br><span class="line">+-------+--------------------+--------------------+</span><br><span class="line">|   name|         description|         locationUri|</span><br><span class="line">+-------+--------------------+--------------------+</span><br><span class="line">|    bdp|                    |hdfs:&#x2F;&#x2F;hdcluster&#x2F;...|</span><br><span class="line">|default|Default Hive data...|hdfs:&#x2F;&#x2F;hdcluster&#x2F;...|</span><br><span class="line">|   test|                    |hdfs:&#x2F;&#x2F;hdcluster&#x2F;...|</span><br><span class="line">+-------+--------------------+--------------------+</span><br></pre></td></tr></table></figure>




<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">scala&gt; catalog.listDatabases.show(false)</span><br><span class="line">+-------+---------------------+--------------------------------------------+</span><br><span class="line">|name   |description          |locationUri                                 |</span><br><span class="line">+-------+---------------------+--------------------------------------------+</span><br><span class="line">|bdp    |                     |hdfs:&#x2F;&#x2F;hdcluster&#x2F;user&#x2F;hive&#x2F;warehouse&#x2F;bdp.db |</span><br><span class="line">|default|Default Hive database|hdfs:&#x2F;&#x2F;hdcluster&#x2F;user&#x2F;hive&#x2F;warehouse        |</span><br><span class="line">|test   |                     |hdfs:&#x2F;&#x2F;hdcluster&#x2F;user&#x2F;hive&#x2F;warehouse&#x2F;test.db|</span><br><span class="line">+-------+---------------------+--------------------------------------------+</span><br></pre></td></tr></table></figure>

<p>查看表</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">scala&gt; catalog.listTables(&quot;test&quot;).show</span><br><span class="line">+-------+--------+-----------+---------+-----------+</span><br><span class="line">|   name|database|description|tableType|isTemporary|</span><br><span class="line">+-------+--------+-----------+---------+-----------+</span><br><span class="line">|  test2|    test|       null|  MANAGED|      false|</span><br><span class="line">| test22|    test|       null|  MANAGED|      false|</span><br><span class="line">|tratest|    test|       null|  MANAGED|      false|</span><br><span class="line">+-------+--------+-----------+---------+-----------+</span><br></pre></td></tr></table></figure>


<p>查看所有函数</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">scala&gt; catalog.listFunctions.show</span><br><span class="line">+----------+--------+-----------+--------------------+-----------+</span><br><span class="line">|      name|database|description|           className|isTemporary|</span><br><span class="line">+----------+--------+-----------+--------------------+-----------+</span><br><span class="line">|         !|    null|       null|org.apache.spark....|       true|</span><br><span class="line">|         %|    null|       null|org.apache.spark....|       true|</span><br><span class="line">|         &amp;|    null|       null|org.apache.spark....|       true|</span><br><span class="line">|         *|    null|       null|org.apache.spark....|       true|</span><br><span class="line">|         +|    null|       null|org.apache.spark....|       true|</span><br><span class="line">|         -|    null|       null|org.apache.spark....|       true|</span><br></pre></td></tr></table></figure>


<p>查看字段</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">scala&gt; catalog.listColumns(&quot;test.test22&quot;).show</span><br><span class="line">+----+-----------+--------+--------+-----------+--------+</span><br><span class="line">|name|description|dataType|nullable|isPartition|isBucket|</span><br><span class="line">+----+-----------+--------+--------+-----------+--------+</span><br><span class="line">| uid|       null|  string|    true|      false|   false|</span><br><span class="line">| pid|       null|  string|    true|      false|   false|</span><br></pre></td></tr></table></figure>


<h3 id="四-DF-DS-RDD-相互转换"><a href="#四-DF-DS-RDD-相互转换" class="headerlink" title="四. DF/DS/RDD 相互转换"></a>四. DF/DS/RDD 相互转换</h3><p>DF/DF/RDD</p>
<p>ROW DF 弱类型</p>
<ul>
<li>df 转ds</li>
</ul>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">import spark.implicits._</span><br><span class="line">val df &#x3D; spark.read.option(&quot;header&quot;,&quot;true&quot;)</span><br><span class="line">.option(&quot;inferSchema&quot;,&quot;true&quot;).csv(&quot;data&#x2F;sales.csv&quot;)</span><br><span class="line">val ds &#x3D; df.as[Sales]</span><br><span class="line">&#x2F;&#x2F; Sales 是一个case class</span><br><span class="line">case class Sales(transactionId:Int,customerId:Int,itemId:Int,amountPaid:Double)</span><br></pre></td></tr></table></figure>


<ul>
<li>ds 转df</li>
</ul>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">import spark.implicits._</span><br><span class="line"></span><br><span class="line">val ds &#x3D; spark.read.textFile(&quot;data&#x2F;access.log&quot;)</span><br><span class="line">      .map(x &#x3D;&gt; &#123;</span><br><span class="line">        val splits &#x3D; x.split(&quot;\t&quot;)</span><br><span class="line">        val platform &#x3D; splits(1)</span><br><span class="line">        val traffic &#x3D; splits(6).toLong</span><br><span class="line">        val province &#x3D; splits(8)</span><br><span class="line">        val city &#x3D; splits(9)</span><br><span class="line">        val isp &#x3D; splits(10)</span><br><span class="line">        (platform, traffic, province, city, isp)</span><br><span class="line">      &#125;)</span><br><span class="line">val df &#x3D; ds.toDF(&quot;platform&quot;, &quot;traffic&quot;, &quot;province&quot;, &quot;city&quot;, &quot;isp&quot;)</span><br></pre></td></tr></table></figure>


<pre><code>  // 如果你想使用SQL来进行处理，那么就是将df注册成一个临时视图
df.createOrReplaceTempView(&quot;log&quot;)</code></pre><p>row_number<br>rank<br>dense_rank<br>的区别</p>
<ul>
<li>RDD =&gt; DF</li>
</ul>
<ol>
<li>反射</li>
</ol>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">import spark.implicits._</span><br><span class="line">  val peopleDF &#x3D; spark.sparkContext</span><br><span class="line">     .textFile(&quot;data&#x2F;people.txt&quot;)</span><br><span class="line">     .map(_.split(&quot;,&quot;))</span><br><span class="line">     .map(x &#x3D;&gt; Person(x(0), x(1).trim.toInt))</span><br><span class="line">     .toDF()</span><br><span class="line"></span><br><span class="line">   peopleDF.show(false)</span><br><span class="line"></span><br><span class="line">case class Person(name:String,age:Int)</span><br></pre></td></tr></table></figure>


<p>分解:</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">val peopleDF &#x3D; spark.sparkContext</span><br><span class="line">     .textFile(&quot;data&#x2F;people.txt&quot;) </span><br><span class="line">得到一个 RDD</span><br></pre></td></tr></table></figure>



<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">.map(_.split(&quot;,&quot;))</span><br><span class="line">&#x2F;&#x2F; 这一步是一个反射, Person是一个case class</span><br><span class="line">&#x2F;&#x2F;已经有schema信息了, 而 RDD 相对于 DF</span><br><span class="line">&#x2F;&#x2F;缺少的就是schema</span><br><span class="line">     .map(x &#x3D;&gt; Person(x(0), x(1).trim.toInt))</span><br><span class="line">     .toDF()</span><br><span class="line">转成一个DF</span><br></pre></td></tr></table></figure>


<ol start="2">
<li>编程自定义<br>// step1: Create an RDD</li>
</ol>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">val peopleRDD &#x3D; spark.sparkContext.textFile(&quot;data&#x2F;people.txt&quot;)</span><br></pre></td></tr></table></figure>



<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">&#x2F;&#x2F; step2: The schema is encoded in a string</span><br><span class="line">    &#x2F;&#x2F; 和官网提供的不同, 因为字段类型多样, 官网只给了字符串类型写法</span><br><span class="line">    &#x2F;&#x2F; 下面是更适用的写法</span><br><span class="line"></span><br><span class="line">    val schema &#x3D; StructType(Array(</span><br><span class="line">      StructField(&quot;name&quot;,StringType),</span><br><span class="line">      StructField(&quot;age&quot;,IntegerType)</span><br><span class="line">    ))</span><br><span class="line"></span><br><span class="line">    &#x2F;&#x2F; Convert records of the RDD (people) to Rows</span><br><span class="line">    val rowRDD &#x3D; peopleRDD</span><br><span class="line">      .map(_.split(&quot;,&quot;))</span><br><span class="line">      .map(attributes &#x3D;&gt; Row(attributes(0), attributes(1).trim.toInt)) &#x2F;&#x2F; 要写toInt</span><br><span class="line"></span><br><span class="line">    &#x2F;&#x2F; Apply the schema to the RDD</span><br><span class="line">    val peopleDF &#x3D; spark.createDataFrame(rowRDD, schema)</span><br><span class="line"></span><br><span class="line">    peopleDF.show()</span><br></pre></td></tr></table></figure>



<p>注意下这段代码, 起了两个 spark context, 会报spark context 异常</p>
<p><img src="/2017/04/12/SparkSQL02/sperror1.png" alt="avatar"><br><img src="/2017/04/12/SparkSQL02/sperror2.png" alt="avatar"></p>
<p>正确的做法是:<br>用 spark session 起 spark context<br><img src="/2017/04/12/SparkSQL02/spok.png" alt="avatar"></p>
<h3 id="五-UDF-函数"><a href="#五-UDF-函数" class="headerlink" title="五. UDF 函数"></a>五. UDF 函数</h3><p>spark sql 注册 udf 的两种方式:</p>
<ol>
<li>sqlContext.udf.register()<br>只对 sql 有效</li>
<li>spark.sql.function.udf()<br>此时注册的方法，对外部可见, 可以使用api</li>
</ol>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line">select AD.* ,</span><br><span class="line">ROUND(AD.adbidseccesscounts&#x2F;AD.adbidcounts * 100, 2) bidrate,</span><br><span class="line">ROUND(AD.adclickcounts&#x2F;AD.addispalycounts * 100, 2) clickrate</span><br><span class="line">from</span><br><span class="line">(</span><br><span class="line">select</span><br><span class="line">province,</span><br><span class="line">city,</span><br><span class="line">sum (case when requestmode&#x3D;1 and processnode&gt;&#x3D;1 then 1 else 0 end) requestmodecounts,</span><br><span class="line">sum (case when requestmode&#x3D;1 and processnode&gt;&#x3D;2 then 1 else 0 end) processnodecounts,</span><br><span class="line">sum (case when requestmode&#x3D;1 and processnode&#x3D;3 then 1 else 0 end) adrequestcounts,</span><br><span class="line">sum (case when adplatformproviderid&gt;&#x3D;100000 and iseffective&#x3D;1</span><br><span class="line">     and isbilling&#x3D;1 and isbid&#x3D;1 and adorderid!&#x3D;0 then 1 else 0 end) adbidcounts,</span><br><span class="line">sum (case when adplatformproviderid&gt;&#x3D;100000 and iseffective&#x3D;1</span><br><span class="line">     and isbilling&#x3D;1 and iswin&#x3D;1 then 1 else 0 end) adbidseccesscounts,</span><br><span class="line">sum (case when requestmode&#x3D;2 and iseffective&#x3D;1 then 1 else 0 end) addispalycounts,</span><br><span class="line">sum (case when requestmode&#x3D;3 and iseffective&#x3D;1 then 1 else 0 end) adclickcounts,</span><br><span class="line">sum (case when requestmode&#x3D;2 and iseffective&#x3D;1 and isbilling&#x3D;1 then 1 else 0 end) mediadispalycounts,</span><br><span class="line">sum (case when requestmode&#x3D;3 and iseffective&#x3D;1 and isbilling&#x3D;1 then 1 else 0 end) mediaclickcounts,</span><br><span class="line">sum (case when adplatformproviderid&gt;&#x3D;100000 and iseffective&#x3D;1</span><br><span class="line">     and isbilling&#x3D;1 and iswin&#x3D;1 and adorderid&gt;200000 then 1 else 0 end) adconsumecounts,</span><br><span class="line">sum (case when adplatformproviderid&gt;&#x3D;100000 and iseffective&#x3D;1</span><br><span class="line">     and isbilling&#x3D;1 and iswin&#x3D;1 and adorderid&gt;200000 then 1 else 0 end) adcostcounts</span><br><span class="line">from SYS_AD  GROUP BY province,city) AD</span><br></pre></td></tr></table></figure>


      
      <!-- reward -->
      
    </div>
    
    
      <!-- copyright -->
      
    <footer class="article-footer">
      
      
  <ul class="article-tag-list" itemprop="keywords"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/tedata/" rel="tag">tedata</a></li></ul>


    </footer>

  </div>

  

  
  
  

  

</article>
    
    <article id="post-SparkSQL01" class="article article-type-post" itemscope
  itemprop="blogPost" data-scroll-reveal>

  <div class="article-inner">
    
    <header class="article-header">
       
<h2 itemprop="name">
  <a class="article-title" href="/2017/04/10/SparkSQL01/"
    >Spark SQL 第一课</a
  >
</h2>
  

    </header>
    

    
    <div class="article-meta">
      <a href="/2017/04/10/SparkSQL01/" class="article-date">
  <time datetime="2017-04-10T12:43:27.000Z" itemprop="datePublished">2017-04-10</time>
</a>
      
      
      
      
    </div>
    

    

    
    <div class="article-entry" itemprop="articleBody">
      


      

      
      <h1 id="SparkSQL01"><a href="#SparkSQL01" class="headerlink" title="SparkSQL01"></a>SparkSQL01</h1><p>学习目标：</p>
<ol>
<li>SparkSQL 介绍及与 hive on spark 区别</li>
<li>SparkSQL 编程 POM 依赖</li>
<li>SparkSQL 编程规范</li>
<li>SparkSQL 连接 数据源演示</li>
</ol>
<h3 id="一-SparkSQL-介绍及与-hive-on-spark-区别"><a href="#一-SparkSQL-介绍及与-hive-on-spark-区别" class="headerlink" title="一. SparkSQL 介绍及与 hive on spark 区别"></a>一. SparkSQL 介绍及与 hive on spark 区别</h3><p>SparkSQL 支持hive 语法, 同时支持 hive 序列化/反序列化 , UDF 等<br>支持 访问存在的 hive 仓库</p>
<p>ExtDS : 外部数据源</p>
<p>Hive是进程级别的<br>Spark是线程级别的， 如果用Shark，会存在线程安全的问题</p>
<p>Hive On Spark:  hive 跑在 spark 引擎之上(原来是跑在MR之上)<br>和 Spark SQL 不是一回事<br>set hive.execution.engine = spark 即可</p>
<p>Spark SQL 是在 Spark 里面的<br>Hive On Spark 是在 hive 里面</p>
<p>Spark SQL<br>1.0<br>称为<br>SchemaRdd ==&gt; Table<br>==&gt; DataFrame<br>==&gt; DataSets  1.6  complie-time type safety, 编译时的类型检查, 提前抛出异常</p>
<p>Dataset:<br>dataset 是一个分布式的数据集<br>A Dataset is a distributed collection of data.<br>named columns , 带名字的列, 可以理解为一个表<br>In scala API, DataFrame is simply a type alias of DataSet[ROW]</p>
<p>Dataset API 仅支持 scala, java, 不支持 python</p>
<p>Spark SQL 添加依赖</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">&lt;dependency&gt;</span><br><span class="line">    &lt;groupId&gt;org.apache.spark&lt;&#x2F;groupId&gt;</span><br><span class="line">    &lt;artifactId&gt;spark-sql_2.11&lt;&#x2F;artifactId&gt;</span><br><span class="line">    &lt;version&gt;$&#123;spark.version&#125;&lt;&#x2F;version&gt;</span><br><span class="line">&lt;&#x2F;dependency&gt;</span><br></pre></td></tr></table></figure>



<p>DataFrame 和 Dataset的关系在 源码中的体现</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">def toDF(): DataFrame &#x3D; new Dataset[Row](sparkSession, queryExecution, RowEncoder(schema))</span><br></pre></td></tr></table></figure>

<p>==A DataFrame is a DataSet organized into named columns==</p>
<p>DataSet API 只在 Scala 和 Java中能用, python 中用不了</p>
<p>Spark Session 是 spark DF/DS 编程的入口点<br>The entry point to programming Spark with the Dataset and DataFrame API</p>
<h3 id="二-SparkSQL-编程-POM-依赖"><a href="#二-SparkSQL-编程-POM-依赖" class="headerlink" title="二. SparkSQL 编程 POM 依赖"></a>二. SparkSQL 编程 POM 依赖</h3><p>IDEA 运行SparkSQL 需要加的依赖</p>
<ol>
<li></li>
</ol>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">&lt;dependency&gt;</span><br><span class="line">  &lt;groupId&gt;org.codehaus.janino&lt;&#x2F;groupId&gt;</span><br><span class="line">  &lt;artifactId&gt;janino&lt;&#x2F;artifactId&gt;</span><br><span class="line">  &lt;version&gt;3.0.8&lt;&#x2F;version&gt;</span><br><span class="line">&lt;&#x2F;dependency&gt;</span><br></pre></td></tr></table></figure>

<p>否则会报错:</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">Exception in thread &quot;main&quot; java.lang.NoClassDefFoundError: org&#x2F;codehaus&#x2F;janino&#x2F;InternalCompilerException</span><br></pre></td></tr></table></figure>



<ol start="2">
<li></li>
</ol>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">&lt;dependency&gt;</span><br><span class="line">  &lt;groupId&gt;org.apache.hive&lt;&#x2F;groupId&gt;</span><br><span class="line">  &lt;artifactId&gt;hive-exec&lt;&#x2F;artifactId&gt;</span><br><span class="line">  &lt;version&gt;$&#123;hive.version&#125;&lt;&#x2F;version&gt;</span><br><span class="line">&lt;&#x2F;dependency&gt;</span><br></pre></td></tr></table></figure>

<p>注意这个 hive.version 要写 ==1.2.1==, 如果选择 cdh 版本的hive 会报错</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">Exception in thread &quot;main&quot; java.lang.NoSuchFieldError: METASTORE_CLIENT_SOCKET_LIFETIME</span><br></pre></td></tr></table></figure>


<p>SparkSQL</p>
<p>show : 展示前20条数据</p>
<p>def show(): Unit = show(20)</p>
<p>如果执行中, 报 mysql 驱动找不到的 错误, 需要指定 mysql-connect 包</p>
<p>./spark-shell –jars mysql-connect-xxx.jar</p>
<p>show(3) //表示展示3条<br><img src="/2017/04/10/SparkSQL01/show1.png" alt="avatar"></p>
<p>def show(truncate: Boolean): Unit = show(20, truncate)<br>此处的truncate 表示 一个数据长度大于 20, 就把它截断, 展示的不完整<br>测试如下<br><img src="/2017/04/10/SparkSQL01/show2.png" alt="avatar"></p>
<p>由于 spark-shell 每次使用比较麻烦, spark 提供了 sql 接口<br>./spark-sql    </p>
<p>如果 报 mysql 连接不上的问题, 可按如下解决:<br>./spark-sql  <br>–jars  mysql-connect-xx.jar \   //虽然官方说 –jars 会在 driver和 executor端都加上驱动, 但是实际上driver端并没有加上, 需要通过下面参数指定<br>–driver-class-path  mysql-connect-xx.jar</p>
<p>进入 spark-sql 后, 就能像sql客户端一样使用了<br><img src="/2017/04/10/SparkSQL01/sparksql.png" alt="avatar"></p>
<h3 id="三-SparkSQL-编程规范"><a href="#三-SparkSQL-编程规范" class="headerlink" title="三. SparkSQL 编程规范"></a>三. SparkSQL 编程规范</h3><p>编程:<br>spark 入口及参数设置</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">val spark &#x3D; SparkSession.builder()</span><br><span class="line">  .master(&quot;local&quot;)</span><br><span class="line">  .appName(&quot;SparkSessionApp&quot;)</span><br><span class="line">  .getOrCreate()</span><br></pre></td></tr></table></figure>

<p>可以通过 .config 设置各种参数</p>
<p>读文本操作<br>第1中写法</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">val df: DataFrame &#x3D; spark.read.format(&quot;text&quot;).load(&quot;data&#x2F;people.txt&quot;)</span><br></pre></td></tr></table></figure>


<p>第2种写法</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">spark.read.text(&quot;data&#x2F;people.txt&quot;).show() &#x2F;&#x2F;read.text &#x3D;&#x3D; read.format(&quot;text&quot;).load(&quot;&quot;)</span><br><span class="line"></span><br><span class="line">def text(paths: String*): DataFrame &#x3D; format(&quot;text&quot;).load(paths : _*)</span><br></pre></td></tr></table></figure>


<p>第3种写法</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">val ds: Dataset[String] &#x3D; spark.read.textFile(&quot;data&#x2F;people.txt&quot;)</span><br><span class="line">    ds.show()</span><br></pre></td></tr></table></figure>

<p>可以传多个路径进去</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">def text(path: String): DataFrame &#x3D; &#123;</span><br><span class="line">  &#x2F;&#x2F; This method ensures that calls that explicit need single argument works, see SPARK-16009</span><br><span class="line">  text(Seq(path): _*)</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">spark.read.format(&quot;text&quot;)</span><br><span class="line">spark.read.textFile</span><br></pre></td></tr></table></figure>

<p>这两个的返回值是不同的，前者是 DataFrame，后面的是 DataSet，所以<br>前面是不能map的，比如加上rdd才可以，后面可以直接map</p>
<p>实例：读取 csv格式数据</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">val df &#x3D; spark.read.format(&quot;csv&quot;)</span><br><span class="line">  .option(&quot;timestampFormat&quot;, &quot;yyyy&#x2F;MM&#x2F;dd HH:mm:ss ZZ&quot;) &#x2F;&#x2F;指定 时间戳格式</span><br><span class="line">  .option(&quot;inferSchema&quot;, &quot;true&quot;)  &#x2F;&#x2F;内部推导Schema开启</span><br><span class="line">  .option(&quot;sep&quot;, &quot;,&quot;)  &#x2F;&#x2F;分隔符</span><br><span class="line">  .option(&quot;header&quot;, &quot;true&quot;) &#x2F;&#x2F; 把行首作为字段</span><br><span class="line">  .load(&quot;data&#x2F;user.csv&quot;)</span><br></pre></td></tr></table></figure>


<p>写数据, 如果路径存在报错<br><img src="/2017/04/10/SparkSQL01/error1.png" alt="avatar"></p>
<p>需要追加一个 写的模式</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">resultDS</span><br><span class="line">.write</span><br><span class="line">.mode(&quot;overwrite&quot;)</span><br><span class="line">.format(&quot;text&quot;)</span><br><span class="line">.save(&quot;out&quot;)</span><br><span class="line"></span><br><span class="line">resultDS.write.mode(SaveMode.Overwrite).format(&quot;text&quot;).save(&quot;out&quot;)</span><br></pre></td></tr></table></figure>


<p>此时执行还是报错<br><img src="/2017/04/10/SparkSQL01/error2.png" alt="avatar"></p>
<p>因为此时有两列,<br>(splits(0), splits(1))<br>改成一列, 输出正常,<br>怎么样可以输出多列?</p>
<p>压缩</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">option(&quot;compression&quot;,&quot;gzip&quot;)</span><br><span class="line">resultDS</span><br><span class="line">.write</span><br><span class="line">.option(&quot;compression&quot;,&quot;gzip&quot;).mode(&quot;overwrite&quot;)</span><br><span class="line">.format(&quot;text&quot;).save(&quot;out&quot;)</span><br></pre></td></tr></table></figure>

<p>如果是 lzo压缩会报错<br><img src="/2017/04/10/SparkSQL01/lzoerror.png" alt="avatar"></p>
<p>源码中 压缩格式如下<br><img src="/2017/04/10/SparkSQL01/compress1.png" alt="avatar"></p>
<p>setCodecConfiguration 的设置, 底层和 MR 一样的<br><img src="/2017/04/10/SparkSQL01/compression2.png" alt="avatar"></p>
<p>导入json 数据, 读取会变成如下错误</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">Exception in thread &quot;main&quot; java.lang.IllegalArgumentException: Illegal pattern component: XXX</span><br></pre></td></tr></table></figure>


<p>原因:<br>maven升级的时候，没有自动加载完整依赖包，jsonAPI对于timeStampFormat有特殊需求，默认为下面这个格式这种格式，是无法被scala-lang包识别的。我们看报错的源码可以看出。</p>
<p>解决<br>加上: option(“timestampFormat”, “yyyy/MM/dd HH:mm:ss ZZ”)</p>
<p>即</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">spark.read</span><br><span class="line">.option(&quot;timestampFormat&quot;, &quot;yyyy&#x2F;MM&#x2F;dd HH:mm:ss ZZ&quot;)</span><br><span class="line">.format(&quot;json&quot;)</span><br><span class="line">.load(&quot;data&#x2F;people.json&quot;)</span><br></pre></td></tr></table></figure>


<p>json/csv 的报错以上方法 都适用</p>

      
      <!-- reward -->
      
    </div>
    
    
      <!-- copyright -->
      
    <footer class="article-footer">
      
      
  <ul class="article-tag-list" itemprop="keywords"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/tedata/" rel="tag">tedata</a></li></ul>


    </footer>

  </div>

  

  
  
  

  

</article>
    
    <article id="post-hadooplzo" class="article article-type-post" itemscope
  itemprop="blogPost" data-scroll-reveal>

  <div class="article-inner">
    
    <header class="article-header">
       
<h2 itemprop="name">
  <a class="article-title" href="/2017/03/12/hadooplzo/"
    >Hadoop 支持 lzo压缩算法</a
  >
</h2>
  

    </header>
    

    
    <div class="article-meta">
      <a href="/2017/03/12/hadooplzo/" class="article-date">
  <time datetime="2017-03-12T04:00:00.000Z" itemprop="datePublished">2017-03-12</time>
</a>
      
      
      
      
    </div>
    

    

    
    <div class="article-entry" itemprop="articleBody">
      


      

      
      <h1 id="Hadoop-支持-lzo压缩算法"><a href="#Hadoop-支持-lzo压缩算法" class="headerlink" title="Hadoop 支持 lzo压缩算法"></a>Hadoop 支持 lzo压缩算法</h1><p>学习目标：</p>
<ol>
<li>安装lzo相关依赖</li>
<li>编译lzo</li>
<li>编译Hadoop-lzo</li>
<li>修改hadoop配置</li>
<li>准备数据</li>
<li>wordcount</li>
<li>文件添加index</li>
<li>安装lzo相关依赖</li>
</ol>
<h3 id="一-lzo-概念和优点"><a href="#一-lzo-概念和优点" class="headerlink" title="一. lzo 概念和优点"></a>一. lzo 概念和优点</h3><p>Hadoop经常用于处理大量的数据，如果期间的输出数据、中间数据能压缩存储，对系统的I/O性能会有提升。综合考虑压缩、解压速度、是否支持split，目前lzo是最好的选择。LZO（LZO是Lempel-Ziv-Oberhumer的缩写）是一种高压缩比和解压速度极快的编码，它的特点是解压缩速度非常快，无损压缩，压缩后的数据能准确还原，lzo是基于block分块的，允许数据被分解成chunk，能够被并行的解压。LZO库实现了许多有下述特点的算法：</p>
<p>　　（1）、解压简单，速度非常快。</p>
<p>　　（2）、解压不需要内存。</p>
<p>　　（3）、压缩相当地快。</p>
<p>　　（4）、压缩需要64 kB的内存。</p>
<p>　　（5）、允许在压缩部分以损失压缩速度为代价提高压缩率，解压速度不会降低。</p>
<p>　　（6）、包括生成预先压缩数据的压缩级别，这样可以得到相当有竞争力的压缩比。</p>
<p>　　（7）、另外还有一个只需要8 kB内存的压缩级别。</p>
<p>　　（8）、算法是线程安全的。</p>
<p>　　（9）、算法是无损的。</p>
<h3 id="二-安装lzo相关依赖"><a href="#二-安装lzo相关依赖" class="headerlink" title="二. 安装lzo相关依赖"></a>二. 安装lzo相关依赖</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">[root@JD ~]# yum install -y svn ncurses-devel</span><br><span class="line">[root@JD ~]# yum install -y gcc gcc-c++ make cmake</span><br><span class="line">[root@JD ~]# yum install -y openssl openssl-devel svn ncurses-devel zlib-devel libtool </span><br><span class="line">[root@JD ~]# yum install -y lzo lzo-devel lzop autoconf automake cmake</span><br></pre></td></tr></table></figure>


<h3 id="三-编译lzo"><a href="#三-编译lzo" class="headerlink" title="三. 编译lzo"></a>三. 编译lzo</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">[hadoop@JD ~]$ wget http:&#x2F;&#x2F;www.oberhumer.com&#x2F;opensource&#x2F;lzo&#x2F;download&#x2F;lzo-2.06.tar.gz</span><br><span class="line">[hadoop@JD ~]$ tar -zxvf lzo-2.06.tar.gz</span><br><span class="line">[hadoop@JD ~]$ cd lzo-2.06</span><br><span class="line">[hadoop@JD ~]$ export CFLAGS&#x3D;-m64</span><br><span class="line">[hadoop@JD ~]$ mkdir compile</span><br><span class="line">[hadoop@JD ~]$ .&#x2F;configure -enable-shared -prefix&#x3D;&#x2F;home&#x2F;hadoop&#x2F;app&#x2F;lzo-2.06&#x2F;compile</span><br><span class="line">[hadoop@JD ~]$ make &amp;&amp;  make install</span><br></pre></td></tr></table></figure>


<h3 id="三-编译Hadoop-lzo"><a href="#三-编译Hadoop-lzo" class="headerlink" title="三 编译Hadoop-lzo"></a>三 编译Hadoop-lzo</h3><p>下载源码</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">wget https:&#x2F;&#x2F;github.com&#x2F;twitter&#x2F;hadoop-lzo&#x2F;archive&#x2F;master.zip</span><br><span class="line">解压</span><br><span class="line"></span><br><span class="line">[hadoop@JD software]$ unzip -d ~&#x2F;app&#x2F; hadoop-lzo-master.zip</span><br></pre></td></tr></table></figure>


<p>进入解压后的目录</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[hadoop@JD app]$ cd hadoop-lzo-master&#x2F;</span><br><span class="line">[hadoop@JD hadoop-lzo-master]$</span><br></pre></td></tr></table></figure>


<p>修改此目录下pom.xml文件</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">&lt;repositories&gt;</span><br><span class="line">	#添加cloudera仓库</span><br><span class="line">    &lt;repository&gt;</span><br><span class="line">     &lt;id&gt;cloudera&lt;&#x2F;id&gt;</span><br><span class="line">     &lt;url&gt;https:&#x2F;&#x2F;repository.cloudera.com&#x2F;artifactory&#x2F;cloudera-repos&lt;&#x2F;url&gt;</span><br><span class="line">    &lt;&#x2F;repository&gt;</span><br><span class="line">  &lt;&#x2F;repositories&gt;</span><br><span class="line">&lt;properties&gt;</span><br><span class="line">    &lt;project.build.sourceEncoding&gt;UTF-8&lt;&#x2F;project.build.sourceEncoding&gt;</span><br><span class="line">    #因为用的是cdh的</span><br><span class="line">    &lt;hadoop.current.version&gt;2.6.0-cdh5.15.1&lt;&#x2F;hadoop.current.version&gt;</span><br><span class="line">    &lt;hadoop.old.version&gt;1.0.4&lt;&#x2F;hadoop.old.version&gt;</span><br><span class="line">&lt;&#x2F;properties&gt;</span><br></pre></td></tr></table></figure>


<p>添加环境变量</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[hadoop@JD hadoop-lzo-master]$ export CFLAGS&#x3D;-m64</span><br><span class="line">[hadoop@JD hadoop-lzo-master]$ export CXXFLAGS&#x3D;-m64</span><br></pre></td></tr></table></figure>



<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">#Modify the actual path for your hadoop</span><br><span class="line">[hadoop@JD hadoop-lzo-master]$ export C_INCLUDE_PATH&#x3D;&#x2F;home&#x2F;hadoop&#x2F;app&#x2F;lzo-2.06&#x2F;compile&#x2F;include</span><br><span class="line">[hadoop@JD hadoop-lzo-master]$ export LIBRARY_PATH&#x3D;&#x2F;home&#x2F;hadoop&#x2F;app&#x2F;lzo-2.06&#x2F;compile&#x2F;lib</span><br></pre></td></tr></table></figure>


<p>编译源码：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">mvn clean package -DskipTests</span><br></pre></td></tr></table></figure>
<p><img src="/2017/03/12/hadooplzo/hl1.png" alt="avatar"></p>
<p>进入target/native/Linux-amd64-64</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">[hadoop@JD hadoop-lzo-master]$ cd target&#x2F;native&#x2F;Linux-amd64-64&#x2F;</span><br><span class="line">[hadoop@JD Linux-amd64-64]$ mkdir ~&#x2F;app&#x2F;hadoop-lzo-files</span><br><span class="line">[hadoop@JD Linux-amd64-64]$ tar -cBf - -C lib . | tar -xBvf - -C ~&#x2F;app&#x2F;hadoop-lzo-files</span><br></pre></td></tr></table></figure>

<p>拷贝文件</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[hadoop@hadoop-01 hadoop-lzo-files]$ cp ~&#x2F;app&#x2F;hadoop-lzo-files&#x2F;libgplcompression* $HADOOP_HOME&#x2F;lib&#x2F;native&#x2F;</span><br></pre></td></tr></table></figure>


<h3 id="四-修改hadoop配置"><a href="#四-修改hadoop配置" class="headerlink" title="四 修改hadoop配置"></a>四 修改hadoop配置</h3><p>vi core-site.xml</p>
<p>修改core-site.xml的配置文件</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">    &lt;property&gt;</span><br><span class="line">        &lt;name&gt;io.compression.codecs&lt;&#x2F;name&gt;</span><br><span class="line">        &lt;value&gt;org.apache.hadoop.io.compress.GzipCodec,</span><br><span class="line">               org.apache.hadoop.io.compress.DefaultCodec,</span><br><span class="line">               org.apache.hadoop.io.compress.BZip2Codec,</span><br><span class="line">               org.apache.hadoop.io.compress.SnappyCodec,</span><br><span class="line">               com.hadoop.compression.lzo.LzoCodec,</span><br><span class="line">               com.hadoop.compression.lzo.LzopCodec</span><br><span class="line">        &lt;&#x2F;value&gt;</span><br><span class="line">    &lt;&#x2F;property&gt;</span><br><span class="line"></span><br><span class="line">  </span><br><span class="line"></span><br><span class="line">&lt;property&gt;</span><br><span class="line">      &lt;name&gt;io.compression.codec.lzo.class&lt;&#x2F;name&gt;</span><br><span class="line">      &lt;value&gt;com.hadoop.compression.lzo.LzoCodec&lt;&#x2F;value&gt;</span><br><span class="line">  &lt;&#x2F;property&gt;</span><br></pre></td></tr></table></figure>

<p>修改mapred-site.xml配置文件</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">&lt;property&gt;</span><br><span class="line">  &lt;name&gt;mapred.compress.map.output&lt;&#x2F;name&gt;</span><br><span class="line">  &lt;value&gt;true&lt;&#x2F;value&gt;</span><br><span class="line">&lt;&#x2F;property&gt;</span><br><span class="line"> </span><br><span class="line">&lt;property&gt;</span><br><span class="line">    &lt;name&gt;mapred.map.output.compression.codec&lt;&#x2F;name&gt;</span><br><span class="line">    &lt;value&gt;com.hadoop.compression.lzo.LzoCodec&lt;&#x2F;value&gt;</span><br><span class="line">&lt;&#x2F;property&gt;</span><br><span class="line"> </span><br><span class="line">&lt;property&gt;</span><br><span class="line">    &lt;name&gt;mapred.child.env&lt;&#x2F;name&gt;</span><br><span class="line">    &lt;value&gt;LD_LIBRARY_PATH&#x3D;&#x2F;usr&#x2F;local&#x2F;hadoop&#x2F;lzo&#x2F;lib&lt;&#x2F;value&gt;</span><br><span class="line">&lt;&#x2F;property&gt;</span><br></pre></td></tr></table></figure>

<h3 id="五-准备数据"><a href="#五-准备数据" class="headerlink" title="五 准备数据"></a>五 准备数据</h3><p>准备一个753M的数据<br><img src="/2017/03/12/hadooplzo/hl2.png" alt="avatar"></p>
<p>然后压缩此文件<br><img src="/2017/03/12/hadooplzo/hl3.png" alt="avatar"></p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">lzop -f access.txt</span><br></pre></td></tr></table></figure>



<h3 id="六-wordcount"><a href="#六-wordcount" class="headerlink" title="六 wordcount"></a>六 wordcount</h3><p>首先把数据上传到hdfs</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hdfs dfs -put access.txt.lzo &#x2F;lzo-data&#x2F;input</span><br></pre></td></tr></table></figure>


<p>计算wordcount</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">hadoop jar \</span><br><span class="line">&#x2F;home&#x2F;hadoop&#x2F;app&#x2F;hadoop&#x2F;share&#x2F;hadoop&#x2F;mapreduce&#x2F;hadoop-mapreduce-examples-2.6.0-cdh5.15.1.jar \</span><br><span class="line">wordcount \</span><br><span class="line">-Dmapreduce.job.inputformat.class&#x3D;com.hadoop.mapreduce.LzoTextInputFormat \</span><br><span class="line">&#x2F;lzo-data&#x2F;input&#x2F;access.txt.lzo \</span><br><span class="line">&#x2F;lzo-data&#x2F;output3</span><br></pre></td></tr></table></figure>

<p>从下图可以看出，没有分片<br><img src="/2017/03/12/hadooplzo/hl5.png" alt="avatar"></p>
<h3 id="七-文件添加index"><a href="#七-文件添加index" class="headerlink" title="七 文件添加index"></a>七 文件添加index</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">hadoop jar \</span><br><span class="line">&#x2F;home&#x2F;hadoop&#x2F;app&#x2F;hadoop-lzo-master&#x2F;target&#x2F;hadoop-lzo-0.4.21-SNAPSHOT.jar \</span><br><span class="line">com.hadoop.compression.lzo.DistributedLzoIndexer  \</span><br><span class="line">&#x2F;lzo-data&#x2F;input&#x2F;access.txt.lzo</span><br></pre></td></tr></table></figure>



<p>再次计算wordcount</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">hadoop jar \</span><br><span class="line">&#x2F;home&#x2F;hadoop&#x2F;app&#x2F;hadoop&#x2F;share&#x2F;hadoop&#x2F;mapreduce&#x2F;hadoop-mapreduce-examples-2.6.0-cdh5.15.1.jar \</span><br><span class="line">wordcount \</span><br><span class="line">-Dmapreduce.job.inputformat.class&#x3D;com.hadoop.mapreduce.LzoTextInputFormat \</span><br><span class="line">&#x2F;lzo-data&#x2F;input&#x2F;access.txt.lzo \</span><br><span class="line">&#x2F;lzo-data&#x2F;output4</span><br></pre></td></tr></table></figure>


<p>从下图，我们可以看出分成3个<br><img src="/2017/03/12/hadooplzo/hl7.png" alt="avatar"></p>
<p>参考文章：</p>
<p><a href="https://www.iteblog.com/archives/992.html" target="_blank" rel="noopener">https://www.iteblog.com/archives/992.html</a></p>

      
      <!-- reward -->
      
    </div>
    
    
      <!-- copyright -->
      
    <footer class="article-footer">
      
      
  <ul class="article-tag-list" itemprop="keywords"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/tedata/" rel="tag">tedata</a></li></ul>


    </footer>

  </div>

  

  
  
  

  

</article>
    
    <article id="post-SparkCore03" class="article article-type-post" itemscope
  itemprop="blogPost" data-scroll-reveal>

  <div class="article-inner">
    
    <header class="article-header">
       
<h2 itemprop="name">
  <a class="article-title" href="/2017/03/10/SparkCore03/"
    >SparkCore 03</a
  >
</h2>
  

    </header>
    

    
    <div class="article-meta">
      <a href="/2017/03/10/SparkCore03/" class="article-date">
  <time datetime="2017-03-10T04:00:00.000Z" itemprop="datePublished">2017-03-10</time>
</a>
      
      
      
      
    </div>
    

    

    
    <div class="article-entry" itemprop="articleBody">
      


      

      
      <h1 id="Spark-Core-03"><a href="#Spark-Core-03" class="headerlink" title="Spark Core 03"></a>Spark Core 03</h1><p>学习目标：</p>
<ol>
<li>RDD 的依赖</li>
<li>Persist/Cache</li>
<li>repartition/coalesce</li>
</ol>
<p>一.  RDD 的依赖</p>
<ol>
<li>先看一个例子</li>
</ol>
<p>执行一个 wc</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">def main(args: Array[String]): Unit &#x3D; &#123;</span><br><span class="line">  val sc &#x3D; ContextUtils.getContext(this.getClass.getSimpleName)</span><br><span class="line"></span><br><span class="line">  val data &#x3D; Array(&quot;hadoop hbase scala&quot;, &quot;hadoop hive scala&quot;, &quot;hadoop spark hive&quot;)</span><br><span class="line">  val input &#x3D; sc.parallelize(data)</span><br><span class="line">  input.flatMap(_.split(&quot; &quot;)).map((_,1)).reduceByKey(_+_).foreach(println)</span><br><span class="line"></span><br><span class="line">  sc.stop()</span><br><span class="line"></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>看下 spark ui 的DAG 图<br>![avatar](/Spark Core03/stageDAG.png)</p>
<p>可以看到，此时产生了两个 Stage：Stage0，Stage1</p>
<p>stage0 产生的是 reduceByKey 之前的DAG<br>![avatar](/Spark Core03/stage0.png)</p>
<p>stage1 产生的是<br>![avatar](/Spark Core03/stage1.png)</p>
<p>可以看到，在reduceByKey 处产生了 ShuffleRDD，stage 一分为二</p>
<ol start="2">
<li><p>什么是ShuffleRDD？<br>所谓 shuffle，指的是相同key的 元素聚合到一起的过程，这个过程因为可能存在跨主机，跨机架，所以是一个开销非常大的操作</p>
</li>
<li><p>图解 RDD 的依赖<br>先根据上面的例子，画一张图<br>![avatar](/Spark Core03/wcDep.png)</p>
</li>
</ol>
<p>看上图，从 flatMap –》Map，Map –》 Combiner，每个父RDD仅被子RDD 使用一次<br>这种依赖称为窄依赖</p>
<p>ReduceByKey中，父RDD 被 子RDD 使用多次，这种依赖称为 宽依赖</p>
<p>一般：map，fliter，union 这些操作都是窄依赖<br>    reduceByKey，groupByKey，countByKey这些操作都是宽依赖</p>
<p>二.  Persist/ Cache<br>persist/cache 指的是把数据缓存起来，下次调用的时候可以直接使用，而不用去再计算生成</p>
<p>persist 有四个参数</p>
<p>UseDisk：是否使用磁盘</p>
<p>UseMemory：是否使用内存</p>
<p>UseOffHeap：是否使用对外内存</p>
<p>Deserialization：是否不使用序列化</p>
<p>比如：<br>MEMORY_ONLY 的设置就是：(false, true, false, true)， 即只使用内存<br>MEMORY_AND_SER 就是：(fasle, true, false, false) 即使用内存并且序列化</p>
<p>persist/cache 的区别？<br>cache 是 persist 的特列，即是 MEMORY_ONLY 这种情况</p>
<p>怎么选择persist？</p>
<ol>
<li>优先选择 MEMORY_ONLY, 这也是spark的默认设置</li>
<li>如果1 不能满足，就选择 MEMORY_AND_SER，但要注意，序列化会增加 CPU的开销</li>
<li>不要选择 disk的方式，这种方式还不如重新算一遍</li>
</ol>
<p>三. repartition/coalease</p>
<p>repartition: 重新分区</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">scala&gt; inputre.mapPartitionsWithIndex((index, partition) &#x3D;&gt; &#123;</span><br><span class="line">     |       partition.map( x &#x3D;&gt; &#123;</span><br><span class="line">     |         println(s&quot;$index, $x&quot;)</span><br><span class="line">     |       &#125;)</span><br><span class="line">     |     &#125;).collect</span><br><span class="line">0, hadoop spark hive</span><br><span class="line">1, hadoop hbase scala</span><br><span class="line">2, hadoop hive scala</span><br><span class="line">res7: Array[Unit] &#x3D; Array((), (), ())</span><br></pre></td></tr></table></figure>



<p>colaease：减小分区</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">scala&gt; inputce.partition</span><br><span class="line">partitioner   partitions</span><br><span class="line"></span><br><span class="line">scala&gt; inputce.partitions.size</span><br><span class="line">res10: Int &#x3D; 2</span><br></pre></td></tr></table></figure>


<p>默认情况下，coalesce 是不能增大分区的，除非在引用方法时在分区数后面加上 true</p>
<p>不加 true，partition 最多增大到本来的分区数</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">scala&gt; val inputce &#x3D; inputre.coalesce(4)</span><br><span class="line">inputce: org.apache.spark.rdd.RDD[String] &#x3D; CoalescedRDD[15] at coalesce at &lt;console&gt;:25</span><br><span class="line"></span><br><span class="line">scala&gt; inputce.partitions.size</span><br><span class="line">res11: Int &#x3D; 3</span><br></pre></td></tr></table></figure>

<p>加上true，分区数可以增加到指定数目</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">scala&gt; val inputce &#x3D; inputre.coalesce(4, true)</span><br><span class="line">inputce: org.apache.spark.rdd.RDD[String] &#x3D; MapPartitionsRDD[20] at coalesce at &lt;console&gt;:25</span><br><span class="line"></span><br><span class="line">scala&gt; inputce.partitions.size</span><br><span class="line">res13: Int &#x3D; 4</span><br></pre></td></tr></table></figure>


      
      <!-- reward -->
      
    </div>
    
    
      <!-- copyright -->
      
    <footer class="article-footer">
      
      
  <ul class="article-tag-list" itemprop="keywords"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/tedata/" rel="tag">tedata</a></li></ul>


    </footer>

  </div>

  

  
  
  

  

</article>
    
    <article id="post-SparkCore 02" class="article article-type-post" itemscope
  itemprop="blogPost" data-scroll-reveal>

  <div class="article-inner">
    
    <header class="article-header">
       
<h2 itemprop="name">
  <a class="article-title" href="/2017/03/07/SparkCore%2002/"
    >Spark Core 第二课</a
  >
</h2>
  

    </header>
    

    
    <div class="article-meta">
      <a href="/2017/03/07/SparkCore%2002/" class="article-date">
  <time datetime="2017-03-07T12:43:27.000Z" itemprop="datePublished">2017-03-07</time>
</a>
      
      
      
      
    </div>
    

    

    
    <div class="article-entry" itemprop="articleBody">
      


      

      
      <p>Spark Core02</p>
<p>学习目标：</p>
<ol>
<li>RDD 的 Action 算子</li>
<li>关于RDD 的一些操作</li>
<li>排序的实现</li>
<li>Spark 关键术语</li>
</ol>
<p>一. RDD 的Action 算子</p>
<ol>
<li><p>foreach</p>
</li>
<li><p>foreachPartition</p>
</li>
<li><p>countByKey</p>
</li>
</ol>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">scala&gt; rdd1.map((_,1)).countByKey</span><br><span class="line">res10: scala.collection.Map[Int,Long] &#x3D; Map(5 -&gt; 1, 1 -&gt; 1, 6 -&gt; 1, 2 -&gt; 1, 3 -&gt; 1, 4 -&gt; 1)</span><br></pre></td></tr></table></figure>


<ol start="4">
<li>collectAsMap</li>
</ol>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">scala&gt; rdd1.map((_,1)).collectAsMap</span><br><span class="line">res14: scala.collection.Map[Int,Int] &#x3D; Map(2 -&gt; 1, 5 -&gt; 1, 4 -&gt; 1, 1 -&gt; 1, 3 -&gt; 1, 6 -&gt; 1)</span><br></pre></td></tr></table></figure>



<p>二. 关于RDD 的一些操作</p>
<ol>
<li>rdd.count</li>
<li>rdd.partitions.size</li>
<li>rdd.first</li>
<li>rdd.top(n)</li>
<li>rdd.takeOrder(n)</li>
</ol>
<p>三.  排序实现</p>
<ol>
<li>算子实现</li>
</ol>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">def main(args: Array[String]): Unit &#x3D; &#123;</span><br><span class="line"></span><br><span class="line">    val sc &#x3D; ContextUtils.getContext(this.getClass.getSimpleName)</span><br><span class="line"></span><br><span class="line">    val rdd1 &#x3D; sc.parallelize(List(&quot;iphone11 7000 20&quot;,&quot;hwpro30 5000 100&quot;,&quot;xiaomi 3000 200&quot;,&quot;sumsung 6000 1000&quot;))</span><br><span class="line"></span><br><span class="line">rdd1.map( x &#x3D;&gt; &#123;</span><br><span class="line">val splits &#x3D; x.split(&quot; &quot;)</span><br><span class="line">  (splits(0), splits(1).trim.toInt, splits(2).trim.toInt)</span><br><span class="line">&#125;).sortBy(_._2).foreach(println)</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>


<ol start="2">
<li>继承 Ordered 类实现</li>
</ol>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">  rdd1.map( x &#x3D;&gt; &#123;</span><br><span class="line">      val splits &#x3D; x.split(&quot; &quot;)</span><br><span class="line">      val product &#x3D; splits(0)</span><br><span class="line">      val price &#x3D; splits(1).toInt</span><br><span class="line">      val amount &#x3D; splits(2).toInt</span><br><span class="line">      MyProduct(product, price, amount)</span><br><span class="line">    &#125;).sortBy(x &#x3D;&gt; x ).foreach(println)</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">case class MyProduct(product: String, price: Int, amount: Int) extends Ordered[MyProduct] &#123;</span><br><span class="line">  override def compare(that: MyProduct): Int &#x3D; &#123;</span><br><span class="line">    -(this.amount - that.amount)</span><br><span class="line">  &#125;</span><br></pre></td></tr></table></figure>


<ol start="3">
<li>隐式转换</li>
</ol>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">implicit def myproduct2orderproduct(myproduct: MyProduct2) &#x3D; new Ordered[MyProduct2] &#123;</span><br><span class="line">      override def compare(that: MyProduct2): Int &#x3D; &#123;</span><br><span class="line">        myproduct.amount - that.amount</span><br><span class="line">      &#125;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    rdd1.map( x &#x3D;&gt; &#123;</span><br><span class="line">      val splits &#x3D; x.split(&quot; &quot;)</span><br><span class="line">      MyProduct2(splits(0).trim, splits(1).trim.toInt, splits(2).trim.toInt)</span><br><span class="line">    &#125;).sortBy(y &#x3D;&gt; y)</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">case class MyProduct2(product: String, price: Int, amount: Int)</span><br></pre></td></tr></table></figure>


<p>四. 关键术语</p>
<h5 id="Application"><a href="#Application" class="headerlink" title="Application:"></a>Application:</h5><p>构建在spark上的应用程序, 包含一个 driver + 多个 executor</p>
<h5 id="Application-jar"><a href="#Application-jar" class="headerlink" title="Application jar:"></a>Application jar:</h5><p>包含 user的Spark Application, . User Jar 不允许包含 Hadoop/Spark的 lib包,然而, 他们可以在运行时候添加进去</p>
<h5 id="Driver-Program"><a href="#Driver-Program" class="headerlink" title="Driver Program:"></a>Driver Program:</h5><p>main 方法, 包含一个 sc, 一个应用程序里面有driver</p>
<h5 id="Cluster-Manager"><a href="#Cluster-Manager" class="headerlink" title="Cluster Manager:"></a>Cluster Manager:</h5><h6 id="Deploy-Mode"><a href="#Deploy-Mode" class="headerlink" title="Deploy Mode:"></a>Deploy Mode:</h6><p>区分driver模式跑在哪里</p>
<p>YARN: RM NM(container)</p>
<p>cluster: Driver 跑在container</p>
<p>client: Driver就运行在你提交的机器的本地</p>
<p>Executor:</p>
<p>process</p>
<p>run tasks</p>
<p>keep data in memory or disk storage across them Each application has its own executors 对应于YARN 上的 container </p>
<h5 id="Task"><a href="#Task" class="headerlink" title="Task"></a>Task</h5><p>发送到 executor上 运行  基本单元</p>
<h5 id="RDD"><a href="#RDD" class="headerlink" title="RDD:"></a>RDD:</h5><p>partitions , 每一个 partition 对应一个 Task</p>
<h5 id="Job"><a href="#Job" class="headerlink" title="Job:"></a>Job:</h5><p>只要遇到 action, 就产生 job</p>
<h5 id="Stage"><a href="#Stage" class="headerlink" title="Stage:"></a>Stage:</h5><p>一组 tasks的集合</p>
<ul>
<li>一个 application: 1到n个 job</li>
<li>一个 Job: 1到 n 个 stage</li>
<li>一个 Stage: 1到n个 task, task与 partition 一一对应 </li>
</ul>
<p>Executor<br>是一个较大的概念<br>对应于YARN 是 Container<br>对应 Master-Slave 就是 Worker Node</p>
<p>Spark application run as independent sets of process on cluster: 指的就是 executor</p>

      
      <!-- reward -->
      
    </div>
    
    
      <!-- copyright -->
      
    <footer class="article-footer">
      
      
  <ul class="article-tag-list" itemprop="keywords"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/tedata/" rel="tag">tedata</a></li></ul>


    </footer>

  </div>

  

  
  
  

  

</article>
    
    <article id="post-SparkCore 01" class="article article-type-post" itemscope
  itemprop="blogPost" data-scroll-reveal>

  <div class="article-inner">
    
    <header class="article-header">
       
<h2 itemprop="name">
  <a class="article-title" href="/2017/03/05/SparkCore%2001/"
    >Spark Core 第一课</a
  >
</h2>
  

    </header>
    

    
    <div class="article-meta">
      <a href="/2017/03/05/SparkCore%2001/" class="article-date">
  <time datetime="2017-03-05T12:43:27.000Z" itemprop="datePublished">2017-03-05</time>
</a>
      
      
      
      
    </div>
    

    

    
    <div class="article-entry" itemprop="articleBody">
      


      

      
      <h1 id="Spark-Core01"><a href="#Spark-Core01" class="headerlink" title="Spark Core01"></a>Spark Core01</h1><p>学习目标：</p>
<ol>
<li>RDD 5大特性</li>
<li>SparkCore 的编程规范</li>
<li>RDD 创建方式</li>
<li>RDD 的算子</li>
</ol>
<p>一. RDD 的5 大特性</p>
<ul>
<li><ol>
<li>RDD 是 一系列的 partition ： A list of partition<br>比如一个数据 val data = Array(1,2,3,4,5,6,7,8,9)<br>会把数据分成一个一个的partition，比如<br>(1,2,3) , (4,5,6), (7,8,9)</li>
</ol>
</li>
<li><ol start="2">
<li>函数(方法) 是作用在每一个 partition 上的: A function for computing each split<br>比如 data.map(_ * 2)  表示 对 每个分区的 元素 * 2</li>
</ol>
</li>
<li><ol start="3">
<li>RDD 之间存在依赖关系：A list of dependency on other RDDS<br>这种依赖指的是 某个RDD 是由另外的RDD 计算推导出来的<br>比如 data.map(_ * 10 )  ==&gt;  data2<br>那么 data2中的 rdd就是由 data 每个元素 * 10 得到的</li>
</ol>
</li>
<li><ol start="4">
<li>对于K,V 结构的RDD，可以进行 partitioner，即把具有相同特征的RDD分到一组</li>
</ol>
</li>
<li><ol start="5">
<li>把计算拉到数据节点</li>
</ol>
</li>
</ul>
<p>二. SparKCore 的编程规范</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">val conf &#x3D; new SparkConf()</span><br><span class="line">  .setMaster(&quot;local[2]&quot;)  传入 master 的类型，一般测试传入 local[2] 即可 </span><br><span class="line">  .setAppName(this.getClass.getSimpleName)  app的名称</span><br><span class="line"></span><br><span class="line">val sc &#x3D; new SparkContext(conf)</span><br></pre></td></tr></table></figure>


<p>如果不传入这两个会报错误，在源码中的体现：<br><img src="/2017/03/05/SparkCore%2001/HEXO/tedatablog/source/pictures/sparkcore01.png" alt="avatar"></p>
<p>三.  RDD 的创建方式</p>
<p>rdd创建方式一:  通过生成数据转换创建</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">scala&gt; val rdd1 &#x3D; sc.parallelize(List(1,2,3,4,5))</span><br><span class="line">rdd1: org.apache.spark.rdd.RDD[Int] &#x3D; ParallelCollectionRDD[6] at parallelize at &lt;console&gt;:24</span><br><span class="line"></span><br><span class="line">scala&gt; rdd1.map(_*2).collect</span><br><span class="line">res4: Array[Int] &#x3D; Array(2, 4, 6, 8, 10)</span><br></pre></td></tr></table></figure>


<p>rdd创建方式二：通过读取文件创建</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">scala&gt; val rdd2 &#x3D; sc.textFile(&quot;file:&#x2F;&#x2F;&#x2F;home&#x2F;bdp&#x2F;tmp&#x2F;wc.data&quot;)</span><br><span class="line">rdd2: org.apache.spark.rdd.RDD[String] &#x3D; file:&#x2F;&#x2F;&#x2F;home&#x2F;bdp&#x2F;tmp&#x2F;wc.data MapPartitionsRDD[5] at textFile at &lt;console&gt;:24</span><br><span class="line"></span><br><span class="line">scala&gt; rdd2.collect</span><br><span class="line">res3: Array[String] &#x3D; Array(boming,boming,boming, niuniu,niuniu, simao)</span><br></pre></td></tr></table></figure>


<p>rdd 创建方式三:<br>通过其他的rdd创建</p>
<p>四.  RDD 算子</p>
<p>RDD 算子可以分为两种类型：transformation 和 action<br>transformation 指的是中间的转换过程，是没有真正执行的<br>action 是真正执行的</p>
<p>transformation 算子</p>
<p>一定要记得，现在的操作都是对RDD 进行的操作，一定要转换成RDD</p>
<ol>
<li>map：作用在rdd 的每个元素上做相同的操作</li>
</ol>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">val rdd &#x3D; sc.parallelize(Array(1,2,3,4,5,6))</span><br><span class="line">scala&gt; rdd.map(x &#x3D;&gt; &#123;x*2&#125;)</span><br><span class="line">res6: Array[Int] &#x3D; Array(2, 4, 6, 8, 10, 12)</span><br></pre></td></tr></table></figure>




<ol start="2">
<li>filter： 过滤出符合条件的元素</li>
</ol>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">scala&gt; rdd.map(x &#x3D;&gt; &#123;x*2&#125;).filter(_ &gt; 6)</span><br><span class="line">res8: Array[Int] &#x3D; Array(8, 10, 12)</span><br><span class="line"></span><br><span class="line">scala&gt; rdd.map(x &#x3D;&gt; &#123;x*2&#125;).filter(x &#x3D;&gt; &#123;x &gt; 6 &#125;)</span><br><span class="line">res9: Array[Int] &#x3D; Array(8, 10, 12)</span><br></pre></td></tr></table></figure>


<ol start="3">
<li>mapPartitions</li>
</ol>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">scala&gt; rdd.mapPartitions( par &#x3D;&gt; &#123;par.map(_*2)&#125;).collect</span><br><span class="line">res20: Array[Int] &#x3D; Array(2, 4, 6, 8, 10, 12)</span><br></pre></td></tr></table></figure>

<p>这个 算子的作用, 比如 有 100个元素, 10个分区, 使用 map 要调用 100次, 使用 mapPartitions 只需10次<br>可以引申到 MySQL的connection, 使用 mapPartitions可以减少很多的连接</p>
<ol start="4">
<li>flatMap : map 之后把 元素拍扁</li>
</ol>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">scala&gt;  val rdd2 &#x3D; sc.parallelize(Array(Array(1,2,3),Array(4,5,6),Array(7,8,9)))</span><br><span class="line">scala&gt; rdd2.flatMap(x &#x3D;&gt; x).collect</span><br><span class="line">res26: Array[Int] &#x3D; Array(1, 2, 3, 4, 5, 6, 7, 8, 9)</span><br></pre></td></tr></table></figure>




<ol start="5">
<li>mapPartitionWIthIndex</li>
</ol>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">rdd1.mapPartitionsWithIndex((index, partition) &#x3D;&gt; &#123;</span><br><span class="line">  partition.map( x &#x3D;&gt; &#123;</span><br><span class="line">    println(s&quot;$&#123;index&#125;, $x&quot;)</span><br><span class="line">  &#125;)</span><br><span class="line">&#125;)</span><br></pre></td></tr></table></figure>

<p>把 partition 和 index 输出</p>
<ol start="6">
<li>mapValues<br>只对KV 结构的V做相同的操作</li>
</ol>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">scala&gt; rdd1.map((_,1)).mapValues(_*3).collect</span><br><span class="line">res4: Array[(String, Int)] &#x3D; Array((iphone11 7000 20,3), (hwpro30 5000 100,3), (xiaomi 3000 200,3), (sumsung 6000 1000,3))</span><br></pre></td></tr></table></figure>


<ol start="7">
<li>reduceByKey</li>
</ol>
<p>把相同的key的元素放在一块，再进行两两相邻的操作</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">scala&gt; val rdd2 &#x3D; sc.parallelize(List(1,2,1,2,3,6,3,1,5))</span><br><span class="line">rdd2: org.apache.spark.rdd.RDD[Int] &#x3D; ParallelCollectionRDD[11] at parallelize at &lt;console&gt;:24</span><br><span class="line"></span><br><span class="line">scala&gt; rdd2.map((_,1)).reduceByKey(_+_).collect</span><br><span class="line">res9: Array[(Int, Int)] &#x3D; Array((1,3), (6,1), (3,2), (5,1), (2,2))</span><br></pre></td></tr></table></figure>



<ol start="8">
<li>union<br>把相同类型的数据放在一起</li>
</ol>
<p>注意，rdd1 和 rdd2 如果类型不同，不能进行 union<br>比如 rdd1为 String，rdd2 为 Int，会报下面的错误</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">scala&gt; rdd1.union(rdd2).collect</span><br><span class="line">&lt;console&gt;:28: error: type mismatch;</span><br><span class="line"> found   : org.apache.spark.rdd.RDD[Int]</span><br><span class="line"> required: org.apache.spark.rdd.RDD[String]</span><br><span class="line">       rdd1.union(rdd2).collect</span><br><span class="line"></span><br><span class="line">scala&gt; val rdd2 &#x3D; sc.parallelize(Array(&quot;ok haixing&quot;))</span><br><span class="line">rdd2: org.apache.spark.rdd.RDD[String] &#x3D; ParallelCollectionRDD[21] at parallelize at &lt;console&gt;:24</span><br><span class="line"></span><br><span class="line">scala&gt; rdd1.union(rdd2).collect</span><br><span class="line">res16: Array[String] &#x3D; Array(iphone11 7000 20, hwpro30 5000 100, xiaomi 3000 200, sumsung 6000 1000, ok haixing)</span><br></pre></td></tr></table></figure>


<ol start="9">
<li>distinct</li>
</ol>
<p>去除 rdd的重复的元素</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">scala&gt; val rdd2 &#x3D; sc.parallelize(List(1,2,1,2,3,6,3,1,5))</span><br><span class="line">rdd2: org.apache.spark.rdd.RDD[Int] &#x3D; ParallelCollectionRDD[23] at parallelize at &lt;console&gt;:24</span><br><span class="line"></span><br><span class="line">scala&gt; rdd2.distinct.collect</span><br><span class="line">res19: Array[Int] &#x3D; Array(1, 6, 3, 5, 2)</span><br></pre></td></tr></table></figure>


<ol start="10">
<li>groupByKey<br>把相同key的元素放到一组，得到的是 CompactBuffer 类型的value</li>
</ol>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">scala&gt; rdd2.map((_,1)).groupByKey().collect</span><br><span class="line">res27: Array[(Int, Iterable[Int])] &#x3D; Array((1,CompactBuffer(1, 1, 1)), (6,CompactBuffer(1)), (3,CompactBuffer(1, 1)), (5,CompactBuffer(1)), (2,CompactBuffer(1, 1)))</span><br></pre></td></tr></table></figure>


<ol start="11">
<li><p>groupBy<br>可以按照不同的分组条件进行分组<br>比如，对 value 进行分组<br>rdd.map((<em>,1)).groupBy(</em>._2).foreach(println)<br>输出：<br>因为value都是1，所以输出的就只有1组<br>(1,CompactBuffer((1,1), (2,1), (3,1), (4,1), (5,1), (6,1)))</p>
</li>
<li><p>join</p>
</li>
</ol>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line">val rdd1 &#x3D; sc.textFile(&quot;data&#x2F;dept.txt&quot;）</span><br><span class="line">val rdd2 &#x3D; sc.textFile(&quot;data&#x2F;emp.txt&quot;)</span><br><span class="line"></span><br><span class="line">val deptRdd &#x3D; rdd1.map(x &#x3D;&gt; &#123;</span><br><span class="line">  val splits &#x3D; x.split(&quot;,&quot;)</span><br><span class="line">  (splits(0).trim, (splits(1).trim, splits(2).trim))</span><br><span class="line">&#125;)</span><br><span class="line">val empRdd &#x3D; rdd2.map(x &#x3D;&gt; &#123;</span><br><span class="line">  val splits &#x3D; x.split(&quot; &quot;)</span><br><span class="line">  (splits(4).trim, (splits(0).trim, splits(1).trim, splits(2).trim, splits(3).trim))</span><br><span class="line">&#125;)</span><br><span class="line">deptRdd.join(empRdd)</span><br><span class="line">deptRdd.leftOutJoin(empRdd)</span><br><span class="line">deptRdd.rightOutJoin(empRdd)</span><br><span class="line">deptRdd.fullOutJoin(empRdd)</span><br><span class="line">(20,(Some((RESEARCH,DALLAS)),Some((1002,bill,ceo,55))))</span><br><span class="line">(30,(Some((SALES,CHICAGO)),Some((1003,cindy,cw,32))))</span><br><span class="line">(40,(Some((OPERATIONS,BOSTON)),None))</span><br><span class="line">(10,(Some((ACCOUNTIN,NEW YORK)),Some((1001,bob,sales,30))))</span><br></pre></td></tr></table></figure>


<ol start="13">
<li>cogroup</li>
</ol>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">deptRdd.cogroup(empRdd).foreach(println)</span><br><span class="line">输出：</span><br><span class="line">(20,(CompactBuffer((RESEARCH,DALLAS)),CompactBuffer((1002,bill,ceo,55))))</span><br><span class="line">(30,(CompactBuffer((SALES,CHICAGO)),CompactBuffer((1003,cindy,cw,32))))</span><br><span class="line">(40,(CompactBuffer((OPERATIONS,BOSTON)),CompactBuffer()))</span><br><span class="line">(10,(CompactBuffer((ACCOUNTIN,NEW YORK)),CompactBuffer((1001,bob,sales,30))))</span><br><span class="line"></span><br><span class="line">join 和 从group的关系</span><br><span class="line">join 底层调用 了 从group</span><br><span class="line">def join[W](other: RDD[(K, W)], partitioner: Partitioner): RDD[(K, (V, W))] &#x3D; 				  self.withScope &#123;</span><br><span class="line">  this.cogroup(other, partitioner).flatMapValues( pair &#x3D;&gt;</span><br><span class="line">    for (v &lt;- pair._1.iterator; w &lt;- pair._2.iterator) yield (v, w)</span><br><span class="line">  )</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>



<ol start="14">
<li>zipWithIndex<br>相当于给每个元素加个索引，形成K,V 结构    </li>
</ol>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">scala&gt; rdd1.zipWithIndex.collectAsMap</span><br><span class="line">res18: scala.collection.Map[Int,Long] &#x3D; Map(2 -&gt; 1, 5 -&gt; 4, 4 -&gt; 3, 1 -&gt; 0, 3 -&gt; 2, 6 -&gt; 5)</span><br></pre></td></tr></table></figure>

      
      <!-- reward -->
      
    </div>
    
    
      <!-- copyright -->
      
    <footer class="article-footer">
      
      
  <ul class="article-tag-list" itemprop="keywords"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/tedata/" rel="tag">tedata</a></li></ul>


    </footer>

  </div>

  

  
  
  

  

</article>
    
    <article id="post-mysql双机热备" class="article article-type-post" itemscope
  itemprop="blogPost" data-scroll-reveal>

  <div class="article-inner">
    
    <header class="article-header">
       
<h2 itemprop="name">
  <a class="article-title" href="/2016/03/28/mysql%E5%8F%8C%E6%9C%BA%E7%83%AD%E5%A4%87/"
    >MySQL双击热备实现</a
  >
</h2>
  

    </header>
    

    
    <div class="article-meta">
      <a href="/2016/03/28/mysql%E5%8F%8C%E6%9C%BA%E7%83%AD%E5%A4%87/" class="article-date">
  <time datetime="2016-03-28T12:43:27.000Z" itemprop="datePublished">2016-03-28</time>
</a>
      
      
      
      
    </div>
    

    

    
    <div class="article-entry" itemprop="articleBody">
      


      

      
      <h1 id="mysql-双机热备"><a href="#mysql-双机热备" class="headerlink" title="mysql 双机热备"></a>mysql 双机热备</h1><h3 id="一-环境及注意事项"><a href="#一-环境及注意事项" class="headerlink" title="一. 环境及注意事项"></a>一. 环境及注意事项</h3><p>msyql双机热备即mysql 的 主主模式，即</p>
<p>mysql的机器可以互相读写，保障了数据的同步</p>
<p>下面是mysql 主主的实现过程:</p>
<p>测试环境: CentOS 6.8</p>
<p>主机 ip: 192.168.163.22;192.168.163.23</p>
<p>配置注意项:</p>
<ol>
<li>两台主机的mysql 版本要一致</li>
<li>两台mysql的初始状态要一致，即mysql库中的数据库要一致</li>
<li>关闭防火墙</li>
</ol>
<h3 id="二-配置过程"><a href="#二-配置过程" class="headerlink" title="二. 配置过程"></a>二. 配置过程</h3><p>1.配置 msyql 配置文件:</p>
<p>在 192.168.163.22 上的配置如下:</p>
<p>vim /etc/my.cnf：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line">log-bin&#x3D;mysql-bin  &#x2F;&#x2F;表示mysql的二进制日志，因为主主的实现是通过读取二进制日志的位置实现的</span><br><span class="line">binlog_format&#x3D;mixed &#x2F;&#x2F;二进制日志格式</span><br><span class="line">server-id&#x3D;22 &#x2F;&#x2F;server-id号，这个号码两台主机上一定不能一样,通常可取ip的最后一位</span><br><span class="line"></span><br><span class="line">read-only&#x3D;0 &#x2F;&#x2F;只读模式为False</span><br><span class="line">binlog-ignore-db&#x3D;mysql  </span><br><span class="line">binlog-ignore-db&#x3D;information_schema</span><br><span class="line">binlog-ignore-db&#x3D;performance_schema</span><br><span class="line">binlog-ignore-db&#x3D;sys  &#x2F;&#x2F; 这几段表示 忽略同步的数据库</span><br><span class="line"></span><br><span class="line">auto-increment-increment&#x3D;1</span><br><span class="line">auto-increment-offset&#x3D;1 &#x2F;&#x2F; 用于在 双主（多主循环）互相备份。 因为每台数据库服务器都可能在同一个表中插入数据，如果表有一个自动增长的主键，那么就会在多服务器上出现主键冲突。  解决这个问题的办法就是让每个数据库的自增主键不连续。  上图说是， 我假设需要将来可能需要10台服务器做备份， 所以auto-increment-increment 设为10.   而 auto-increment-offset&#x3D;1 表示这台服务器的序号。 从1开始， 不超过auto-increment-increment。</span><br><span class="line">这样做之后， 我在这台服务器上插入的第一个id就是 1， 第二行的id就是 11了， 而不是2</span><br><span class="line"></span><br><span class="line">replicate-ignore-db&#x3D;mysql</span><br><span class="line">replicate-ignore-db&#x3D;information_schema</span><br><span class="line">replicate-ignore-db&#x3D;performance_schema</span><br><span class="line">replicate-ignore-db&#x3D;sys &#x2F;&#x2F; 这几段表示 复制忽略的数据库</span><br><span class="line">relay_log&#x3D;mysqld-relay-bin &#x2F;&#x2F;中继日志</span><br><span class="line">log-slave-updates&#x3D;ON &#x2F;&#x2F;中继日志执行之后，这些变化是否需要计入自己的binarylog。 当你的B服务器需要作为另外一个服务器的主服务器的时候需要打开。  就是双主互相备份，或者多主循环备份。 我们这里需要， 所以打开。</span><br></pre></td></tr></table></figure>



<p>在 192.168.163.23 上的配置如下:</p>
<p>vim /etc/my.cnf:</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">log-bin&#x3D;mysql-bin</span><br><span class="line">binlog_format&#x3D;mixed</span><br><span class="line">server-id&#x3D;23</span><br><span class="line"></span><br><span class="line">read-only&#x3D;0</span><br><span class="line">binlog-ignore-db&#x3D;mysql</span><br><span class="line">binlog-ignore-db&#x3D;information_schema</span><br><span class="line">binlog-ignore-db&#x3D;performance_schema</span><br><span class="line">binlog-ignore-db&#x3D;sys</span><br><span class="line">auto-increment-increment&#x3D;10</span><br><span class="line">auto-increment-offset&#x3D;2</span><br><span class="line"></span><br><span class="line">replicate-ignore-db&#x3D;mysql</span><br><span class="line">replicate-ignore-db&#x3D;information_schema</span><br><span class="line">replicate-ignore-db&#x3D;performance_schema</span><br><span class="line">replicate-ignore-db&#x3D;sys</span><br><span class="line">relay_log&#x3D;mysqld-relay-bin</span><br><span class="line">log-slave-updates&#x3D;ON</span><br></pre></td></tr></table></figure>


<p>2.对两台主机进行授权:</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">22上:</span><br><span class="line">GRANT REPLICATION SLAVE ON *.* to &#39;root&#39;@&#39;192.168.163.23&#39; identified by &#39;Bb123456@&#39;;</span><br><span class="line"></span><br><span class="line">23上:</span><br><span class="line">GRANT REPLICATION SLAVE ON *.* to &#39;root&#39;@&#39;192.168.163.22&#39; identified by &#39;Bb123456@&#39;;</span><br></pre></td></tr></table></figure>


<p>该步骤的目的是让两台主机在连接时拥有权限，否则连接不被允许</p>
<p>3.进行主从sql 设置:</p>
<p>查看 两台主机的master 状态:</p>
<p>22上:<br><img src="/2016/03/28/mysql%E5%8F%8C%E6%9C%BA%E7%83%AD%E5%A4%87/mysqls1.png" alt="avatar"></p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">在 22 上执行:</span><br><span class="line">change master to master_host&#x3D;&#39;192.168.163.23&#39;,master_user&#x3D;&#39;root&#39;,master_password&#x3D;&#39;Bb123456@&#39;,master_log_file&#x3D;&#39;mysql-bin.000003&#39;,master_log_pos&#x3D;2266;</span><br></pre></td></tr></table></figure>


<p>23上:<br><img src="/2016/03/28/mysql%E5%8F%8C%E6%9C%BA%E7%83%AD%E5%A4%87/mysqls2.png" alt="avatar"></p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">在 23 上执行:</span><br><span class="line">change master to master_host&#x3D;&#39;192.168.163.23&#39;,master_user&#x3D;&#39;root&#39;,master_password&#x3D;&#39;Bb123456@&#39;,master_log_file&#x3D;&#39;mysql-bin.000007&#39;,master_log_pos&#x3D;503;</span><br></pre></td></tr></table></figure>


<p>该操作就让mysql的两台主机 从该二级制日志的位置开始同步</p>
<p>4.开启并查看同步状态:</p>
<p>两台主机上都执行以下操作:</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">start slave;</span><br></pre></td></tr></table></figure>
<p><img src="/2016/03/28/mysql%E5%8F%8C%E6%9C%BA%E7%83%AD%E5%A4%87/mysqls3.png" alt="avatar"></p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">show slave status\G;</span><br></pre></td></tr></table></figure>
<p><img src="/2016/03/28/mysql%E5%8F%8C%E6%9C%BA%E7%83%AD%E5%A4%87/mysqls4.png" alt="avatar"></p>
<p>—两台主机上只用 这两个为 Yes是才表示同步成功</p>
<p>5.测试同步</p>
<p>首先查看两台主机数据库的初始状态:<br><img src="/2016/03/28/mysql%E5%8F%8C%E6%9C%BA%E7%83%AD%E5%A4%87/mysqls5.png" alt="avatar"></p>
<p>然后在 22上删除 数据库 bigeye_dev<br><img src="/2016/03/28/mysql%E5%8F%8C%E6%9C%BA%E7%83%AD%E5%A4%87/mysqls6.png" alt="avatar"></p>
<p>可以看到，从库中的bigeye_dev也删除了</p>
<p>最后到 23 上创建数据库 bigeye_dev<br><img src="/2016/03/28/mysql%E5%8F%8C%E6%9C%BA%E7%83%AD%E5%A4%87/mysqls7.png" alt="avatar"></p>
<p>可以看到，bigeye_dev 也被创建了</p>
<p>报错:</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">Error: &#39;Lost connection to MySQL server at &#39;reading initial communication packet&#39;, system error: 113&#39; errno: 2013 retry-time: 60 retries: 86400</span><br></pre></td></tr></table></figure>


<p>原因:没有授权，导致连接时不被允许</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">解决:GRANT REPLICATION SLAVE ON *.* to &#39;root&#39;@&#39;192.168.163.22&#39; identified by &#39;Bb123456@&#39;;</span><br></pre></td></tr></table></figure>



<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">报错:Fatal error: The slave I&#x2F;O thread stops because master and slave have equal MySQL server UUIDs; these UUIDs must be different for replication to work.</span><br></pre></td></tr></table></figure>


<p>原因:UUID冲突</p>
<p>解决:vi /var/lib/mysql/auto.cnf<br><img src="/2016/03/28/mysql%E5%8F%8C%E6%9C%BA%E7%83%AD%E5%A4%87/mysqls8.png" alt="avatar"></p>
<p>这两个uuid改成不一致即可</p>

      
      <!-- reward -->
      
    </div>
    
    
      <!-- copyright -->
      
    <footer class="article-footer">
      
      
  <ul class="article-tag-list" itemprop="keywords"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/tedata/" rel="tag">tedata</a></li></ul>


    </footer>

  </div>

  

  
  
  

  

</article>
    
    <article id="post-shell三剑客" class="article article-type-post" itemscope
  itemprop="blogPost" data-scroll-reveal>

  <div class="article-inner">
    
    <header class="article-header">
       
<h2 itemprop="name">
  <a class="article-title" href="/2015/04/23/shell%E4%B8%89%E5%89%91%E5%AE%A2/"
    >shell 使用</a
  >
</h2>
  

    </header>
    

    
    <div class="article-meta">
      <a href="/2015/04/23/shell%E4%B8%89%E5%89%91%E5%AE%A2/" class="article-date">
  <time datetime="2015-04-23T12:43:27.000Z" itemprop="datePublished">2015-04-23</time>
</a>
      
      
      
      
    </div>
    

    

    
    <div class="article-entry" itemprop="articleBody">
      


      

      
      <h1 id="shell-三剑客-grep-sed-awk"><a href="#shell-三剑客-grep-sed-awk" class="headerlink" title="shell 三剑客- grep, sed, awk"></a>shell 三剑客- grep, sed, awk</h1><h3 id="一-awk"><a href="#一-awk" class="headerlink" title="一. awk"></a>一. awk</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line">awk  &#39;条件类型1&#123;动作1&#125; 条件类型2&#123;动作2&#125; ...&#39;  filename</span><br><span class="line"></span><br><span class="line">awk -F &#39;:&#39;  &#39;&#123;OFS&#x3D;&quot;:&quot;; $3&gt;100,$7&#x3D;$3+$4 &#123;print $0&#125;&#39; 12.txt</span><br><span class="line">要区分动作和条件，条件可以不用加符号，动作要加符号 &#123; &#125;</span><br><span class="line"></span><br><span class="line">例如</span><br><span class="line">awk -F &#39;:&#39; &#39;&#123;sum&#x3D;sum+$3&#125; END &#123;print sum&#125;&#39; 12.txt</span><br><span class="line">因为 sum&#x3D;sum+$3 和 print sum都是动作，所以要用 &#123; &#125;</span><br><span class="line"></span><br><span class="line">awk的内置变量 </span><br><span class="line">NR 表示行数</span><br><span class="line">NF 表示列数 $NF&#x3D;7</span><br><span class="line"></span><br><span class="line">awk的打印功能</span><br><span class="line">awk -F &#39;:&#39;  &#39;&#123;print $1,$3&#125;&#39;  12.txt 打印文档12.txt的第1和第三段</span><br><span class="line"></span><br><span class="line">awk的匹配功能</span><br><span class="line">awk -F &#39;:&#39;  &#39;&#x2F;rr&#x2F;&#39; 12.txt  匹配 12.txt中带有rr的字符</span><br><span class="line"></span><br><span class="line">awk中的判断与句匹配</span><br><span class="line">awk -F &#39;:&#39; &#39;$3&gt;100 &#123;print $7&#125;&#39; 12.txt</span><br><span class="line">awk -F &#39;:&#39; &#39;$3&#x3D;&#x3D;$4;$1~&quot;root&quot; &#123;print $7&#125;&#39; 12.txt</span><br></pre></td></tr></table></figure>



<h3 id="二-grep-用法"><a href="#二-grep-用法" class="headerlink" title="二. grep 用法"></a>二. grep 用法</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">grep &#39;过滤内容’ 文档</span><br><span class="line">grep  &#39;abc&#39;  1.txt</span><br><span class="line"></span><br><span class="line">grep -E 或egrep  代表可以匹配的时候脱义特殊字符，例如 + ？</span><br><span class="line">grep --color 过滤时候给匹配到的文档加上颜色 </span><br><span class="line">grep -v 表示过滤出和 grep 匹配相反的内容</span><br></pre></td></tr></table></figure>


<h3 id="三-sed-用法"><a href="#三-sed-用法" class="headerlink" title="三. sed 用法"></a>三. sed 用法</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line">sed 打印功能</span><br><span class="line">sed   -n &#39;1,5&#39;p 12.txt 表示打印文档12.txt 的1-5段</span><br><span class="line">注意要加 -n 和p </span><br><span class="line">不加-n 会打印所有行</span><br><span class="line">p表示打印意思</span><br><span class="line">sed的脱义符号是 -r</span><br><span class="line"></span><br><span class="line">sed匹配功能</span><br><span class="line">sed  -n &#39;&#x2F;匹配内容&#x2F;&#39;p 12.txt</span><br><span class="line">注意也有 -n 和p</span><br><span class="line"></span><br><span class="line">sed同时进行多个任务</span><br><span class="line">sed -e &#39;&#x2F;root&#x2F;p&#39; -e &#39;&#x2F;body&#39;p 12.txt</span><br><span class="line">sed &#39;&#x2F;root&#x2F;p;&#x2F;body&#x2F;&#39;p 12.txt</span><br><span class="line"></span><br><span class="line">sed 删除功能</span><br><span class="line">sed  &#39;1,5&#39;d 12.txt 表示显示删除文档12.txt的1-5段</span><br><span class="line">sed -i ‘1,5&#39; 12.txt 表示真实删除文档12.txt的1-5段</span><br><span class="line"></span><br><span class="line">sed替换功能</span><br><span class="line">sed  &#39;s&#x2F;被替换内容&#x2F;替换内容&#x2F;g&#39; 文档 全部替换</span><br><span class="line">sed  &#39;1,5s&#x2F;被替换内容&#x2F;替换内容&#x2F;g&#39; 文档 部分替换，替换1-5行</span><br><span class="line"></span><br><span class="line">注意一个语句</span><br><span class="line">sed &#39;s&#x2F;(^[0-9a-zA-Z].*)(:.*:)(.*$)&#x2F;\3\2\1&#x2F;&#39; 12.txt</span><br><span class="line">表示把12.txt最后部分和开始部分调换，注意没有g</span><br></pre></td></tr></table></figure>



      
      <!-- reward -->
      
    </div>
    
    
      <!-- copyright -->
      
    <footer class="article-footer">
      
      
  <ul class="article-tag-list" itemprop="keywords"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/tedata/" rel="tag">tedata</a></li></ul>


    </footer>

  </div>

  

  
  
  

  

</article>
    
  </article>
  

  
  <nav class="page-nav">
    
    <a class="extend prev" rel="prev" href="/">上一页</a><a class="page-number" href="/">1</a><span class="page-number current">2</span>
  </nav>
  
</section>
</div>

      <footer class="footer">
  <div class="outer">
    <ul class="list-inline">
      <li>
        &copy;
        2015-2020
        tedwang0714
      </li>
      <li>
        
          Powered by
        
        
        <a href="https://hexo.io" target="_blank">Hexo</a> Theme <a href="https://github.com/Shen-Yu/hexo-theme-ayer" target="_blank">Ayer</a>
        
      </li>
    </ul>
    <ul class="list-inline">
      <li>
        
        
        <span>
  <i>PV:<span id="busuanzi_value_page_pv"></span></i>
  <i>UV:<span id="busuanzi_value_site_uv"></span></i>
</span>
        
      </li>
      <li>
        <!-- cnzz统计 -->
        
        <script type="text/javascript" src='https://s9.cnzz.com/z_stat.php?id=1278069914&amp;web_id=1278069914'></script>
        
      </li>
    </ul>
  </div>
</footer>
    <div class="to_top">
        <div class="totop" id="totop">
  <i class="ri-arrow-up-line"></i>
</div>
      </div>
    </main>
      <aside class="sidebar">
        <button class="navbar-toggle"></button>
<nav class="navbar">
  
  <div class="logo">
    <a href="/"><img src="/images/ayer-side.svg" alt="TedWang 的大数据"></a>
  </div>
  
  <ul class="nav nav-main">
    
    <li class="nav-item">
      <a class="nav-item-link" href="/">主页</a>
    </li>
    
    <li class="nav-item">
      <a class="nav-item-link" href="/archives">归档</a>
    </li>
    
    <li class="nav-item">
      <a class="nav-item-link" href="/categories">分类</a>
    </li>
    
    <li class="nav-item">
      <a class="nav-item-link" href="/tags">标签</a>
    </li>
    
  </ul>
</nav>
<nav class="navbar navbar-bottom">
  <ul class="nav">
    <li class="nav-item">
      
      <a class="nav-item-link nav-item-search"  title="Search">
        <i class="ri-search-line"></i>
      </a>
      
      
      <a class="nav-item-link" target="_blank" href="/atom.xml" title="RSS Feed">
        <i class="ri-rss-line"></i>
      </a>
      
    </li>
  </ul>
</nav>
<div class="search-form-wrap">
  <div class="local-search local-search-plugin">
  <input type="search" id="local-search-input" class="local-search-input" placeholder="Search...">
  <div id="local-search-result" class="local-search-result"></div>
</div>
</div>
      </aside>
      <div id="mask"></div>

<!-- #reward -->
<div id="reward">
  <span class="close"><i class="ri-close-line"></i></span>
  <p class="reward-p"><i class="ri-cup-line"></i>请我喝杯咖啡吧~</p>
  <div class="reward-box">
    
    <div class="reward-item">
      <img class="reward-img" src="/images/alipay.jpg">
      <span class="reward-type">支付宝</span>
    </div>
    
    
    <div class="reward-item">
      <img class="reward-img" src="/images/wechat.jpg">
      <span class="reward-type">微信</span>
    </div>
    
  </div>
</div>
      
<script src="/js/jquery-2.0.3.min.js"></script>


<script src="/js/jquery.justifiedGallery.min.js"></script>


<script src="/js/lazyload.min.js"></script>


<script src="/js/busuanzi-2.3.pure.min.js"></script>


<script src="/js/share.js"></script>



<script src="/fancybox/jquery.fancybox.min.js"></script>




<script>
  try {
    var typed = new Typed("#subtitle", {
    strings: ['人类精神必须置于技术之上','',''],
    startDelay: 0,
    typeSpeed: 200,
    loop: true,
    backSpeed: 100,
    showCursor: true
    });
  } catch (err) {
  }
  
</script>




<script>
  var ayerConfig = {
    mathjax: false
  }
</script>


<script src="/js/ayer.js"></script>


<script src="https://cdn.jsdelivr.net/npm/jquery-modal@0.9.2/jquery.modal.min.js"></script>
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/jquery-modal@0.9.2/jquery.modal.min.css">


<!-- Root element of PhotoSwipe. Must have class pswp. -->
<div class="pswp" tabindex="-1" role="dialog" aria-hidden="true">

    <!-- Background of PhotoSwipe. 
         It's a separate element as animating opacity is faster than rgba(). -->
    <div class="pswp__bg"></div>

    <!-- Slides wrapper with overflow:hidden. -->
    <div class="pswp__scroll-wrap">

        <!-- Container that holds slides. 
            PhotoSwipe keeps only 3 of them in the DOM to save memory.
            Don't modify these 3 pswp__item elements, data is added later on. -->
        <div class="pswp__container">
            <div class="pswp__item"></div>
            <div class="pswp__item"></div>
            <div class="pswp__item"></div>
        </div>

        <!-- Default (PhotoSwipeUI_Default) interface on top of sliding area. Can be changed. -->
        <div class="pswp__ui pswp__ui--hidden">

            <div class="pswp__top-bar">

                <!--  Controls are self-explanatory. Order can be changed. -->

                <div class="pswp__counter"></div>

                <button class="pswp__button pswp__button--close" title="Close (Esc)"></button>

                <button class="pswp__button pswp__button--share" style="display:none" title="Share"></button>

                <button class="pswp__button pswp__button--fs" title="Toggle fullscreen"></button>

                <button class="pswp__button pswp__button--zoom" title="Zoom in/out"></button>

                <!-- Preloader demo http://codepen.io/dimsemenov/pen/yyBWoR -->
                <!-- element will get class pswp__preloader--active when preloader is running -->
                <div class="pswp__preloader">
                    <div class="pswp__preloader__icn">
                        <div class="pswp__preloader__cut">
                            <div class="pswp__preloader__donut"></div>
                        </div>
                    </div>
                </div>
            </div>

            <div class="pswp__share-modal pswp__share-modal--hidden pswp__single-tap">
                <div class="pswp__share-tooltip"></div>
            </div>

            <button class="pswp__button pswp__button--arrow--left" title="Previous (arrow left)">
            </button>

            <button class="pswp__button pswp__button--arrow--right" title="Next (arrow right)">
            </button>

            <div class="pswp__caption">
                <div class="pswp__caption__center"></div>
            </div>

        </div>

    </div>

</div>

<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/photoswipe@4.1.3/dist/photoswipe.min.css">
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/photoswipe@4.1.3/dist/default-skin/default-skin.css">
<script src="https://cdn.jsdelivr.net/npm/photoswipe@4.1.3/dist/photoswipe.min.js"></script>
<script src="https://cdn.jsdelivr.net/npm/photoswipe@4.1.3/dist/photoswipe-ui-default.min.js"></script>

<script>
    function viewer_init() {
        let pswpElement = document.querySelectorAll('.pswp')[0];
        let $imgArr = document.querySelectorAll(('.article-entry img:not(.reward-img)'))

        $imgArr.forEach(($em, i) => {
            $em.onclick = () => {
                // slider展开状态
                // todo: 这样不好，后面改成状态
                if (document.querySelector('.left-col.show')) return
                let items = []
                $imgArr.forEach(($em2, i2) => {
                    let img = $em2.getAttribute('data-idx', i2)
                    let src = $em2.getAttribute('data-target') || $em2.getAttribute('src')
                    let title = $em2.getAttribute('alt')
                    // 获得原图尺寸
                    const image = new Image()
                    image.src = src
                    items.push({
                        src: src,
                        w: image.width || $em2.width,
                        h: image.height || $em2.height,
                        title: title
                    })
                })
                var gallery = new PhotoSwipe(pswpElement, PhotoSwipeUI_Default, items, {
                    index: parseInt(i)
                });
                gallery.init()
            }
        })
    }
    viewer_init()
</script>




<script type="text/javascript" src="https://js.users.51.la/20544303.js"></script>
  </div>
</body>

</html>