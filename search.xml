<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title>hadoopjob</title>
    <url>/2020/02/22/hadoopjob/</url>
    <content><![CDATA[<h1 id="Hadoop-job-的执行流程"><a href="#Hadoop-job-的执行流程" class="headerlink" title="Hadoop job 的执行流程"></a>Hadoop job 的执行流程</h1><p>Hadoop 中的 job 任务包含 Mapper 和 Reducer 过程；但这只是最简单的划分，为了理清 job的执行过程将其划分为：job 层、MR 层，每层又由很多小部分组成。</p>
<p>job 层<br>job.waitForCompletion(true) 是我们提交 job 任务代码，从这个切入点出发：</p>
<a id="more"></a>

<h3 id="一、判断-job-状态，准备提交任务"><a href="#一、判断-job-状态，准备提交任务" class="headerlink" title="一、判断 job 状态，准备提交任务"></a>一、判断 job 状态，准备提交任务</h3><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">&#x2F;&#x2F; job.java</span><br><span class="line">public boolean waitForCompletion(boolean verbose</span><br><span class="line">                               ) throws IOException, InterruptedException,</span><br><span class="line">                                        ClassNotFoundException &#123;</span><br><span class="line">&#x2F;&#x2F; state 初始化就是 JobState.DEFINE</span><br><span class="line">&#x2F;&#x2F; 注意 state 标识跟随 job 整个周期</span><br><span class="line">if (state &#x3D;&#x3D; JobState.DEFINE) &#123;</span><br><span class="line">  &#x2F;&#x2F; 重点在这里</span><br><span class="line">  submit();</span><br><span class="line">&#125;</span><br><span class="line">&#x2F;&#x2F; 等待 job 结束</span><br><span class="line">if (verbose) &#123;</span><br><span class="line">  &#x2F;&#x2F; monitorAndPrintJob 中 1s 调用 isComplete() 一次；progMonitorPollIntervalMillis 控制</span><br><span class="line">  monitorAndPrintJob();</span><br><span class="line">&#125; else &#123;</span><br><span class="line">  &#x2F;&#x2F; get the completion poll interval from the client.</span><br><span class="line">  int completionPollIntervalMillis &#x3D; </span><br><span class="line">    Job.getCompletionPollInterval(cluster.getConf());</span><br><span class="line"></span><br><span class="line">  &#x2F;&#x2F; completionPollIntervalMillis 调用一次，isComplete 实质是调用 clientCache.getClient(jobId).getJobStatus(jobId) 获取状态</span><br><span class="line">  while (!isComplete()) &#123;</span><br><span class="line">    try &#123;</span><br><span class="line">      Thread.sleep(completionPollIntervalMillis);</span><br><span class="line">    &#125; catch (InterruptedException ie) &#123;</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br><span class="line">return isSuccessful();</span><br></pre></td></tr></table></figure>

<h3 id="二、提交任务，修改-job-状态为-RUNNING"><a href="#二、提交任务，修改-job-状态为-RUNNING" class="headerlink" title="二、提交任务，修改 job 状态为 RUNNING"></a>二、提交任务，修改 job 状态为 RUNNING</h3><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">&#x2F;&#x2F; job.java</span><br><span class="line">&#x2F;** * Submit the job to the cluster and return immediately. * @throws IOException *&#x2F;</span><br><span class="line">public void submit() </span><br><span class="line">     throws IOException, InterruptedException, ClassNotFoundException &#123;</span><br><span class="line">&#x2F;&#x2F; 确认状态</span><br><span class="line">ensureState(JobState.DEFINE);</span><br><span class="line">setUseNewAPI();</span><br><span class="line">&#x2F;&#x2F; 为了得到 cluster -&gt; YARNRunner（集群）</span><br><span class="line">connect();</span><br><span class="line">&#x2F;&#x2F; 有 cluster 后，构建 submitter 对象</span><br><span class="line">final JobSubmitter submitter &#x3D; </span><br><span class="line">    getJobSubmitter(cluster.getFileSystem(), cluster.getClient());</span><br><span class="line">status &#x3D; ugi.doAs(new PrivilegedExceptionAction&lt;JobStatus&gt;() &#123;</span><br><span class="line">  public JobStatus run() throws IOException, InterruptedException, </span><br><span class="line">  ClassNotFoundException &#123;</span><br><span class="line">	&#x2F;&#x2F; submitter 对象提交任务</span><br><span class="line">    return submitter.submitJobInternal(Job.this, cluster);</span><br><span class="line">  &#125;</span><br><span class="line">&#125;);</span><br><span class="line">state &#x3D; JobState.RUNNING;</span><br><span class="line">LOG.info(&quot;The url to track the job: &quot; + getTrackingURL());</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>


<h3 id="三、submitJobInternal-才是我们真正需要关注的东西"><a href="#三、submitJobInternal-才是我们真正需要关注的东西" class="headerlink" title="三、submitJobInternal 才是我们真正需要关注的东西"></a>三、submitJobInternal 才是我们真正需要关注的东西</h3><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">&#x2F;&#x2F; JobSubmitter.java</span><br><span class="line"> JobStatus submitJobInternal(Job job, Cluster cluster) </span><br><span class="line">	throws ClassNotFoundException, InterruptedException, IOException &#123;</span><br><span class="line"></span><br><span class="line">&#x2F;&#x2F; 检查输出目录</span><br><span class="line">checkSpecs(job);</span><br><span class="line">...</span><br><span class="line">&#x2F;&#x2F; 创建一个 staging 目录</span><br><span class="line">Path jobStagingArea &#x3D; JobSubmissionFiles.getStagingDir(cluster, conf);</span><br><span class="line">...</span><br><span class="line">&#x2F;&#x2F; 获取 job ID:application_id &#123; id: 3 cluster_timestamp: 1567478115673 &#125;</span><br><span class="line">JobID jobId &#x3D; submitClient.getNewJobID();</span><br><span class="line">job.setJobID(jobId);</span><br><span class="line"></span><br><span class="line">&#x2F;&#x2F; 本Job对应的目录: staging + jobid &#x3D;&gt; &#x2F;tmp&#x2F;hadoop-yarn&#x2F;staging&#x2F;hadoop&#x2F;.staging&#x2F;job_1567478115673_0003</span><br><span class="line">&#x2F;&#x2F; 此目录存放 job 涉及到的jar、tmpfiles 等文件 </span><br><span class="line">Path submitJobDir &#x3D; new Path(jobStagingArea, jobId.toString());</span><br><span class="line"></span><br><span class="line">try &#123;</span><br><span class="line">  ...</span><br><span class="line"></span><br><span class="line">  &#x2F;&#x2F; job 的资源复制到 submitJobDir 目录(JobResourceUploader.java)</span><br><span class="line">  copyAndConfigureFiles(job, submitJobDir);</span><br><span class="line"> </span><br><span class="line">  &#x2F;&#x2F; &#x2F;tmp&#x2F;hadoop-yarn&#x2F;staging&#x2F;hadoop&#x2F;.staging&#x2F;job_1567478115673_0003&#x2F;job.xml</span><br><span class="line">  &#x2F;&#x2F; 保存 job 任务的属性</span><br><span class="line">  Path submitJobFile &#x3D; JobSubmissionFiles.getJobConfPath(submitJobDir);</span><br><span class="line">  </span><br><span class="line">  &#x2F;&#x2F; InputSplit 这是重点，MR 中会详细分析</span><br><span class="line">  LOG.debug(&quot;Creating splits at &quot; + jtFs.makeQualified(submitJobDir));</span><br><span class="line">  int maps &#x3D; writeSplits(job, submitJobDir);</span><br><span class="line">  conf.setInt(MRJobConfig.NUM_MAPS, maps);</span><br><span class="line">  LOG.info(&quot;number of splits:&quot; + maps);</span><br><span class="line"></span><br><span class="line">  ...</span><br><span class="line">	</span><br><span class="line">  &#x2F;&#x2F; Job 的信息写入到 job.xml</span><br><span class="line">  writeConf(conf, submitJobFile);</span><br><span class="line">  </span><br><span class="line">  &#x2F;&#x2F; 下面才真的提交任务 ^-^</span><br><span class="line">  &#x2F;&#x2F;</span><br><span class="line">  &#x2F;&#x2F; Now, actually submit the job (using the submit name)</span><br><span class="line">  &#x2F;&#x2F;</span><br><span class="line">  status &#x3D; submitClient.submitJob(</span><br><span class="line">      jobId, submitJobDir.toString(), job.getCredentials());</span><br><span class="line">  if (status !&#x3D; null) &#123;</span><br><span class="line">    return status;</span><br><span class="line">  &#125; else &#123;</span><br><span class="line">    throw new IOException(&quot;Could not launch job&quot;);</span><br><span class="line">  &#125;</span><br><span class="line">&#125; finally &#123;</span><br><span class="line">  if (status &#x3D;&#x3D; null) &#123;</span><br><span class="line">    LOG.info(&quot;Cleaning up the staging area &quot; + submitJobDir);</span><br><span class="line">    if (jtFs !&#x3D; null &amp;&amp; submitJobDir !&#x3D; null)</span><br><span class="line">	  &#x2F;&#x2F; 任务提交成功后，清空 Job 对应的目录</span><br><span class="line">      jtFs.delete(submitJobDir, true);</span><br><span class="line"></span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>


<p>3.1、检查输出目录(已存在抛异常)</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">&#x2F;&#x2F; 继承 OutputFormat 的类，即任务输出类</span><br><span class="line">public void checkOutputSpecs(JobContext job</span><br><span class="line">                           ) throws FileAlreadyExistsException, IOException&#123;</span><br><span class="line">&#x2F;&#x2F; Ensure that the output directory is set and not already there</span><br><span class="line">Path outDir &#x3D; getOutputPath(job);</span><br><span class="line">&#x2F;&#x2F; 输出路径为空， &quot;Output directory not set.&quot; 异常</span><br><span class="line">if (outDir &#x3D;&#x3D; null) &#123;</span><br><span class="line">  throw new InvalidJobConfException(&quot;Output directory not set.&quot;);</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">&#x2F;&#x2F; get delegation token for outDir&#39;s file system</span><br><span class="line">TokenCache.obtainTokensForNamenodes(job.getCredentials(),</span><br><span class="line">    new Path[] &#123; outDir &#125;, job.getConfiguration());</span><br><span class="line"></span><br><span class="line">&#x2F;&#x2F; 输出目录已存在，&quot;Output directory &quot; + outDir + &quot; already exists&quot; 异常</span><br><span class="line">if (outDir.getFileSystem(job.getConfiguration()).exists(outDir)) &#123;</span><br><span class="line">  throw new FileAlreadyExistsException(&quot;Output directory &quot; + outDir + </span><br><span class="line">                                       &quot; already exists&quot;);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>


<p>3.2、staging 目录</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">&#x2F;&#x2F; MRApps.java &#x3D;&gt; &#x2F;tmp&#x2F;hadoop-yarn&#x2F;staging&#x2F;hadoop&#x2F;.staging</span><br><span class="line">private static final String STAGING_CONSTANT &#x3D; &quot;.staging&quot;;</span><br><span class="line">public static Path getStagingAreaDir(Configuration conf, String user) &#123;</span><br><span class="line">return new Path(conf.get(MRJobConfig.MR_AM_STAGING_DIR,</span><br><span class="line">    MRJobConfig.DEFAULT_MR_AM_STAGING_DIR)</span><br><span class="line">    + Path.SEPARATOR + user + Path.SEPARATOR + STAGING_CONSTANT);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>


<p>3.3、job 提交到 ResourceManager</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">&#x2F;&#x2F; YARNRunner.java</span><br><span class="line">public JobStatus submitJob(JobID jobId, String jobSubmitDir, Credentials ts)</span><br><span class="line">throws IOException, InterruptedException &#123;</span><br><span class="line">addHistoryToken(ts);</span><br><span class="line"></span><br><span class="line">ApplicationSubmissionContext appContext &#x3D;</span><br><span class="line">  createApplicationSubmissionContext(conf, jobSubmitDir, ts);</span><br><span class="line"></span><br><span class="line">&#x2F;&#x2F; Submit to ResourceManager</span><br><span class="line">try &#123;</span><br><span class="line">  ApplicationId applicationId &#x3D;</span><br><span class="line">      resMgrDelegate.submitApplication(appContext);</span><br><span class="line"></span><br><span class="line">  ApplicationReport appMaster &#x3D; resMgrDelegate</span><br><span class="line">      .getApplicationReport(applicationId);</span><br><span class="line">  String diagnostics &#x3D;</span><br><span class="line">      (appMaster &#x3D;&#x3D; null ?</span><br><span class="line">          &quot;application report is null&quot; : appMaster.getDiagnostics());</span><br><span class="line">  if (appMaster &#x3D;&#x3D; null</span><br><span class="line">      || appMaster.getYarnApplicationState() &#x3D;&#x3D; YarnApplicationState.FAILED</span><br><span class="line">      || appMaster.getYarnApplicationState() &#x3D;&#x3D; YarnApplicationState.KILLED) &#123;</span><br><span class="line">    throw new IOException(&quot;Failed to run job : &quot; +</span><br><span class="line">        diagnostics);</span><br><span class="line">  &#125;</span><br><span class="line">  &#x2F;&#x2F; 获取 job 状态</span><br><span class="line">  &#x2F;&#x2F; 上面等待 job 结束实质也是调用这段代码</span><br><span class="line">  return clientCache.getClient(jobId).getJobStatus(jobId);</span><br><span class="line">&#125; catch (YarnException e) &#123;</span><br><span class="line">  throw new IOException(e);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>


<p>3.4: job 客户端</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">&#x2F;&#x2F; ResourceMgrDelegate.java</span><br><span class="line">public ApplicationId submitApplication(ApplicationSubmissionContext appContext)</span><br><span class="line">      throws YarnException, IOException &#123;</span><br><span class="line">return client.submitApplication(appContext);</span><br><span class="line">​</span><br><span class="line"></span><br><span class="line">&#x2F;&#x2F; YarnClientImpl.java</span><br><span class="line">public ApplicationId</span><br><span class="line">  submitApplication(ApplicationSubmissionContext appContext)</span><br><span class="line">      throws YarnException, IOException &#123;</span><br><span class="line">...</span><br><span class="line"></span><br><span class="line">&#x2F;&#x2F; job 包装成 request</span><br><span class="line">SubmitApplicationRequest request &#x3D;</span><br><span class="line">    Records.newRecord(SubmitApplicationRequest.class);</span><br><span class="line">request.setApplicationSubmissionContext(appContext);</span><br><span class="line"></span><br><span class="line">&#x2F;&#x2F; rmClient 是 RM 的代理类，这里才是真的提交 job 到 yarn </span><br><span class="line">&#x2F;&#x2F;TODO: YARN-1763:Handle RM failovers during the submitApplication call.</span><br><span class="line">rmClient.submitApplication(request);</span><br><span class="line"></span><br><span class="line">int pollCount &#x3D; 0;</span><br><span class="line">long startTime &#x3D; System.currentTimeMillis();</span><br><span class="line">EnumSet&lt;YarnApplicationState&gt; waitingStates &#x3D; </span><br><span class="line">                             EnumSet.of(YarnApplicationState.NEW,</span><br><span class="line">                             YarnApplicationState.NEW_SAVING,</span><br><span class="line">                             YarnApplicationState.SUBMITTED);</span><br><span class="line">EnumSet&lt;YarnApplicationState&gt; failToSubmitStates &#x3D; </span><br><span class="line">                              EnumSet.of(YarnApplicationState.FAILED,</span><br><span class="line">                              YarnApplicationState.KILLED);		</span><br><span class="line"></span><br><span class="line">&#x2F;&#x2F; 死等 job 提交成功或者job 异常才返回;200 ms 查询一次状态</span><br><span class="line">while (true) &#123;</span><br><span class="line">  try &#123;</span><br><span class="line">    ApplicationReport appReport &#x3D; getApplicationReport(applicationId);</span><br><span class="line">    YarnApplicationState state &#x3D; appReport.getYarnApplicationState();</span><br><span class="line">    if (!waitingStates.contains(state)) &#123;</span><br><span class="line">      if(failToSubmitStates.contains(state)) &#123;</span><br><span class="line">        throw new YarnException(&quot;Failed to submit &quot; + applicationId + </span><br><span class="line">            &quot; to YARN : &quot; + appReport.getDiagnostics());</span><br><span class="line">      &#125;</span><br><span class="line">      LOG.info(&quot;Submitted application &quot; + applicationId);</span><br><span class="line">      break;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">   ...</span><br><span class="line">	&#x2F;&#x2F; 200 ms</span><br><span class="line">    try &#123;</span><br><span class="line">      Thread.sleep(submitPollIntervalMillis);</span><br><span class="line">    &#125; catch (InterruptedException ie) &#123;</span><br><span class="line">      LOG.error(&quot;Interrupted while waiting for application &quot;</span><br><span class="line">          + applicationId</span><br><span class="line">          + &quot; to be successfully submitted.&quot;);</span><br><span class="line">    &#125;</span><br><span class="line">  &#125; catch (ApplicationNotFoundException ex) &#123;</span><br><span class="line">    &#x2F;&#x2F; FailOver or RM restart happens before RMStateStore saves</span><br><span class="line">    &#x2F;&#x2F; ApplicationState</span><br><span class="line">    LOG.info(&quot;Re-submit application &quot; + applicationId + &quot;with the &quot; +</span><br><span class="line">        &quot;same ApplicationSubmissionContext&quot;);</span><br><span class="line">	&#x2F;&#x2F; 异常，重新提交</span><br><span class="line">    rmClient.submitApplication(request);</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">return applicationId;</span><br></pre></td></tr></table></figure>


<p>3.5 实时获取 job 状态</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">&#x2F;&#x2F; job.java</span><br><span class="line">public boolean isComplete() throws IOException &#123;</span><br><span class="line">    ensureState(JobState.RUNNING);</span><br><span class="line">    updateStatus();</span><br><span class="line">    return status.isJobComplete();</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">&#x2F;** Some methods need to update status immediately. So, refresh * immediately * @throws IOException *&#x2F;</span><br><span class="line">synchronized void updateStatus() throws IOException &#123;</span><br><span class="line">    try &#123;</span><br><span class="line">      this.status &#x3D; ugi.doAs(new PrivilegedExceptionAction&lt;JobStatus&gt;() &#123;</span><br><span class="line">        @Override</span><br><span class="line">        public JobStatus run() throws IOException, InterruptedException &#123;</span><br><span class="line">		  &#x2F;&#x2F; 获取状态</span><br><span class="line">          return cluster.getClient().getJobStatus(getJobID());</span><br><span class="line">        &#125;</span><br><span class="line">      &#125;);</span><br><span class="line">    &#125;</span><br><span class="line">    catch (InterruptedException ie) &#123;</span><br><span class="line">      throw new IOException(ie);</span><br><span class="line">    &#125;</span><br><span class="line">    if (this.status &#x3D;&#x3D; null) &#123;</span><br><span class="line">      throw new IOException(&quot;Job status not available &quot;);</span><br><span class="line">    &#125;</span><br><span class="line">    this.statustime &#x3D; System.currentTimeMillis();</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>job 层流程分析到这里，下面以一副图来总结下<br><img src="/2020/02/22/hadoopjob/hj.png" alt="avatar"></p>
]]></content>
  </entry>
  <entry>
    <title>flinkhbase01</title>
    <url>/2020/02/21/flinkhbase01/</url>
    <content><![CDATA[<h1 id="flink-对接-HBase-开发"><a href="#flink-对接-HBase-开发" class="headerlink" title="flink 对接 HBase 开发"></a>flink 对接 HBase 开发</h1><h3 id="一-开发背景"><a href="#一-开发背景" class="headerlink" title="一. 开发背景"></a>一. 开发背景</h3><ol>
<li>简介<br>flink 官方并没有提供现成的API 进行HBase 的调用</li>
</ol>
<p>其实，在flink 中，有关于 Hadoop 的 InputFormat/OutputFormat 的描述<br><a href="https://ci.apache.org/projects/flink/flink-docs-release-1.9/dev/batch/hadoop_compatibility.html#using-hadoop-outputformats" target="_blank" rel="noopener">https://ci.apache.org/projects/flink/flink-docs-release-1.9/dev/batch/hadoop_compatibility.html#using-hadoop-outputformats</a></p>
<p>我们知道，HBase的数据是存放在HDFS之上的，所有沿着这个思路，从更底层去探索HBase的<br>Source/Sink 开发</p>
<ol start="2">
<li>前期准备</li>
</ol>
<p>开发准备：<br>开发需要的依赖包</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">&lt;dependency&gt;</span><br><span class="line">  &lt;groupId&gt;org.apache.flink&lt;&#x2F;groupId&gt;</span><br><span class="line">  &lt;artifactId&gt;flink-hbase_2.11&lt;&#x2F;artifactId&gt;</span><br><span class="line">  &lt;version&gt;$&#123;flink.version&#125;&lt;&#x2F;version&gt;</span><br><span class="line">&lt;&#x2F;dependency&gt;</span><br><span class="line"></span><br><span class="line">&lt;dependency&gt;</span><br><span class="line">  &lt;groupId&gt;org.apache.flink&lt;&#x2F;groupId&gt;</span><br><span class="line">  &lt;artifactId&gt;flink-hadoop-compatibility_2.11&lt;&#x2F;artifactId&gt;</span><br><span class="line">  &lt;version&gt;$&#123;flink.version&#125;&lt;&#x2F;version&gt;</span><br><span class="line">&lt;&#x2F;dependency&gt;</span><br></pre></td></tr></table></figure>



<h3 id="二-自定义Sink"><a href="#二-自定义Sink" class="headerlink" title="二. 自定义Sink"></a>二. 自定义Sink</h3><p>根据官网的内容，实际上是把数据转成HBase的格式，用 HadoopOutputFormat 写出去</p>
<p>即</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">new HadoopOutputFormat[Text, Mutation](new TableOutputFormat[Text](), Job.getInstance(configuration))</span><br><span class="line">Text 是CF的类型， Mutation 是value的类型，里面传入的参数就是 TableOutputFormat实例，和 Job实例</span><br></pre></td></tr></table></figure>


<p>这里面还需要注意，需要把造的数据转成 HBase支持的格式</p>
<p>这个代码如下：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">def convertToHBase(input: DataSet[(String, String, Int, String)]) &#x3D; &#123;</span><br><span class="line">  input.map(new RichMapFunction[(String, String, Int, String), (Text, Mutation)] &#123;</span><br><span class="line"></span><br><span class="line">    val cf &#x3D; &quot;o&quot;.getBytes()  &#x2F;&#x2F; cf的名称</span><br><span class="line"></span><br><span class="line">&#x2F;&#x2F; 重写 map 方法</span><br><span class="line">    override def map(value: (String, String, Int, String)): (Text, Mutation) &#x3D; &#123;</span><br><span class="line"></span><br><span class="line">      val id &#x3D; value._1</span><br><span class="line">      val name &#x3D; value._2</span><br><span class="line">      val age &#x3D; value._3</span><br><span class="line">      val city &#x3D; value._4</span><br><span class="line"></span><br><span class="line">      val text &#x3D; new Text(id)</span><br><span class="line">      val put &#x3D; new Put(id.getBytes())</span><br><span class="line"></span><br><span class="line">      if(StringUtils.isNotEmpty(name)) &#123;</span><br><span class="line">&#x2F;&#x2F;判断字段是否存在</span><br><span class="line">        put.addColumn(cf, Bytes.toBytes(&quot;name&quot;), Bytes.toBytes(name))</span><br><span class="line">      &#125;</span><br><span class="line"></span><br><span class="line">      if(StringUtils.isNotEmpty(age+&quot;&quot;)) &#123;</span><br><span class="line">        put.addColumn(cf, Bytes.toBytes(&quot;age&quot;), Bytes.toBytes(age.toString))</span><br><span class="line">      &#125;</span><br><span class="line"></span><br><span class="line">      if(StringUtils.isNotEmpty(city)) &#123;</span><br><span class="line">        put.addColumn(cf, Bytes.toBytes(&quot;city&quot;), Bytes.toBytes(city))</span><br><span class="line">      &#125;</span><br><span class="line"></span><br><span class="line">      (text, put)</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;)</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>


<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">def main(args: Array[String]): Unit &#x3D; &#123;</span><br><span class="line"></span><br><span class="line">  val environment &#x3D; ExecutionEnvironment.getExecutionEnvironment</span><br><span class="line"></span><br><span class="line">  val students &#x3D; ListBuffer[(String, String, Int, String)]()</span><br><span class="line"></span><br><span class="line">  for(i&lt;- 1 to 10) &#123;</span><br><span class="line">    students.append((i+ &quot;&quot;, &quot;Tedata&quot; +i, 20+i, &quot;SZ&quot; +i))</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  val input &#x3D; environment.fromCollection(students)</span><br><span class="line">  val result &#x3D; convertToHBase(input) </span><br><span class="line"></span><br><span class="line">    &#x2F;&#x2F; 获得HBase的configuration，以设置连接的信息，包括zk的服务器，端口，操作的表，mapreduce输出目录</span><br><span class="line">  val configuration &#x3D; HBaseConfiguration.create()</span><br><span class="line">  configuration.set(&quot;hbase.zookeeper.quorum&quot;,&quot;tedata01&quot;)</span><br><span class="line">  configuration.set(&quot;hbase.zookeeper.property.clientPort&quot;,&quot;2181&quot;)</span><br><span class="line">  configuration.set(TableOutputFormat.OUTPUT_TABLE, &quot;tedata_stu2&quot;)</span><br><span class="line">  configuration.set(&quot;mapreduce.output.fileoutputformat.outputdir&quot;,&quot;&#x2F;tmp&quot;)</span><br><span class="line"></span><br><span class="line">  val job &#x3D; Job.getInstance(configuration)</span><br><span class="line">  result.output(new HadoopOutputFormat[Text, Mutation](new TableOutputFormat[Text](), job))</span><br><span class="line"></span><br><span class="line">  environment.execute(this.getClass.getSimpleName)</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">input.map(new RichMapFunction[(String, String, Int, String), (Text, Mutation)] 这里是重写了map方法</span><br></pre></td></tr></table></figure>




<h3 id="三-自定义-Source"><a href="#三-自定义-Source" class="headerlink" title="三. 自定义 Source"></a>三. 自定义 Source</h3><p>首先，按照 TableInuptFormat开发</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">def main(args: Array[String]): Unit &#x3D; &#123;</span><br><span class="line"></span><br><span class="line">  val environment &#x3D; StreamExecutionEnvironment.getExecutionEnvironment</span><br><span class="line"></span><br><span class="line">  val cf &#x3D; &quot;o&quot;.getBytes()</span><br><span class="line"></span><br><span class="line">  val stream &#x3D; environment.createInput(new TableInputFormat[(String, String, Int, String)] &#123;</span><br><span class="line">    override def getScanner: Scan &#x3D; &#123;</span><br><span class="line"></span><br><span class="line">      val scan &#x3D; new Scan()</span><br><span class="line">      scan.addFamily(cf)</span><br><span class="line">      scan</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    override def getTableName: String &#x3D; &#123;</span><br><span class="line">      &quot;tedata_sku3&quot;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    override def mapResultToTuple(result: Result): (String, String, Int, String) &#x3D; &#123;</span><br><span class="line">      (Bytes.toString(result.getRow)</span><br><span class="line">        ,</span><br><span class="line">        Bytes.toString(result.getValue(cf, &quot;name&quot;.getBytes()))</span><br><span class="line">        ,</span><br><span class="line">        Bytes.toString(result.getValue(cf, &quot;age&quot;.getBytes())).toInt</span><br><span class="line">        ,</span><br><span class="line">        Bytes.toString(result.getValue(cf, &quot;city&quot;.getBytes()))</span><br><span class="line">      )</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">  )</span><br><span class="line">  stream.print()</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>


<p>是没有报错的，但是执行的时候，会报错：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">Error:(18, 46) type arguments [(String, String, Int, String)] do not conform to class TableInputFormat&#39;s type parameter bounds [T &lt;: org.apache.flink.api.java.tuple.Tuple]</span><br><span class="line">    val stream &#x3D; environment.createInput(new TableInputFormat[(String, String, Int, String)] &#123;</span><br></pre></td></tr></table></figure>

<p>显然，是报类型错误，<br>需要这样 构造 </p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">new TableInputFormat[Tuple4[String, String, Int, String]]</span><br></pre></td></tr></table></figure>


<p>写好后，执行，又报错：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">ERROR ZooKeeperWatcher: hconnection-0x40e60ece0x0, quorum&#x3D;localhost:2181, baseZNode&#x3D;&#x2F;hbase Received unexpected KeeperException, re-throwing exception [main]</span><br><span class="line">org.apache.zookeeper.KeeperException$ConnectionLossException: KeeperErrorCode &#x3D; ConnectionLoss for &#x2F;hbase&#x2F;hbaseid</span><br></pre></td></tr></table></figure>


<p>这个错误实际上是因为：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">override def configure(parameters: Configuration)&#x3D;&#123;</span><br><span class="line">  table &#x3D; createTable</span><br><span class="line">  if (table !&#x3D; null) &#123;</span><br><span class="line">    scan &#x3D; getScanner</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>


<p>完整代码为：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">def main(args: Array[String]): Unit &#x3D; &#123;</span><br><span class="line"></span><br><span class="line">  val environment &#x3D; ExecutionEnvironment.getExecutionEnvironment</span><br><span class="line"></span><br><span class="line">  val cf &#x3D; &quot;o&quot;.getBytes()</span><br><span class="line"></span><br><span class="line">  val stream &#x3D; environment.createInput(new TableInputFormat[Tuple4[String, String, Int, String]] &#123;</span><br><span class="line"></span><br><span class="line">    def createTable()&#x3D; &#123;</span><br><span class="line">      val configuration &#x3D; HBaseConfiguration.create()</span><br><span class="line">      configuration.set(&quot;hbase.zookeeper.quorum&quot;, &quot;tedata01&quot;)</span><br><span class="line">      configuration.set(&quot;hbase.zookeeper.property.clientPort&quot;, &quot;2181&quot;)</span><br><span class="line">      new HTable(configuration, getTableName)</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    override def configure(parameters: Configuration)&#x3D;&#123;</span><br><span class="line">      table &#x3D; createTable</span><br><span class="line">      if (table !&#x3D; null) &#123;</span><br><span class="line">        scan &#x3D; getScanner</span><br><span class="line">      &#125;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    override def getScanner: Scan &#x3D; &#123;</span><br><span class="line">      val scan &#x3D; new Scan()</span><br><span class="line">      scan.addFamily(cf)</span><br><span class="line">      scan</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    override def getTableName: String &#x3D; &#123;</span><br><span class="line">      &quot;tedata_stu3&quot;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    override def mapResultToTuple(result: Result): Tuple4[String, String, Int, String] &#x3D;&#123;</span><br><span class="line">      new Tuple4(Bytes.toString(result.getRow)</span><br><span class="line">        ,</span><br><span class="line">        Bytes.toString(result.getValue(cf, &quot;name&quot;.getBytes()))</span><br><span class="line">        ,</span><br><span class="line">        Bytes.toString(result.getValue(cf, &quot;age&quot;.getBytes())).toInt</span><br><span class="line">        ,</span><br><span class="line">        Bytes.toString(result.getValue(cf, &quot;city&quot;.getBytes())))</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">  )</span><br><span class="line">  stream.print()</span><br><span class="line"></span><br><span class="line">  environment.execute(this.getClass.getSimpleName)</span><br></pre></td></tr></table></figure>

<p>}</p>
]]></content>
  </entry>
  <entry>
    <title>Scala 种种</title>
    <url>/2019/10/10/scala%20%E7%A7%8D%E7%A7%8D/</url>
    <content><![CDATA[<p>scala 种种</p>
<p>学习目标：</p>
<p>scala 隐式转换</p>
<p>scala 伴生对象和伴生类</p>
<p>scala 模式匹配</p>
<p>scala 泛型 </p>
<p>scala 偏函数</p>
<p>scala 类边界</p>
<p>一.  scala 隐式转换</p>
<p>隐式实例一<br>def main(args: Array[String]): Unit = {<br>    implicit def shop2supermarket(shop: Shop) = new Supermarket(“foods”)<br>    val shop = new Shop()<br>    shop.restrant()<br>  }<br>}<br>class Shop()<br>class Supermarket(goods: String) {<br>  def restrant(): Unit = {<br>    println(s”supermarket has restrant…, so many $goods”)<br>  }<br>}<br>该隐式转换实现了 把 shop 类拥有了 supermarket 类的变量 和 方法</p>
<a id="more"></a>

<p>隐式转换的套路是：</p>
<p>implicit def 普通类2牛逼类(普通类变量：普通类) = new 牛逼类(参数)</p>
<p>隐式实例二<br> def main(args: Array[String]): Unit = {<br>    implicit def file2richfile(file: File): RichFile0211 = new RichFile0211(file)<br>    val file = new File(“D:\Program Files (x86)\IdeaProject\tedata-spark\data\emp.txt”)<br>    val data = file.read()<br>    println(data)<br>  }<br>}</p>
<p>class RichFile0211(file: File) {<br>  def read()= {<br>    Source.fromFile(file).mkString<br>  }<br>}</p>
<p>通过 RichFile 类，使得 java 的File 能够 通过 scala 的Source 类 读取数据，实现了read方法</p>
<p>隐式实例三<br>  def main(args: Array[String]): Unit = {</p>
<pre><code>implicit val speaking = &quot;Peach World&quot;

sayHello
sayHello(&quot;ok&quot;)</code></pre><p>  }</p>
<p>  def sayHello(implicit word:String): Unit = {<br>    println(s”speak loud this word: $word”)<br>  }</p>
<p>sayHello方法，即使没有传入变量，但由于 隐式转换的存在，使得  implicit val speaking = “Peach World” 去匹配 String 类型的参数，传入给函数</p>
<p>隐式实例四<br>  implicit val a = 3<br>  println(add(2)(3))<br>}</p>
<p>def add(x: Int)(y: Int)(implicit z: Int)={<br>   x + y +z<br>}</p>
<p>隐式转换一定要写在最后的变量里面，否则会报错</p>
<p>def add(x: Int)(implicit y: Int, z: Int)={<br>   x + y +z<br>}<br>可以这样写，表示 y，z都是隐式转换的变量<br>调用写成 add(10)</p>
<p>def add(x: Int)(y: Int)(implicit z: Int)={<br>   x + y +z<br>}<br>这样写，表示 只有z 是隐式变量<br>函数调用要 写成 add(9)(10)</p>
<p>二. 伴生类和伴生对象</p>
<p>  def main(args: Array[String]): Unit = {</p>
<p>// new 的是 ApplyTest 的class 类<br>    val applytest0211 = new ApplyTest0211()</p>
<p>// 不带 new 的 类 实例化的是 Object 类，调用 apply 方法时，<br>实际上也 进行了 new 的方法<br>    val test0211 = ApplyTest0211<br>    test0211.apply()<br>// object 可以直接调用方法<br>    ApplyTest0211.static()</p>
<p>  }</p>
<p>}</p>
<p>class ApplyTest0211() {<br>  def apply: Unit = {<br>    println(“this is class applytest 0211”)<br>  }<br>}</p>
<p>object ApplyTest0211 {</p>
<p>  def static(): Unit = {<br>    println(“this is static…”)<br>  }</p>
<p>  def apply(): Unit = {<br>    println(“this is object applytest 0211 “)<br>    new ApplyTest0211()<br>  }<br>}</p>
<p>三. 模式匹配</p>
<p>  def main(args: Array[String]): Unit = {</p>
<pre><code>val names = Array(&quot;bm&quot;, &quot;nn&quot;, &quot;xz&quot;)
val name = names(Random.nextInt(names.length))



name match {
  case &quot;bm&quot; =&gt; println(&quot;奥特曼&quot;)
  case &quot;nn&quot; =&gt; println(&quot;天线宝宝&quot;)
  case &quot;xz&quot; =&gt; println(&quot;冬冬腔&quot;)
}
matchType(List(1))

val classes = Array(Fruit, Light, Phone)
val className = classes(Random.nextInt(classes.length))</code></pre><p>//    val phone = Phone(111,222)</p>
<pre><code>className match {
  case Fruit =&gt; println(&quot;水果园&quot;)
  case Light =&gt; println(&quot;光照时间长&quot;)
  case Phone =&gt; Phone(111,222).content
}</code></pre><p>  }</p>
<p>  def matchType(obj: Any)= obj match  {<br>    case obj: Int =&gt; println(“整形”)<br>    case obj: String =&gt; println(“字符串”)<br>    case _ =&gt; println(“其他”)<br>  }<br>}</p>
<p>case class Fruit(location: String, weight: Double)<br>case class Light(level: Int, lightNum: Int)<br>case class Phone(weight: Double, screenSize: Double) {<br>  def content: Unit = {<br>    println(s”weight: $weight, screenSize: $screenSize”)</p>
<p>  }<br>}</p>
<p>四. 泛型<br>  def main(args: Array[String]): Unit = {<br>    val mm1 = new MM0211[Int, cupNum, Double](90, cupNum.D, 168.2)<br>    mm1.info()<br>  }<br>}</p>
<p>class MM0211[K, T, V](fv: K, cupNum: T, height: V) {<br>  def info(): Unit = {<br>    println(s”fv: $fv, cup: $cupNum, height: $height”)<br>  }<br>}</p>
<p>object cupNum extends Enumeration {<br>  type cupNum = Value<br>  val A,B,C,D,E = Value<br>}</p>
]]></content>
      <categories>
        <category>scala</category>
      </categories>
      <tags>
        <tag>scala</tag>
      </tags>
  </entry>
  <entry>
    <title>记一次Sqoop 抽数失败问题</title>
    <url>/2019/04/25/sqoop01/</url>
    <content><![CDATA[<h1 id="记一次Sqoop-抽数失败问题"><a href="#记一次Sqoop-抽数失败问题" class="headerlink" title="记一次Sqoop 抽数失败问题"></a>记一次Sqoop 抽数失败问题</h1><p>需求:</p>
<p>需要从  hive 导数到 mysql</p>
<p>hive 里面的 表是以 bdp_log  源里的 dashboard_access 表 的 storage_id 存储的, 字段名是 字段id</p>
<p>如下图:<br><img src="/2019/04/25/sqoop01/sqoop1.png" alt="avatar"></p>
<p>执行以下语句导数:</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">sqoop export --connect  &quot;jdbc:mysql:&#x2F;&#x2F;99.13.106.97&#x2F;bdp_service_zh?userUnicode&#x3D;true&amp;characterEncoding&#x3D;utf-8&quot; \</span><br><span class="line">--username root \</span><br><span class="line">--password 123456 \</span><br><span class="line">--table dashboard_access \</span><br><span class="line">--input-fields-terminated-by &#39;,&#39; \</span><br></pre></td></tr></table></figure>


<p>执行之后报错:</p>
<a id="more"></a>

<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">16&#x2F;12&#x2F;16 22:21:09 WARN spi.Registration: Not loading URI patterns in org.kitesdk.data.spi.hive.Loader</span><br><span class="line">16&#x2F;12&#x2F;16 22:21:09 ERROR sqoop.Sqoop: Got exception running Sqoop: org.kitesdk.data.DatasetNotFoundException: Descriptor location does not exist:     hdfs:&#x2F;&#x2F;nameservice1&#x2F;bdp&#x2F;STORAGE_ID&#x2F;.metadata</span><br><span class="line">org.kitesdk.data.DatasetNotFoundException: Descriptor location does not exist: hdfs:&#x2F;&#x2F;nameservice1&#x2F;bdp&#x2F;STORAGE_ID&#x2F;.metadata</span><br><span class="line">    at org.kitesdk.data.spi.filesystem.FileSystemMetadataProvider.checkExists(FileSystemMetadataProvider.java:562)</span><br><span class="line">    at org.kitesdk.data.spi.filesystem.FileSystemMetadataProvider.find(FileSystemMetadataProvider.java:605)</span><br><span class="line">    at org.kitesdk.data.spi.filesystem.FileSystemMetadataProvider.load(FileSystemMetadataProvider.java:114)</span><br><span class="line">    at org.kitesdk.data.spi.filesystem.FileSystemDatasetRepository.load(FileSystemDatasetRepository.java:197)</span><br><span class="line">    at org.kitesdk.data.Datasets.load(Datasets.java:108)</span><br><span class="line">    at org.kitesdk.data.Datasets.load(Datasets.java:140)</span><br><span class="line">    at org.kitesdk.data.mapreduce.DatasetKeyInputFormat$ConfigBuilder.readFrom(DatasetKeyInputFormat.java:92)</span><br><span class="line">    at org.kitesdk.data.mapreduce.DatasetKeyInputFormat$ConfigBuilder.readFrom(DatasetKeyInputFormat.java:139)</span><br><span class="line">    at org.apache.sqoop.mapreduce.JdbcUpdateExportJob.configureInputFormat(JdbcUpdateExportJob.java:192)</span><br><span class="line">    at org.apache.sqoop.mapreduce.ExportJobBase.runExport(ExportJobBase.java:432)</span><br><span class="line">    at org.apache.sqoop.manager.MySQLManager.upsertTable(MySQLManager.java:145)</span><br><span class="line">    at org.apache.sqoop.tool.ExportTool.exportTable(ExportTool.java:74)</span><br><span class="line">    at org.apache.sqoop.tool.ExportTool.run(ExportTool.java:100)</span><br><span class="line">    at org.apache.sqoop.Sqoop.run(Sqoop.java:143)</span><br><span class="line">    at org.apache.hadoop.util.ToolRunner.run(ToolRunner.java:70)</span><br><span class="line">    at org.apache.sqoop.Sqoop.runSqoop(Sqoop.java:179)</span><br><span class="line">    at org.apache.sqoop.Sqoop.runTool(Sqoop.java:218)</span><br><span class="line">    at org.apache.sqoop.Sqoop.runTool(Sqoop.java:227)</span><br><span class="line">    at org.apache.sqoop.Sqoop.main(Sqoop.java:236)</span><br></pre></td></tr></table></figure>

<p>问题原因：</p>
<p>这是 sqoop-1.4.6的 一个 bug, 由于 kite-sdk 1.0 版本造成的, 要升级到 1.1版本才可以, 咱们这个环节升级麻烦, 于是 </p>
<p>需要加入catalog参数才可以，</p>
<p>执行下面命令</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">sqoop export --connect  &quot;jdbc:mysql:&#x2F;&#x2F;xx&#x2F;bdp_service_zh?userUnicode&#x3D;true&amp;characterEncoding&#x3D;utf-8&quot; \</span><br><span class="line">--username root \</span><br><span class="line">--password 123456 \</span><br><span class="line">--table dashboard_access \</span><br><span class="line">--input-fields-terminated-by &#39;,&#39; \</span><br><span class="line"></span><br><span class="line">-hcatalog-table  &quot;STORAGE_ID&quot; \</span><br><span class="line">-hcatalog-database &quot;bdp&quot;</span><br></pre></td></tr></table></figure>


<p>执行 之后, 报错:</p>
<p>空指针异常, 对比了下, mysql里面 建的是 dashboard_access 表, 字段不是 字段id命名的, 改成字段id后, 再执行 ,ok 了</p>
]]></content>
      <tags>
        <tag>tedata</tag>
      </tags>
  </entry>
  <entry>
    <title>Flink Watermarks介绍</title>
    <url>/2018/08/25/Flinkwatermarks/</url>
    <content><![CDATA[<h1 id="Flink-Watermarks-介绍"><a href="#Flink-Watermarks-介绍" class="headerlink" title="Flink Watermarks 介绍"></a>Flink Watermarks 介绍</h1><h3 id="一、watermark的概念"><a href="#一、watermark的概念" class="headerlink" title="一、watermark的概念"></a>一、watermark的概念</h3><p>watermark是一种衡量Event Time进展的机制，它是数据本身的一个隐藏属性。通常基于Event Time的数据，自身都包含一个timestamp，例如1472693399700（2016-09-01 09:29:59.700），而这条数据的watermark时间则可能是：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">watermark(1472693399700) &#x3D; 1472693396700(2016-09-0109:29:56.700)</span><br></pre></td></tr></table></figure>


<p>这条数据的watermark时间是什么含义呢？即：timestamp小于1472693396700(2016-09-01 09:29:56.700)的数据，都已经到达了。</p>
<p><img src="/2018/08/25/Flinkwatermarks/w1.png" alt="avatar"></p>
<p>图中蓝色虚线和实线代表着watermark的时间。</p>
<h3 id="二、watermark有什么用？"><a href="#二、watermark有什么用？" class="headerlink" title="二、watermark有什么用？"></a>二、watermark有什么用？</h3><p>watermark是用于处理乱序事件的，而正确的处理乱序事件，通常用watermark机制结合window来实现。</p>
<p>我们知道，流处理从事件产生，到流经source，再到operator，中间是有一个过程和时间的。虽然大部分情况下，流到operator的数据都是按照事件产生的时间顺序来的，但是也不排除由于网络、背压等原因，导致乱序的产生（out-of-order或者说late element）。</p>
<p>但是对于late element，我们又不能无限期的等下去，必须要有个机制来保证一个特定的时间后，必须触发window去进行计算了。这个特别的机制，就是watermark。</p>
<h3 id="三、watermark如何分配？"><a href="#三、watermark如何分配？" class="headerlink" title="三、watermark如何分配？"></a>三、watermark如何分配？</h3><p>通常，在接收到source的数据后，应该立刻生成watermark；但是，也可以在source后，应用简单的map或者filter操作，然后再生成watermark。</p>
<p>生成watermark的方式主要有2大类：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">(1):WithPeriodicWatermarks</span><br><span class="line">(2):WithPunctuatedWatermarks</span><br></pre></td></tr></table></figure>

<p>第一种可以定义一个最大允许乱序的时间，这种情况应用较多。</p>
<p>我们主要来围绕Periodic Watermarks来说明，下面是生成periodic watermark的方法：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">&#x2F;**</span><br><span class="line"> * This generator generates watermarks assuming that elements come out of order to a certain degree only.</span><br><span class="line"> * The latest elements for a certain timestamp t will arrive at most n milliseconds after the earliest</span><br><span class="line"> * elements for timestamp t.</span><br><span class="line"> *&#x2F;</span><br><span class="line">class BoundedOutOfOrdernessGenerator extends AssignerWithPeriodicWatermarks[MyEvent] &#123;</span><br><span class="line"></span><br><span class="line">    val maxOutOfOrderness &#x3D; 3500L; &#x2F;&#x2F; 3.5 seconds</span><br><span class="line"></span><br><span class="line">    var currentMaxTimestamp: Long;</span><br><span class="line"></span><br><span class="line">    override def extractTimestamp(element: MyEvent, previousElementTimestamp: Long): Long &#x3D; &#123;</span><br><span class="line">        val timestamp &#x3D; element.getCreationTime()</span><br><span class="line">        currentMaxTimestamp &#x3D; max(timestamp, currentMaxTimestamp)</span><br><span class="line">        timestamp;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    override def getCurrentWatermark(): Watermark &#x3D; &#123;</span><br><span class="line">        &#x2F;&#x2F; return the watermark as current highest timestamp minus the out-of-orderness bound</span><br><span class="line">        new Watermark(currentMaxTimestamp - maxOutOfOrderness);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>


<p>程序中有一个extractTimestamp方法，就是根据数据本身的Event time来获取；还有一个getCurrentWatermar方法，<br>是用currentMaxTimestamp - maxOutOfOrderness来获取的。</p>
<p>这里的概念有点抽象，下面我们结合数据，在window中来实际演示下每个element的timestamp和watermark是多少，以及何时触发window。</p>
<h3 id="四、生成并跟踪watermark代码"><a href="#四、生成并跟踪watermark代码" class="headerlink" title="四、生成并跟踪watermark代码"></a>四、生成并跟踪watermark代码</h3><p>4.1、程序说明</p>
<p>我们从socket接收数据，然后经过map后立刻抽取timetamp并生成watermark，之后应用window来看看watermark和event time如何变化，才导致window被触发的。</p>
<p>4.2、代码如下</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">import java.text.SimpleDateFormat</span><br><span class="line"></span><br><span class="line">import org.apache.flink.streaming.api.scala._</span><br><span class="line">import org.apache.flink.streaming.api.TimeCharacteristic</span><br><span class="line">import org.apache.flink.streaming.api.functions.AssignerWithPeriodicWatermarks</span><br><span class="line">import org.apache.flink.streaming.api.scala.StreamExecutionEnvironment</span><br><span class="line">import org.apache.flink.streaming.api.scala.function.WindowFunction</span><br><span class="line">import org.apache.flink.streaming.api.watermark.Watermark</span><br><span class="line">import org.apache.flink.streaming.api.windowing.assigners.TumblingEventTimeWindows</span><br><span class="line">import org.apache.flink.streaming.api.windowing.time.Time</span><br><span class="line">import org.apache.flink.streaming.api.windowing.windows.TimeWindow</span><br><span class="line">import org.apache.flink.util.Collector</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">object WatermarkTest &#123;</span><br><span class="line"></span><br><span class="line">  def main(args: Array[String]): Unit &#x3D; &#123;</span><br><span class="line">    if (args.length !&#x3D; 2) &#123;</span><br><span class="line">      System.err.println(&quot;USAGE:\nSocketWatermarkTest &lt;hostname&gt; &lt;port&gt;&quot;)</span><br><span class="line">      return</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    val hostName &#x3D; args(0)</span><br><span class="line">    val port &#x3D; args(1).toInt</span><br><span class="line"></span><br><span class="line">    val env &#x3D; StreamExecutionEnvironment.getExecutionEnvironment</span><br><span class="line">    env.setStreamTimeCharacteristic(TimeCharacteristic.EventTime)</span><br><span class="line"></span><br><span class="line">    val input &#x3D; env.socketTextStream(hostName,port)</span><br><span class="line"></span><br><span class="line">    val inputMap &#x3D; input.map(f&#x3D;&gt; &#123;</span><br><span class="line">      val arr &#x3D; f.split(&quot;\\W+&quot;)</span><br><span class="line">      val code &#x3D; arr(0)</span><br><span class="line">      val time &#x3D; arr(1).toLong</span><br><span class="line">      (code,time)</span><br><span class="line">    &#125;)</span><br><span class="line"></span><br><span class="line">    val watermark &#x3D; inputMap.assignTimestampsAndWatermarks(new AssignerWithPeriodicWatermarks[(String,Long)] &#123;</span><br><span class="line"></span><br><span class="line">      var currentMaxTimestamp &#x3D; 0L</span><br><span class="line">      val maxOutOfOrderness &#x3D; 10000L&#x2F;&#x2F;最大允许的乱序时间是10s</span><br><span class="line"></span><br><span class="line">      var a : Watermark &#x3D; null</span><br><span class="line"></span><br><span class="line">      val format &#x3D; new SimpleDateFormat(&quot;yyyy-MM-dd HH:mm:ss.SSS&quot;)</span><br><span class="line"></span><br><span class="line">      override def getCurrentWatermark: Watermark &#x3D; &#123;</span><br><span class="line">        a &#x3D; new Watermark(currentMaxTimestamp - maxOutOfOrderness)</span><br><span class="line">        a</span><br><span class="line">      &#125;</span><br><span class="line"></span><br><span class="line">      override def extractTimestamp(t: (String,Long), l: Long): Long &#x3D; &#123;</span><br><span class="line">        val timestamp &#x3D; t._2</span><br><span class="line">        currentMaxTimestamp &#x3D; Math.max(timestamp, currentMaxTimestamp)</span><br><span class="line">        println(&quot;timestamp:&quot; + t._1 +&quot;,&quot;+ t._2 + &quot;|&quot; +format.format(t._2) +&quot;,&quot;+  currentMaxTimestamp + &quot;|&quot;+ format.format(currentMaxTimestamp) + &quot;,&quot;+ a.toString)</span><br><span class="line">        timestamp</span><br><span class="line">      &#125;</span><br><span class="line">    &#125;)</span><br><span class="line"></span><br><span class="line">    val window &#x3D; watermark</span><br><span class="line">    .keyBy(_._1)</span><br><span class="line">    .window(TumblingEventTimeWindows.of(Time.seconds(3)))</span><br><span class="line">    .apply(new WindowFunctionTest)</span><br><span class="line"></span><br><span class="line">    window.print()</span><br><span class="line"></span><br><span class="line">    env.execute()</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  class WindowFunctionTest extends WindowFunction[(String,Long),(String, Int,String,String,String,String),String,TimeWindow]&#123;</span><br><span class="line"></span><br><span class="line">    override def apply(key: String, window: TimeWindow, input: Iterable[(String, Long)], out: Collector[(String, Int,String,String,String,String)]): Unit &#x3D; &#123;</span><br><span class="line">      val list &#x3D; input.toList.sortBy(_._2)</span><br><span class="line">      val format &#x3D; new SimpleDateFormat(&quot;yyyy-MM-dd HH:mm:ss.SSS&quot;)</span><br><span class="line">      out.collect(key,input.size,format.format(list.head._2),format.format(list.last._2),format.format(window.getStart),format.format(window.getEnd))</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>



<p>4.3、程序详解</p>
<p>（1）接收socket数据</p>
<p>（2）将每行数据按照字符分隔，每行map成一个tuple类型（code，time）</p>
<p>（3）抽取timestamp生成watermark。并打印（code，time，格式化的time，currentMaxTimestamp，currentMaxTimestamp的格式化时间，watermark时间）。</p>
<p>（4）event time每隔3秒触发一次窗口，输出（code，窗口内元素个数，窗口内最早元素的时间，窗口内最晚元素的时间，窗口自身开始时间，窗口自身结束时间）</p>
<p>注意：new AssignerWithPeriodicWatermarks[(String,Long)中有抽取timestamp和生成watermark2个方法，在执行时，它是先抽取timestamp，后生成watermark，因此我们在这里print的watermark时间，其实是上一条的watermark时间，我们到数据输出时再解释。<br><img src="/2018/08/25/Flinkwatermarks/w2.png" alt="avatar"></p>
<p>生成的Job Graph</p>
<h3 id="五、通过数据跟踪watermark的时间"><a href="#五、通过数据跟踪watermark的时间" class="headerlink" title="五、通过数据跟踪watermark的时间"></a>五、通过数据跟踪watermark的时间</h3><p>我们重点看看watermark与timestamp的时间，并通过数据来看看window的触发时机。</p>
<p>首先，我们开启socket，输入第一条数据：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">000001,1461756862000</span><br></pre></td></tr></table></figure>

<p>输出的out文件如下：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">timestamp:000001,1461756862000|2016-04-27 19:34:22.000,1461756862000|2016-04-27 19:34:22.000,Watermark @ -10000</span><br></pre></td></tr></table></figure>

<p>这里，看下watermark的值，-10000，即0-10000得到的。这就说明程序先执行timestamp，后执行watermark。<br>所以，每条记录打印出的watermark，都应该是上一条的watermark。为了观察方便，我汇总了输出如下：<br><img src="/2018/08/25/Flinkwatermarks/w3.png" alt="avatar"></p>
<p>此时，wartermark的时间按照逻辑，已经落后于currentMaxTimestamp10秒了。我们继续输入：<br><img src="/2018/08/25/Flinkwatermarks/w4.png" alt="avatar"></p>
<p>此时，输出内容如下：<br><img src="/2018/08/25/Flinkwatermarks/w5.png" alt="avatar"></p>
<p>我们再次汇总，见下表：<br><img src="/2018/08/25/Flinkwatermarks/w6.png" alt="avatar"></p>
<p>我们继续输入，这时我们再次输入：<br><img src="/2018/08/25/Flinkwatermarks/w7.png" alt="avatar"></p>
<p>输出如下：<br><img src="/2018/08/25/Flinkwatermarks/w8.png" alt="avatar"></p>
<p>汇总如下：<br><img src="/2018/08/25/Flinkwatermarks/w9.png" alt="avatar"></p>
<p>到这里，window仍然没有被触发，此时watermark的时间已经等于了第一条数据的Event Time了。那么window到底什么时候被触发呢？我们再次输入：<br><img src="/2018/08/25/Flinkwatermarks/w10.png" alt="avatar"></p>
<p>输出：<br><img src="/2018/08/25/Flinkwatermarks/w11.png" alt="avatar"></p>
<p>汇总：<br><img src="/2018/08/25/Flinkwatermarks/w12.png" alt="avatar"></p>
<p>OK，window仍然没有触发，此时，我们的数据已经发到2016-04-27 19:34:33.000了，最早的数据已经过去了11秒了，还没有开始计算。那是不是要等到13（10+3）秒过去了，才开始触发window呢？答案是否定的。</p>
<p>我们再次增加1秒，输入：<br><img src="/2018/08/25/Flinkwatermarks/w13.png" alt="avatar"></p>
<p>输出：<br><img src="/2018/08/25/Flinkwatermarks/w14.png" alt="avatar"></p>
<p>汇总：<br><img src="/2018/08/25/Flinkwatermarks/w15.png" alt="avatar"></p>
<p>到这里，我们做一个说明：</p>
<p>window的触发机制，是先按照自然时间将window划分，如果window大小是3秒，那么1分钟内会把window划分为如下的形式:</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[00:00:00,00:00:03)[00:00:03,00:00:06)...[00:00:57,00:01:00)</span><br></pre></td></tr></table></figure>


<p>如果window大小是10秒，则window会被分为如下的形式：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[00:00:00,00:00:10)[00:00:10,00:00:20)...[00:00:50,00:01:00)</span><br></pre></td></tr></table></figure>

<p>window的设定无关数据本身，而是系统定义好了的。</p>
<p>输入的数据中，根据自身的Event Time，将数据划分到不同的window中，如果window中有数据，则当watermark时间&gt;=Event Time时，就符合了window触发的条件了，最终决定window触发，还是由数据本身的Event Time所属的window中的window_end_time决定。</p>
<p>上面的测试中，最后一条数据到达后，其水位线已经升至19:34:24秒，正好是最早的一条记录所在window的window_end_time，所以window就被触发了。</p>
<p>为了验证window的触发机制，我们继续输入数据：<br><img src="/2018/08/25/Flinkwatermarks/w16.png" alt="avatar"></p>
<p>输出：<br><img src="/2018/08/25/Flinkwatermarks/w17.png" alt="avatar"></p>
<p>汇总：<br><img src="/2018/08/25/Flinkwatermarks/w18.png" alt="avatar"></p>
<p>此时，watermark时间虽然已经达到了第二条数据的时间，但是由于其没有达到第二条数据所在window的结束时间，所以window并没有被触发。那么，第二条数据所在的window时间是：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[19:34:24,19:34:27)</span><br></pre></td></tr></table></figure>


<p>也就是说，我们必须输入一个19:34:27秒的数据，第二条数据所在的window才会被触发。我们继续输入：<br><img src="/2018/08/25/Flinkwatermarks/w19.png" alt="avatar"></p>
<p>输出：<br><img src="/2018/08/25/Flinkwatermarks/w20.png" alt="avatar"></p>
<p>汇总：<br><img src="/2018/08/25/Flinkwatermarks/w21.png" alt="avatar"></p>
<p>此时，我们已经看到，window的触发要符合以下几个条件：</p>
<p>1、watermark时间 &gt;= window_end_time2、在[window_start_time,window_end_time)中有数据存在<br>1<br>2<br>同时满足了以上2个条件，window才会触发。</p>
<p>而且，这里要强调一点，watermark是一个全局的值，不是某一个key下的值，所以即使不是同一个key的数据，其warmark也会增加，例如：</p>
<p>输入：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">000002,1461756879000</span><br></pre></td></tr></table></figure>

<p>输出：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">timestamp:000002,1461756879000|2016-04-27 19:34:39.000,1461756879000|2016-04-27 19:34:39.000,Watermark @ 1461756867000</span><br></pre></td></tr></table></figure>


<p>我们看到，currentMaxTimestamp也增加了。</p>
<h3 id="六、watermark-window处理乱序"><a href="#六、watermark-window处理乱序" class="headerlink" title="六、watermark+window处理乱序"></a>六、watermark+window处理乱序</h3><p>我们上面的测试，数据都是按照时间顺序递增的，现在，我们输入一些乱序的（late）数据，看看watermark结合window机制，是如何处理乱序的。</p>
<p>输入：<br><img src="/2018/08/25/Flinkwatermarks/w22.png" alt="avatar"></p>
<p>输出：<br><img src="/2018/08/25/Flinkwatermarks/w23.png" alt="avatar"></p>
<p>汇总：<br><img src="/2018/08/25/Flinkwatermarks/w24.png" alt="avatar"></p>
<p>可以看到，虽然我们输入了一个19:34:31的数据，但是currentMaxTimestamp和watermark都没变。此时，按照我们上面提到的公式：</p>
<p>1、watermark时间 &gt;= window_end_time2、在[window_start_time,window_end_time)中有数据存在</p>
<p>watermark时间（19:34:29） &lt; window_end_time（19:34:33），因此不能触发window。</p>
<p>那如果我们再次输入一条19:34:43的数据，此时watermark时间会升高到19:34:33，这时的window一定就会触发了，我们试一试：</p>
<p>输入：<br><img src="/2018/08/25/Flinkwatermarks/w25.png" alt="avatar"></p>
<p>输出：<br><img src="/2018/08/25/Flinkwatermarks/w26.png" alt="avatar"></p>
<p>这里，我么看到，窗口中有2个数据，19:34:31和19:34:32的，但是没有19:34:33的数据，原因是窗口是一个前闭后开的区间，19:34:33的数据是属于[19:34:33,19:34:36)的窗口的。</p>
<p>上边的结果，已经表明，对于out-of-order的数据，Flink可以通过watermark机制结合window的操作，来处理一定范围内的乱序数据。那么对于“迟到”太多的数据，Flink是怎么处理的呢？</p>
<h3 id="七、late-element的处理"><a href="#七、late-element的处理" class="headerlink" title="七、late element的处理"></a>七、late element的处理</h3><p>我们输入一个乱序很多的（其实只要Event Time &lt; watermark时间）数据来测试下：</p>
<p>输入：<br><img src="/2018/08/25/Flinkwatermarks/w27.png" alt="avatar"></p>
<p>输出：<br><img src="/2018/08/25/Flinkwatermarks/w28.png" alt="avatar"></p>
<p>我们看到，我们输入的数据是19:34:32的，而当前watermark时间已经来到了19:34:33，Event Time &lt; watermark时间，所以来一条就触发一个window。</p>
<h3 id="八、总结"><a href="#八、总结" class="headerlink" title="八、总结"></a>八、总结</h3><p>8.1、Flink如何处理乱序？</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">watermark+window机制</span><br></pre></td></tr></table></figure>

<p>window中可以对input进行按照Event Time排序，使得完全按照Event Time发生的顺序去处理数据，以达到处理乱序数据的目的。</p>
<p>8.2、Flink何时触发window？</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">1、Event Time &lt; watermark时间（对于late element太多的数据而言）</span><br></pre></td></tr></table></figure>

<p>或者</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">1. watermark时间 &gt;&#x3D; window_end_time（对于out-of-order以及正常的数据而言）</span><br><span class="line">2. 在[window_start_time,window_end_time)中有数据存在</span><br></pre></td></tr></table></figure>

<p>8.3、Flink应该如何设置最大乱序时间？</p>
<p>这个要结合自己的业务以及数据情况去设置。如果maxOutOfOrderness设置的太小，而自身数据发送时由于网络等原因导致乱序或者late太多，那么最终的结果就是会有很多单条的数据在window中被触发，数据的正确性影响太大。</p>
<p>最后，我们通过一张图来看看watermark、Event Time和window的关系：<br><img src="/2018/08/25/Flinkwatermarks/w29.png" alt="avatar"></p>
<p>文章转自：<a href="https://blog.csdn.net/lmalds/article/details/52704170" target="_blank" rel="noopener">https://blog.csdn.net/lmalds/article/details/52704170</a></p>
]]></content>
      <tags>
        <tag>Flink</tag>
      </tags>
  </entry>
  <entry>
    <title>YARN 的架构及提交流程</title>
    <url>/2018/07/18/YARN%E7%9A%84%E6%9E%B6%E6%9E%84%E5%8F%8A%E6%8F%90%E4%BA%A4%E6%B5%81%E7%A8%8B/</url>
    <content><![CDATA[<h1 id="YARN-的架构及提交流程"><a href="#YARN-的架构及提交流程" class="headerlink" title="YARN 的架构及提交流程"></a>YARN 的架构及提交流程</h1><h3 id="一-Yarn-架构"><a href="#一-Yarn-架构" class="headerlink" title="一. Yarn 架构"></a>一. Yarn 架构</h3><p>YARN 的全称是 Yet Another Resource Negotiator，YARN 整体上是 Master/Slave 结构，在整个框架中，ResourceManager 为 Master，NodeManager 为 Slave，如下图所示：<br><img src="/2018/07/18/YARN%E7%9A%84%E6%9E%B6%E6%9E%84%E5%8F%8A%E6%8F%90%E4%BA%A4%E6%B5%81%E7%A8%8B/yarn20.gif" alt="avatar"></p>
<p>YARN 基本架构</p>
<h4 id="ResourceManager（RM）"><a href="#ResourceManager（RM）" class="headerlink" title="ResourceManager（RM）"></a>ResourceManager（RM）</h4><p>RM 是一个全局的资源管理器，负责整个系统的资源管理和分配，它主要有两个组件构成：<br>调度器：Scheduler；<br>应用程序管理器：Applications Manager，ASM。<br>调度器<br>调度器根据容量、􏳴队列等限制条件（如某个队列分配一定的资源，最多执行一定数量的作业等），将系统中的资源分配给各个正在运行的应用程序。􏰣要注意的是，该调度器是一个纯调度器，它不再从事任何与应用程序有关的工作，比如不负责重新启动（因应用程序失败或者硬件故障导致的失败），这些均交由应用程序相关的 ApplicationMaster 完成。调度器仅根据各个应用程序的资源需求进行资源分配，而资源分配单位用一个抽象概念 资源容器(Resource Container，也即 Container)，Container 是一个动态资源分配单位，它将内存、CPU、磁盘、网络等资源封装在一起，从而限定每个任务使用的资源量。此外，该调度器是一个可插拔的组件，用户可根据自己的需求设计新的调度器，YARN 提供了多种直接可用的调度器，比如 Fair Scheduler 和 Capacity Schedule 等。<br>应用程序管理器<br>应用程序管理器负责管理整个系统中所有应用程序，包括应用程序提交、与调度器协商资源以 AM、监控 AM 运行状态并在失败时重新启动它等。</p>
<h4 id="NodeManager（NM）"><a href="#NodeManager（NM）" class="headerlink" title="NodeManager（NM）"></a>NodeManager（NM）</h4><p>NM 是每个节点上运行的资源和任务管理器，一方面，它会定时向 RM 汇报本节点上的资源使用情况和各个 Container 的运行状态；另一方面，它接收并处理来自 AM 的 Container 启动/停止等各种请求。</p>
<h4 id="ApplicationMaster（AM）"><a href="#ApplicationMaster（AM）" class="headerlink" title="ApplicationMaster（AM）"></a>ApplicationMaster（AM）</h4><p>提交的每个作业都会包含一个 AM，主要功能包括：<br>与 RM 协商以获取资源（用 container 表示）；<br>将得到的任务进一步分配给内部的任务；<br>与 NM 通信以启动/停止任务；<br>监控所有任务的运行状态，当任务有失败时，重新为任务申请资源并重启任务。<br>MapReduce 就是原生支持 ON YARN 的一种框架，可以在 YARN 上运行 MapReduce 作业。有很多分布式应用都开发了对应的应用程序框架，用于在 YARN 上运行任务，例如 Spark，Storm、Flink 等。<br>Container<br>Container 是 YARN 中的资源抽象，它封装了某个节点上的多维度资源，如内存、CPU、磁盘、网络等，当 AM 向 RM 申请资源时，RM 为 AM 返回的资源便是用 Container 表示的。 YARN 会为每个任务分配一个 Container 且该任务只能使用该 Container 中描述的资源。</p>
<h3 id="二-YARN-作业提交流程"><a href="#二-YARN-作业提交流程" class="headerlink" title="二. YARN 作业提交流程"></a>二. YARN 作业提交流程</h3><p>当用户向 YARN 中提交一个应用程序后，YARN 将分两个阶段运行该应用程序：第一个阶段是启动 ApplicationMaster；第二个阶段是由 ApplicationMaster 创建应用程序，为它申请资源，并监控它的整个运行过程，直到运行完成，如下图所示<br><img src="/2018/07/18/YARN%E7%9A%84%E6%9E%B6%E6%9E%84%E5%8F%8A%E6%8F%90%E4%BA%A4%E6%B5%81%E7%A8%8B/yarnflow.png" alt="avatar"></p>
<h5 id="YARN-工作流程"><a href="#YARN-工作流程" class="headerlink" title="YARN 工作流程"></a>YARN 工作流程</h5><p>上图所示的 YARN 工作流程分为以下几个步骤：</p>
<ul>
<li>用户向 YARN 提交应用程序，其中包括 ApplicationMaster 程序，启动 ApplicationMaster 命令、用户程序等;</li>
<li>RM 为该应用程序分配第一个 Container，并与对应的 NM 通信，要求它在这个 Container 中启动应用程序的 ApplicationMaster；</li>
<li>ApplicationMaster 首先向 RM 注册，这样用户可以直接通过 NM 查看应用程序的运行状态，然后它将为各个任务申请资源，并监控它的运行状态，直到运行结束，一直重复下面的 4-7 步；</li>
<li>ApplicationMaster 采用轮询的方式通过 RPC 协议向 RM 申请和领取资源；</li>
<li>一旦 ApplicationMaster 申请到资源后，便与对应的 NM 通信，要求它启动任务；</li>
<li>NM 为任务设置好运行环境（包括环境变量、jar 包等）后，将任务启动命令写到一个脚本中，并通过运行该脚本启动任务；</li>
<li>各个任务通过某个 RPC 协议向 ApplicationMaster 汇报自己的状态和进度，以让 ApplicationMaster 随时掌握各个任务的运行状态，从而可以在任务失败时重新启动任务；</li>
<li>应用程序运行完成后，ApplicationMaster 向 RM 注销并关闭自己（当然像 Storm、Flink 这种常驻应用程序列外）。</li>
</ul>
]]></content>
      <tags>
        <tag>tedata</tag>
      </tags>
  </entry>
  <entry>
    <title>kafka 第二课</title>
    <url>/2018/07/18/kafka02/</url>
    <content><![CDATA[<h1 id="kafka02"><a href="#kafka02" class="headerlink" title="kafka02"></a>kafka02</h1><ul>
<li><input checked disabled type="checkbox"> 主要内容：</li>
<li><input checked disabled type="checkbox"> <ol>
<li>核心概念</li>
</ol>
</li>
<li><input checked disabled type="checkbox"> <ol start="2">
<li>交付语义</li>
</ol>
</li>
<li><input checked disabled type="checkbox"> <ol start="3">
<li>分区选择</li>
</ol>
</li>
<li><input checked disabled type="checkbox"> <ol start="4">
<li>全局有序</li>
</ol>
</li>
<li><input checked disabled type="checkbox"> <ol start="5">
<li>调优</li>
</ol>
</li>
<li><input checked disabled type="checkbox"> <ol start="6">
<li>监控</li>
</ol>
</li>
<li><input checked disabled type="checkbox"> <ol start="7">
<li>故障案例</li>
</ol>
</li>
</ul>
<h3 id="一-核心概念"><a href="#一-核心概念" class="headerlink" title="一. 核心概念"></a>一. 核心概念</h3><p>1.1 Consumers 消费者</p>
<p>容错性的消费机制；比如下图的 C1, C2 在一个消费组里，C1出故障，C2继续消费<br><img src="/2018/07/18/kafka02/customergroup.png" alt="avatar"></p>
<p>一个组内，共享一个公共的消费group id ；<br>组内的所有消费者协调在一起 去消费指定的topic的所有分区；<br>每个分区只能由一个消费组的一个消费者来消费，绝不会出现一个分区<br>被一个消费组的多个消费者进行重复消费；</p>
<p>1.2 Segment 分段<br>一个partition被切割成多个相同大小的segment<br>命名规则: partition的全局的第一个segment必然是从0开始，后续的segment的名称为上一个<br>segment文件的最后一个消息的offset值来标识。<br>参数 log.segment.bytes=102400</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">-rw-rw-r-- 1 hadoop hadoop  184 Oct 18 14:24 00000000000000000000.index</span><br><span class="line">-rw-rw-r-- 1 hadoop hadoop 100K Oct 18 14:24 00000000000000000000.log  offset&#x3D;0</span><br><span class="line">-rw-rw-r-- 1 hadoop hadoop  300 Oct 18 14:24 00000000000000000000.timeindex</span><br><span class="line"></span><br><span class="line">-rw-rw-r-- 1 hadoop hadoop  10M Oct 18 14:25 00000000000000001997.index</span><br><span class="line">-rw-rw-r-- 1 hadoop hadoop  16K Oct 18 14:25 00000000000000001997.log offset&#x3D;1997+1&#x3D;1998</span><br><span class="line">-rw-rw-r-- 1 hadoop hadoop   10 Oct 18 14:24 00000000000000001997.snapshot</span><br><span class="line"></span><br><span class="line">-rw-rw-r-- 1 hadoop hadoop  10M Oct 18 14:25 00000000000000003330.index</span><br><span class="line">-rw-rw-r-- 1 hadoop hadoop  16K Oct 18 14:25 00000000000000003330.log offset&#x3D;3330+1&#x3D;3331</span><br><span class="line">-rw-rw-r-- 1 hadoop hadoop   10 Oct 18 14:24 00000000000000003330.snapshot</span><br></pre></td></tr></table></figure>


<p>1.3 看图说话</p>
<p>index文件:</p>
<ul>
<li>相对offset,分区的每个segment的log的唯一</li>
<li>物理地址，消息在log文件的物理地址  byte</li>
<li>稀疏表维护的，并不是每一条的消息的相对offset和物理地址都维护</li>
</ul>
<p>log文件:</p>
<ul>
<li>存储message</li>
</ul>
<p>offset偏移量:</p>
<ul>
<li>绝对offset 是从0开始，分区唯一的<br>  相对offset     1      分区的每个segment的log的唯一</li>
</ul>
<p>1.4 如何查找offset为2002的消息？（*****）</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">a.二分查找到&lt;&#x3D;2002的最大的segment段的文件 1997；</span><br><span class="line">b.2002 - 文件名称的offset 1997 &#x3D;5,相对offset；</span><br><span class="line">c.二分查找index文件的&lt;&#x3D;5的最大相对offset为3，对应的物理偏移量为90；</span><br><span class="line">d.在log文件，从90位置按顺序查找，直到找到绝对offset为2002的消息为m2002。</span><br></pre></td></tr></table></figure>


<h3 id="二-交付语义"><a href="#二-交付语义" class="headerlink" title="二. 交付语义"></a>二. 交付语义</h3><p><a href="http://kafka.apache.org/documentation/#semantics" target="_blank" rel="noopener">http://kafka.apache.org/documentation/#semantics</a></p>
<ul>
<li>At most once—Messages may be lost but are never redelivered. 0 1</li>
<li>At least once—Messages are never lost but may be redelivered. &gt;=1</li>
<li>Exactly once—this is what people actually want, each message is delivered once and only once. =1</li>
</ul>
<p>0.11 Exactly once 不是是指消费者 正确的是指的 producer端的保障</p>
<p>消费段的消费语义？<br>kafka+SS<br><a href="http://spark.apache.org/docs/latest/streaming-kafka-0-10-integration.html#storing-offsets" target="_blank" rel="noopener">http://spark.apache.org/docs/latest/streaming-kafka-0-10-integration.html#storing-offsets</a></p>
<p>checkpoints 缺点：<br>小文件<br>代码不能变更 </p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">Kafka itself 官方&amp;J哥            至少一次消费语义</span><br><span class="line">stream.foreachRDD &#123; rdd &#x3D;&gt;</span><br><span class="line">  	val offsetRanges &#x3D; rdd.asInstanceOf[HasOffsetRanges].offsetRanges</span><br><span class="line"></span><br><span class="line">  	&#x2F;&#x2F; some time later, after outputs have completed</span><br><span class="line">  	stream.asInstanceOf[CanCommitOffsets].commitAsync(offsetRanges)</span><br><span class="line"></span><br><span class="line">&#125;</span><br><span class="line">缺点：As with HasOffsetRanges, the cast to CanCommitOffsets will only succeed if called on the result of createDirectStream, not after transformations. The commitAsync call is threadsafe, but must occur after outputs if you want meaningful semantics.</span><br></pre></td></tr></table></figure>


<p>Your own data store 业务真正需要 精准一次消费语义</p>
<p>心得:</p>
<ul>
<li>a.交付语义的前半段无需关心 ，关键在于后半段的消费者的offset如何存储</li>
<li>b.我司<br>MySQL–》maxwell–》kafka–&gt;SS+Phoenix–》HBASE<br>至少一次消费语义+断批还原+巧妙的使用upsert语法  幂等性<br>c.假如非要选择外部存储，精准一次消费语义，很难做到代码级别的事务性<br>offset可能维护了，但是数据没有写进去<br>要不一起成功 要不一起失败<br>解决方案：数据+offset–》新的数据，一次性输出 </li>
</ul>
<h3 id="三-分区选择"><a href="#三-分区选择" class="headerlink" title="三. 分区选择"></a>三. 分区选择</h3><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">https:&#x2F;&#x2F;github.com&#x2F;apache&#x2F;kafka&#x2F;blob&#x2F;2.2.1&#x2F;clients&#x2F;src&#x2F;main&#x2F;java&#x2F;org&#x2F;apache&#x2F;kafka&#x2F;clients&#x2F;producer&#x2F;internals&#x2F;DefaultPartitioner.java</span><br><span class="line"></span><br><span class="line">&#x2F;**</span><br><span class="line">     * Compute the partition for the given record. 消息发送到哪个partition</span><br><span class="line">     *</span><br><span class="line">     * @param topic The topic name</span><br><span class="line">     * @param key The key to partition on (or null if no key)</span><br><span class="line">     * @param keyBytes serialized key to partition on (or null if no key)</span><br><span class="line">     * @param value The value to partition on or null</span><br><span class="line">     * @param valueBytes serialized value to partition on or null</span><br><span class="line">     * @param cluster The current cluster metadata</span><br><span class="line">     *&#x2F;</span><br><span class="line">    public int partition(String topic, Object key, byte[] keyBytes, Object value, byte[] valueBytes, Cluster cluster) &#123;</span><br><span class="line">        List&lt;PartitionInfo&gt; partitions &#x3D; cluster.partitionsForTopic(topic);</span><br><span class="line">        int numPartitions &#x3D; partitions.size();</span><br><span class="line">&#x2F;&#x2F; key不传值就走这个逻辑 </span><br><span class="line">        if (keyBytes &#x3D;&#x3D; null) &#123;</span><br><span class="line">            int nextValue &#x3D; nextValue(topic);  &#x2F;&#x2F; 自增长的方法</span><br><span class="line">            List&lt;PartitionInfo&gt; availablePartitions &#x3D; cluster.availablePartitionsForTopic(topic);</span><br><span class="line">            if (availablePartitions.size() &gt; 0) &#123;</span><br><span class="line">                int part &#x3D; Utils.toPositive(nextValue) % availablePartitions.size();</span><br><span class="line">                return availablePartitions.get(part).partition();</span><br><span class="line">            &#125; else &#123;</span><br><span class="line">                &#x2F;&#x2F; no partitions are available, give a non-available partition</span><br><span class="line">                return Utils.toPositive(nextValue) % numPartitions;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125; else &#123;  &#x2F;&#x2F; key传值就走 该逻辑</span><br><span class="line">            &#x2F;&#x2F; hash the keyBytes to choose a partition</span><br><span class="line">            return Utils.toPositive(Utils.murmur2(keyBytes)) % numPartitions;</span><br><span class="line">				100  % 3&#x3D;33。。。1   p1					</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br></pre></td></tr></table></figure>


<p> 生产上produer.send消息到kafka，指定key，根据key hash求出一个固定的分区</p>
<h3 id="四-全局有序"><a href="#四-全局有序" class="headerlink" title="四. 全局有序"></a>四. 全局有序</h3><p> topic  ruozedata  3个分区</p>
<p> insert u1 u2 u3 u4 delete 1–&gt;0</p>
<ul>
<li><p>p0：u1,u4</p>
</li>
<li><p>p1: i,u2</p>
</li>
<li><p>p2: u3,d</p>
<p>u3,d,i,u2,u1,u4  不光光是错误的 且1–》1</p>
<p>只设置一个分区</p>
<p>后端消费者做，分组时间排序 性能差</p>
<p>特征数据@若泽数据  特征数据 </p>
</li>
</ul>
<p>是不是指比如对于一个库/表的操作，指定一个特别的key<br>这个key hash之后只会发到特定的 partition，在这个partition内部是有序的</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">producer.send(new ProducerRecord&lt;&gt;(topic,messageNo,messageStr)).</span><br><span class="line"></span><br><span class="line">key&#x3D;ruozedata.emp.100</span><br><span class="line"></span><br><span class="line">hash(&quot;ruozedata.emp.100&quot;) 69 % 3&#x3D;23...0  p0</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"> p0：insert u1 u2 u3 u4 delete    </span><br><span class="line"> p1: </span><br><span class="line"> p2:</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    retries: 100</span><br><span class="line">    max.in.flight.requests.per.connection &#x3D; 1</span><br></pre></td></tr></table></figure>


<h3 id="五-调优"><a href="#五-调优" class="headerlink" title="五. 调优"></a>五. 调优</h3><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">Producer: </span><br><span class="line">    acks: all</span><br><span class="line">    buffer.memory: 536870912</span><br><span class="line">    compression.type :snappy</span><br><span class="line">    retries: 100</span><br><span class="line">    max.in.flight.requests.per.connection &#x3D; 1  &#x2F;&#x2F; 每次请求只发送一个消息</span><br><span class="line"></span><br><span class="line">    batch.size: 10240字节 不是条数 </span><br><span class="line">    max.request.size &#x3D; 2097152</span><br><span class="line">    request.timeout.ms &#x3D; 360000    大于 replica.lag.time.max.ms </span><br><span class="line">    metadata.fetch.timeout.ms&#x3D; 360000</span><br><span class="line">    timeout.ms &#x3D; 360000</span><br><span class="line"></span><br><span class="line">    linger.ms 5s (生产不用)</span><br><span class="line">    max.block.ms 1800000  &#x2F;&#x2F; 阻塞时间，要调大</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">Broker: CDH</span><br><span class="line">    message.max.bytes 2560KB  1条消息的大小</span><br><span class="line">    zookeeper.session.timeout.ms 180000</span><br><span class="line">    replica.fetch.max.bytes 5M   大于message.max.bytes</span><br><span class="line">    num.replica.fetchers 6</span><br><span class="line">    replica.lag.max.messages 6000</span><br><span class="line">    replica.lag.time.max.ms 15000</span><br><span class="line"></span><br><span class="line">    log.flush.interval.messages 10000</span><br><span class="line">    log.flush.interval.ms 5s</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">Consumer:</span><br><span class="line">https:&#x2F;&#x2F;issues.apache.org&#x2F;jira&#x2F;browse&#x2F;SPARK-22968</span><br><span class="line">    , &quot;max.partition.fetch.bytes&quot; -&gt; (5242880: java.lang.Integer) &#x2F;&#x2F;default: 1048576</span><br><span class="line">    , &quot;request.timeout.ms&quot; -&gt; (90000: java.lang.Integer) &#x2F;&#x2F;default: 60000</span><br><span class="line">    , &quot;session.timeout.ms&quot; -&gt; (60000: java.lang.Integer) &#x2F;&#x2F;default: 30000</span><br><span class="line">    , &quot;heartbeat.interval.ms&quot; -&gt; (5000: java.lang.Integer)</span><br><span class="line">    , &quot;receive.buffer.bytes&quot; -&gt; (10485760: java.lang.Integer)</span><br></pre></td></tr></table></figure>



<p>注意:</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">Minor changes required for Kafka 0.10 and the new consumer compared to laughing_man&#39;s answer:</span><br><span class="line"></span><br><span class="line">Broker:   No changes, you still need to increase properties message.max.bytes </span><br><span class="line">          and replica.fetch.max.bytes. message.max.bytes has to be equal or smaller(*) than </span><br><span class="line">      replica.fetch.max.bytes.</span><br><span class="line">Producer: Increase max.request.size to send the larger message.</span><br><span class="line">Consumer: Increase max.partition.fetch.bytes to receive larger messages.</span><br><span class="line">(*) Read the comments to learn more about message.max.bytes&lt;&#x3D;replica.fetch.max.bytes</span><br></pre></td></tr></table></figure>



<h3 id="六-监控-CDH"><a href="#六-监控-CDH" class="headerlink" title="六. 监控 CDH"></a>六. 监控 CDH</h3><ol>
<li>读写趋势一致 </li>
<li>且在同一个时间点吻合，数据条数吻合，但是数据量为什么不吻合</li>
<li>写 123 3字节</li>
</ol>
<ul>
<li>fetched data： 加了额外信息的数据<br>  比如  读topic p offset time  123 比如 20字节</li>
<li>received data： 接受的原始数据</li>
</ul>
<p>完全吻合说明： 消费及时，没有延时，kafka无压力</p>
<p><img src="/2018/07/18/kafka02/jk2.png" alt="avatar"></p>
<p>fetched data数据比 receive data 大 receive data？<br>也就是说 fetched data 是有读topic p offset time 这些额外信息，而receive 没有？</p>
<p>kafka 生产者 消息个数监控</p>
<h3 id="七-故障案例二"><a href="#七-故障案例二" class="headerlink" title="七. 故障案例二"></a>七. 故障案例二</h3><p>7.1 磁盘暴了<br>遇见错误不可怕 冷静分析<br>预警真的很重要<br>EOF 错误标识，文件不完整，一般是文件损坏</p>
<p>7.2 断电<br>kafka启动 都是绿色状态，其实不然，生产者 消费者无法work，抛异常</p>
<ul>
<li>a.服务down </li>
<li>b.删除kafka 重新安装</li>
<li>c.重新刷数据  5点  3点开始重新刷数据。</li>
</ul>
]]></content>
      <tags>
        <tag>tedata</tag>
      </tags>
  </entry>
  <entry>
    <title>kafka 第一课</title>
    <url>/2018/07/09/kafka01/</url>
    <content><![CDATA[<h1 id="kafka-01"><a href="#kafka-01" class="headerlink" title="kafka 01"></a>kafka 01</h1><p>学习目标：</p>
<ol>
<li>kafka 架构</li>
<li>kafka 存储结构</li>
<li>kafka  LEO , HW</li>
<li>kafka ISR</li>
<li>kafka 如何保证数据不丢失</li>
<li>kafka 选举机制</li>
<li>kafka 消费语义</li>
</ol>
<h3 id="一-kafka-架构"><a href="#一-kafka-架构" class="headerlink" title="一. kafka 架构"></a>一. kafka 架构</h3><p><img src="/2018/07/09/kafka01/kafkajg.png" alt="avatar"></p>
<p>kafka 架构有四个角色：</p>
<ul>
<li>producer：生产数据</li>
<li>consumer：消费数据</li>
<li>kafka broker：中间件，用于接收producer数据和给 consumer传递数据</li>
<li>zookeeper：用于 kafka leader的选举</li>
</ul>
<p>二. kafka 存储结构<br>kafka存储设计多个概念：</p>
<ul>
<li>topic</li>
<li>partition</li>
<li>segment</li>
<li>replication</li>
</ul>
<h5 id="topics"><a href="#topics" class="headerlink" title="topics"></a>topics</h5><p>是一类数据的集合，比如把订单数据作为一个topic，产品数据作为另外一个topic，两个topic之间互补干扰</p>
<h5 id="partition"><a href="#partition" class="headerlink" title="partition"></a>partition</h5><p>partition 是 topic的分区，有了partition，就可以把一个topic的数据分散到多台机器上，避免了因单台机器IO造成的数据吞吐量瓶颈</p>
<h5 id="segment"><a href="#segment" class="headerlink" title="segment"></a>segment</h5><p>为什么有了partition，还需要segment呢？因为partition如果只是一个单一文件，那么会造成一个问题：即partition这个文件会随着数据增大而变得越来越大，成为一个巨型文件，对于清理数据来说是很不方便的，所以把partition再按照固定大小分割成 segment文件，该文件形如 </p>
<p>000000000000xxxx.log</p>
<p>000000000000xxxx.index</p>
<p>即包含索引文件和数据文件，每个log文件的数字就是上一个文件的最后一个LEO大小</p>
<h5 id="如何根据segment查找消息？"><a href="#如何根据segment查找消息？" class="headerlink" title="如何根据segment查找消息？"></a>如何根据segment查找消息？</h5><p>查找消息为 2002的位置<br>流程</p>
<ul>
<li>首先通过绝对offset，找到&lt;=2002 最大的索引，比如 0000001997.index</li>
<li>在 0000001997.index内部，查找 2002-1997=5 的消息，通过二分查找，找到 &lt;=5 最大的索引，比如3</li>
<li>通过 000000.1997.index 对应到 0000001997.log文件里面去顺序查找3之后的信息，从而找到5对应的信息，就找到了 2002的信息</li>
</ul>
<h3 id="三-kafka-LEO-HW"><a href="#三-kafka-LEO-HW" class="headerlink" title="三. kafka LEO, HW"></a>三. kafka LEO, HW</h3><p>LEO: Log End Offset 即日志最后的偏移量的位置</p>
<p>HW: High WaterMarker，高水位线，实际上是消费者能够看到的kafka的最后的偏移量</p>
<h3 id="四-kafka-ISR"><a href="#四-kafka-ISR" class="headerlink" title="四. kafka ISR"></a>四. kafka ISR</h3><p>ISR: In-Sync Replicas 指的是副本同步队列</p>
<p>它的组成是 leader + 满足一定条件的followers<br>满足的条件是在规定的时间内，同步leader的数据，这个规定的<br>时间是由 kafka.lag.time.max.ms 指定的，如果超过这个时间<br>followers没有同步leader的数据，leader会把followers剔除</p>
<p>由此我们可以知道，kafka的同步策略既不是要所有节点都commit才算数据提交，也不是只要发给leader就算数据提交，它是数据可靠性和数据吞吐量的折衷，只要满足 ISR中的数据提交就算一次成功写入</p>
<h3 id="五-如何保证数据不丢失"><a href="#五-如何保证数据不丢失" class="headerlink" title="五. 如何保证数据不丢失"></a>五. 如何保证数据不丢失</h3><p>是不是 有 了 ISR就会保证数据不丢失呢？<br>当然不是，因为 ISR 中有可能出现只有leader一个节点存在的情况，<br>这个时候如果 leader挂了，就会造成数据丢失<br>所以除了 </p>
<p>==request.required.acks = -1== ，</p>
<p>还需要保证</p>
<p>==min.isr.replicas &gt;= 2== </p>
<p>即ISR 中的节点个数至少要为2个，这样数据要么提交成功，要么抛异常，不会出现数据丢失</p>
<h3 id="六-kafka的选举机制"><a href="#六-kafka的选举机制" class="headerlink" title="六. kafka的选举机制"></a>六. kafka的选举机制</h3><p>kafka 的选举机制不同于 zookeeper的 “少数服从多数” 的机制<br>即对于 2n + 1 个节点，最多容忍n个节点挂，至少保证n+1 个节点才能<br>选举出leader<br>kafka 不是这样的，它n+1 个节点，可以容忍n个节点的挂</p>
<p>kafka的选举参数 unclean.leader.election.enable<br>当为false 时，表示必须是 ISR 里的节点才能被选为leader<br>当为true时，表示只要时AR里的节点就可以被选为leader<br>这就需要在数据可靠性和可用性之间进行平衡，<br>如果设为 false，那么数据是可靠的，但是如果ISR里的服务全部宕了<br>长时间不恢复，那么将忍受长时间的对外服务中断<br>如果设为true，恢复的时间将比上面要快，但有可能造成数据丢失</p>
]]></content>
      <tags>
        <tag>tedata</tag>
      </tags>
  </entry>
  <entry>
    <title>Spark 优化之加速启动</title>
    <url>/2018/03/09/Spark%20%E4%BC%98%E5%8C%96%E4%B9%8B%E5%8A%A0%E9%80%9F%E5%90%AF%E5%8A%A8/</url>
    <content><![CDATA[<h1 id="Spark-优化之加速启动"><a href="#Spark-优化之加速启动" class="headerlink" title="Spark 优化之加速启动"></a>Spark 优化之加速启动</h1><p>一. 问题背景</p>
<p>Spark on YARN 每次启动时会将本地的 spark jar 和 conf 上传到 HDFS，这样会消耗很长的时间</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[hadoop@danner000 jars]$  spark-submit --class org.apache.spark.examples.SparkPi --master yarn --deploy-mode cluster spark-examples_2.11-2.4.4.jar 3</span><br><span class="line">...</span><br><span class="line">&#96;19&#x2F;10&#x2F;18 13:58:26 WARN yarn.Client: Neither spark.yarn.jars nor spark.yarn.archive is set, falling back to uploading libraries under SPARK_HOME.&#96;</span><br><span class="line">&#96;19&#x2F;10&#x2F;18 13:58:30 INFO yarn.Client: Uploading resource file:&#x2F;tmp&#x2F;spark-294ab9b7-97ff-4ffa-8e4f-ae44a89dd5da&#x2F;__spark_libs__1410305138065236635.zip -&gt; hdfs:&#x2F;&#x2F;192.168.22.147:9000&#x2F;user&#x2F;hadoop&#x2F;.sparkStaging&#x2F;application_1571146456067_0024&#x2F;__spark_libs__1410305138065236635.zip&#96;</span><br><span class="line">19&#x2F;10&#x2F;18 13:58:44 INFO yarn.Client: Uploading resource file:&#x2F;home&#x2F;hadoop&#x2F;app&#x2F;spark-2.4.4-bin-2.6.0-cdh5.15.1&#x2F;examples&#x2F;jars&#x2F;spark-examples_2.11-2.4.4.jar -&gt; hdfs:&#x2F;&#x2F;192.168.22.147:9000&#x2F;user&#x2F;hadoop&#x2F;.sparkStaging&#x2F;application_1571146456067_0024&#x2F;spark-examples_2.11-2.4.4.jar</span><br><span class="line">19&#x2F;10&#x2F;18 13:58:45 INFO yarn.Client: Uploading resource file:&#x2F;tmp&#x2F;spark-294ab9b7-97ff-4ffa-8e4f-ae44a89dd5da&#x2F;__spark_conf__5888474803491307773.zip -&gt; hdfs:&#x2F;&#x2F;192.168.22.147:9000&#x2F;user&#x2F;hadoop&#x2F;.sparkStaging&#x2F;application_1571146456067_0024&#x2F;__spark_conf__.zip</span><br><span class="line">...</span><br></pre></td></tr></table></figure>

<p>查看上面日志，是由于没有设置 ==spark.yarn.archive== 或 ==spark.yarn.jars==，所以每次启动的时候都会上传libs</p>
<h3 id="二-优化过程"><a href="#二-优化过程" class="headerlink" title="二. 优化过程"></a>二. 优化过程</h3><p>既然知道是哪个属性的原因，那我们就从源码里看看如何设置</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">&#x2F;&#x2F; org.apache.spark.deploy.yarn.Client private def createContainerLaunchContext &#123;</span><br><span class="line">    ... </span><br><span class="line">    val appStagingDirPath &#x3D; new Path(appStagingBaseDir, getAppStagingDir(appId))</span><br><span class="line">    ...</span><br><span class="line">    val localResources &#x3D; prepareLocalResources(appStagingDirPath, pySparkArchives)</span><br><span class="line">    ...</span><br><span class="line">&#125;</span><br><span class="line">&#x2F;&#x2F; org.apache.spark.deploy.yarn.Client def prepareLocalResources &#123;</span><br><span class="line">    ...</span><br><span class="line">    </span><br><span class="line">    def distribute &#123;</span><br><span class="line">      val trimmedPath &#x3D; path.trim()</span><br><span class="line">      val localURI &#x3D; Utils.resolveURI(trimmedPath)</span><br><span class="line">      if (localURI.getScheme !&#x3D; LOCAL_SCHEME) &#123;</span><br><span class="line">        if (addDistributedUri(localURI)) &#123;</span><br><span class="line">          val localPath &#x3D; getQualifiedLocalPath(localURI, hadoopConf)</span><br><span class="line">          val linkname &#x3D; targetDir.map(_ + &quot;&#x2F;&quot;).getOrElse(&quot;&quot;) +</span><br><span class="line">           destName.orElse(Option(localURI.getFragment())).getOrElse(localPath.getName())</span><br><span class="line">          val destPath &#x3D; copyFileToRemote(destDir, localPath, replication, symlinkCache)</span><br><span class="line">          val destFs &#x3D; FileSystem.get(destPath.toUri(), hadoopConf)</span><br><span class="line">          distCacheMgr.addResource(</span><br><span class="line">            destFs, hadoopConf, destPath, localResources, resType, linkname, statCache,</span><br><span class="line">            appMasterOnly &#x3D; appMasterOnly)</span><br><span class="line">          (false, linkname)</span><br><span class="line">        &#125; else &#123;</span><br><span class="line">          (false, null)</span><br><span class="line">        &#125;</span><br><span class="line">      &#125; else &#123;</span><br><span class="line">        (true, trimmedPath)</span><br><span class="line">      &#125;</span><br><span class="line">    &#125;</span><br><span class="line">    ...</span><br><span class="line">    &#x2F;** * Add Spark to the cache. There are two settings that control what files to add to the cache: * - if a Spark archive is defined, use the archive. The archive is expected to contain * jar files at its root directory. * - if a list of jars is provided, filter the non-local ones, resolve globs, and * add the found files to the cache. * * Note that the archive cannot be a &quot;local&quot; URI. If none of the above settings are found, * then upload all files found in $SPARK_HOME&#x2F;jars. *&#x2F;</span><br><span class="line">    val sparkArchive &#x3D; sparkConf.get(SPARK_ARCHIVE)</span><br><span class="line">    if (sparkArchive.isDefined) &#123;</span><br><span class="line">      val archive &#x3D; sparkArchive.get</span><br><span class="line">      require(!isLocalUri(archive), s&quot;$&#123;SPARK_ARCHIVE.key&#125; cannot be a local URI.&quot;)</span><br><span class="line">      distribute(Utils.resolveURI(archive).toString,</span><br><span class="line">        resType &#x3D; LocalResourceType.ARCHIVE,</span><br><span class="line">        destName &#x3D; Some(LOCALIZED_LIB_DIR))</span><br><span class="line">    &#125;else &#123;</span><br><span class="line">      sparkConf.get(SPARK_JARS) match &#123;</span><br><span class="line">      	case Some(jars) &#x3D;&gt;&#123;</span><br><span class="line">            &#x2F;&#x2F; 操作类似 SPARK_ARCHIVE，把 SPARK_JARS 上传；两者设置一个即可             ... </span><br><span class="line">        &#125;</span><br><span class="line">       case None &#x3D;&gt;</span><br><span class="line">          &#x2F;&#x2F; No configuration, so fall back to uploading local jar files.           logWarning(s&quot;Neither $&#123;SPARK_JARS.key&#125; nor $&#123;SPARK_ARCHIVE.key&#125; is set, falling back &quot; + &quot;to uploading libraries under SPARK_HOME.&quot;)</span><br><span class="line">          &#x2F;&#x2F; 把 spark&#x2F;jars 所有 jar 上传       &#125;</span><br><span class="line">       </span><br><span class="line">    ...</span><br><span class="line">&#125;</span><br><span class="line"> private[yarn] def copyFileToRemote &#123;</span><br><span class="line">        val destFs &#x3D; destDir.getFileSystem(hadoopConf)</span><br><span class="line">        val srcFs &#x3D; srcPath.getFileSystem(hadoopConf)</span><br><span class="line">        var destPath &#x3D; srcPath</span><br><span class="line">        if (force || !compareFs(srcFs, destFs) || &quot;file&quot;.equals(srcFs.getScheme)) &#123;</span><br><span class="line">            destPath &#x3D; new Path(destDir, destName.getOrElse(srcPath.getName()))</span><br><span class="line">            logInfo(s&quot;Uploading resource $srcPath -&gt; $destPath&quot;)</span><br><span class="line">            FileUtil.copy(srcFs, srcPath, destFs, destPath, false, hadoopConf)</span><br><span class="line">            destFs.setReplication(destPath, replication)</span><br><span class="line">            destFs.setPermission(destPath, new FsPermission(APP_FILE_PERMISSION))</span><br><span class="line">        &#125; else &#123;</span><br><span class="line">            logInfo(s&quot;Source and destination file systems are the same. Not copying $srcPath&quot;)</span><br><span class="line">        &#125;</span><br><span class="line">        ...</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>设置 SPARK_ARCHIVE 后，将 SPARK_ARCHIVE 目录分发 (distribute)<br>distribute 中判断是否为本地文件和是否已上传</p>
<p>copyFileToRemote 判断原文件和目标文件是否为同个文件系统，若相同则不上传</p>
<p>destPath 在 createContainerLaunchContext 函数被赋值 appStagingDirPath，根据 hadoop job 执行流程 可知 StagingDir 是 Yarn job 为执行任务存放文件而临时创建的目录；在本案例中就是 HDFS 目录<br>由以上分析可知，只需将 SPARK_ARCHIVE 设置为 hdfs 目录就可以避免每次上传的困扰。</p>
<p>将 spark/jars/*.jar 打包成 zip 并上传到 HDFS</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">spark-submit --class org.apache.spark.examples.SparkPi --master yarn --deploy-mode cluster &#96;--conf spark.yarn.archive&#x3D;hdfs:&#x2F;&#x2F;192.168.22.147:9000&#x2F;lib&#x2F;dep&#x2F;spark&#x2F;spark_jar.zip&#96; spark-examples_2.11-2.4.4.jar 3</span><br><span class="line">...</span><br></pre></td></tr></table></figure>


<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">19&#x2F;10&#x2F;18 12:39:16 INFO yarn.Client: Preparing resources for our AM container</span><br><span class="line">19&#x2F;10&#x2F;18 12:39:16 INFO yarn.Client: &#96;Source and destination file systems are the same. Not copying hdfs:&#x2F;&#x2F;192.168.22.147:9000&#x2F;lib&#x2F;dep&#x2F;spark&#x2F;spark_jar.tar&#96;</span><br><span class="line">...</span><br></pre></td></tr></table></figure>

<p>spark.yarn.archive 也可以设置在 spark-defaults.conf ;</p>
<p>spark.yarn.jars 相同操作，两者等效</p>
<p>Conf<br>看日志可知，每次启动也都上传，它会上传 SPARK_CONF_DIR 和 HADOOP_CONF_DIR目录下的文件。但此 Conf 无法优化，因为就是算是源文件和目标文件在同个文件系统，也会强制复制</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">&#x2F;&#x2F; This code forces the archive to be copied, so that unit tests pass (since in that case both     &#x2F;&#x2F; file systems are the same and the archive wouldn&#39;t normally be copied). In most (all?)     &#x2F;&#x2F; deployments, the archive would be copied anyway, since it&#39;s a temp file in the local file     &#x2F;&#x2F; system.     val remoteConfArchivePath &#x3D; new Path(destDir, LOCALIZED_CONF_ARCHIVE)</span><br><span class="line">    val remoteFs &#x3D; FileSystem.get(remoteConfArchivePath.toUri(), hadoopConf)</span><br><span class="line">    sparkConf.set(CACHED_CONF_ARCHIVE, remoteConfArchivePath.toString())</span><br><span class="line"></span><br><span class="line">    val localConfArchive &#x3D; new Path(createConfArchive().toURI())</span><br><span class="line">    copyFileToRemote(destDir, localConfArchive, replication, symlinkCache, force &#x3D; true,</span><br><span class="line">      destName &#x3D; Some(LOCALIZED_CONF_ARCHIVE))</span><br></pre></td></tr></table></figure>
]]></content>
      <tags>
        <tag>tedata</tag>
      </tags>
  </entry>
  <entry>
    <title>HBase Flush,合并及Rowkey设计</title>
    <url>/2018/02/10/HBase02/</url>
    <content><![CDATA[<h1 id="HBase02"><a href="#HBase02" class="headerlink" title="HBase02"></a>HBase02</h1><p>学习目标：</p>
<ol>
<li>HBase memstore flush 调优</li>
<li>HBase compaction </li>
<li>Rowkey 设计</li>
</ol>
<h3 id="一-HBase-memstore-flush-调优"><a href="#一-HBase-memstore-flush-调优" class="headerlink" title="一. HBase memstore flush 调优"></a>一. HBase memstore flush 调优</h3><ol>
<li>memstore 级别调优</li>
</ol>
<p>habse.region.memstore.flush.size 默认是 128M，生产上建议调大<br>我们的经验值是 512M</p>
<p>这个值表示 某一个 memstore 的值超过 512M，就会进行 flush到storefile</p>
<ol start="2">
<li>region级别调优</li>
</ol>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">hbase.region.memstore.flush.size</span><br><span class="line">hbase.region.memstore.multipiler</span><br></pre></td></tr></table></figure>

<p>这个参数表示 一个 region 中的 所有 memstore 之和加起来超过</p>
<p>hbase.region.memstore.flush.size * hbase.region.memstore.multipiler </p>
<p>就会触发 flush，这个参数是为了防止 有很多个 memstore，但是每个memstore都没有查过 hbase.region.memstore.flush.size<br>但是生产上一般不会出现这种情况，因为一般 CF 不会超过3个，memstore数量也就小于3，不会产生上面情况</p>
<ol start="3">
<li>hregionserver 级别调优</li>
</ol>
<p>这个级别产生 flush，就是灾难的，因为 hregionserver 级别，它里面包含很多的 region，会影响其他表的工作<br>它受以下几个参数影响：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">Java Heap Size Of HBase：默认 50M</span><br><span class="line">hbase.regionserver.upperLimit:  默认 0.4</span><br><span class="line">hbase.regionserver.lowerLimit:  默认 0.95</span><br></pre></td></tr></table></figure>
<p><img src="/2018/02/10/HBase02/h1.png" alt="avatar"></p>
<p>如图，regionserver 规定了一个flush的上界 Upper和下界Lower，Upper 值为 Java Heap Size * 0.4， Lower 值为 Java Heap Size * 0.4 * 0.95<br>比如 Java Heap Size 为 1000M，<br>那么 Upper 为 400M，Lower 为 390M<br>如果 memstore 值 达到 390M，就会触发flush，如果继续增大到 400M，也会进行 flush，直到 memstore值小于 390M 以下才ok</p>
<p>生产上，一般会调整 Java Heap Size ，如果资源充足，建议调整到 32G，不要超过这个值，否则会出现指针压缩失效问题<br>hbase.regionserver.upperLimit 不用作调整<br>hbase.regionserver.lowerLimit 可以在 0.9 - 0.95 之间调整，小于0.9 会报错</p>
<h3 id="二-HBase-compaction"><a href="#二-HBase-compaction" class="headerlink" title="二. HBase compaction"></a>二. HBase compaction</h3><ol>
<li>什么是大合并和小合并</li>
</ol>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">minor compaction</span><br><span class="line">major compaction</span><br></pre></td></tr></table></figure>


<p>如图，相邻storefile 之间的合并称为小合并，小合并之后形成的大的Storefile 之间的合并成为大合并<br>大合并一般包括：<br>TTL 删除<br>put 多版本 合并<br>delete 操作</p>
<p><img src="/2018/02/10/HBase02/h2.png" alt="avatar"></p>
<ol start="2">
<li>何时触发合并</li>
</ol>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">minor compaction</span><br><span class="line">hbase.hstore.compaction.min：触发合并的最小值 </span><br><span class="line">hbase.hstore.compaction.min.size : 小于该值的storefile 一定会加到 合并队列中</span><br><span class="line">hbase.hstore.compaction.max：触发合并的最大值</span><br><span class="line">hbase.hstore.compaction.max.size ： 大于该值的storefile</span><br></pre></td></tr></table></figure>
<p> 一定会被合并队列排除，但这个最大值为 Longvalue.max ，一般不会达到</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">major compaction</span><br><span class="line">hbase.regionserver.majorcompaction  7</span><br><span class="line">hbase.regionserver.majorcompaction.jitter 0.5</span><br></pre></td></tr></table></figure>

<p>那么触发 大合并的时间区间为 [7 - 7 * 0.5 -  7 + 7  * 0.5]<br>生产上禁止此参数</p>
<p>定期检查</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">hbase.server.thread.wakefrequency * hbase.server.compactchecker.interval.multipiler &#x3D; 10000ms * 1000 &#x3D; 10000s &#x3D; 2h 46m 40s</span><br></pre></td></tr></table></figure>

<p>每隔2小时46分40秒检查是否需要compaction</p>
<p>手动触发</p>
<p>执行 major_compact ‘ns1:t1’</p>
<h3 id="三-Rowkey-设计"><a href="#三-Rowkey-设计" class="headerlink" title="三. Rowkey 设计"></a>三. Rowkey 设计</h3><ol>
<li>“加盐”</li>
<li>即 newRK = 随机数 + RK<br>比如 原来的RK 为  1001，1002，1003，1004<br>现在变为 </li>
</ol>
<p>a-1001</p>
<p>b-1002</p>
<p>c-1003</p>
<p>d-1004</p>
<p>在建表的时候</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">create &#39;t1&#39;, &#39;f1&#39;, SPLITS &#x3D;&gt; [&#39;a&#39;, &#39;b&#39;, &#39;c&#39;, &#39;d&#39;]，</span><br></pre></td></tr></table></figure>
<p> 那么就会建一个表，包含</p>
<p>null, a</p>
<p>a,b</p>
<p>b,c</p>
<p>c,d</p>
<p>d,null </p>
<p>这样的表，这样设计的好处<br>原来容易产生热点问题的RK，现在形成了良好的随机性，可以分散到不同的节点上</p>
<p>这样设计的缺点：<br>a. 第一个分区，此处是null，a 这个分区没有值落在上面，造成了分区的浪费<br>b. 对于同一个RK，比如 1001，因为加前缀，很可能造成落在不同的regionserver，在查询时候要拉取各个不同regionserver上的数据，开销大</p>
<p>但是如何 配合 phoinex + 二级索引，可以解决上面的问题</p>
<ol start="2">
<li>hash</li>
</ol>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">RK 设计是 newRK &#x3D; hash(RK) + RK</span><br></pre></td></tr></table></figure>


<p>建表语句</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">create &#39;t1&#39;, &#39;f1&#39;, &#123;NUMREGIONS &#x3D;&gt; 15, SPLITALGO &#x3D;&gt; &#39;HexStringSplit&#39;&#125;</span><br></pre></td></tr></table></figure>

<p>其中 15表示分区的数量</p>
<p>这里相比于 盐表，因为 对于相同的RK，其Hash值是确定相同的，这样，对于同一个RK，<br>Hash(RK) + RK 的设计，其好处是：<br>就能保证同一个RK 能落在同一个 regionserver上，对于同RK 的查询<br>就不存在盐表中的多regionserver 查询问题，同时，也不会因为 随机值带来的第一个分区不能利用的问题<br>缺点是:<br>考虑一钟场景：Hash(1001) + 1001, Hash(1002) + 1002, Hash(1003) + 1003， 这样的数据很可能是落在<br>不同的server上的，如果想要查询它们，作聚合等操作，实际上，这种操作时分常常见的，那么就会产生<br>去不同server拉取数据的情况，造成很大的开销</p>
<ol start="3">
<li>反转<br>反转设计指的是把key值进行倒序设计<br>比如</li>
</ol>
<p>13812345670</p>
<p>13812345671</p>
<p>13812345672</p>
<p>13812345673</p>
<p>13812345674</p>
<p>13812345675</p>
<p>13812345676</p>
<p>13812345677</p>
<p>13812345678</p>
<p>13812345679</p>
<p>这样，反转后，就变成了0-9 开头的RK ，建表时，就可以按 数字进行split<br>create ‘t1’, ‘f1’, SPLITS =&gt; [‘0’,’1’, ‘2’,’3’,’4’,’5’,’6’,’7’,’8’,’9’]<br>形成了和盐表一样的设计<br>这种设计需要RK 在尾部形成良好的随机性</p>
]]></content>
      <tags>
        <tag>tedata</tag>
      </tags>
  </entry>
  <entry>
    <title>HBase 第一课</title>
    <url>/2018/01/24/HBase01/</url>
    <content><![CDATA[<h1 id="HBase01"><a href="#HBase01" class="headerlink" title="HBase01"></a>HBase01</h1><p>学习目标：</p>
<ol>
<li>hbase 架构</li>
<li>hbase 写流程</li>
<li>hbase 读流程</li>
<li>hbase web页面</li>
</ol>
<h3 id="一-hbase架构"><a href="#一-hbase架构" class="headerlink" title="一. hbase架构"></a>一. hbase架构</h3><ol>
<li>hbase 逻辑图<br><img src="/2018/01/24/HBase01/RegionLogic.png" alt="avatar"><br><img src="/2018/01/24/HBase01/datalogic.png" alt="avatar"></li>
</ol>
<p>一个表包含多个region，如果不指定 splitKey，则默认是一个region，当数据达到一定量时，开始分裂成多个region</p>
<p>一个region包含 一个或多个ColumnFamily，即列族，它是一些有相近概念的字段的集合，比如 SKU族， Order族，一般列族不超过3个</p>
<p>一个列族对应一个store，一个store包含 一个 memstore和0到多个storefile<br>数据写是先写到memstore，如何超过一定数据量，就flush到磁盘写道storefile中</p>
<ol start="2">
<li>HBase 的数据模型<br>我们以 下图为例，它有两个 CF, 即 SKU 和 Order<br>左边 五列实际上是一个 store，右边 5列是另一个 store<br>row1的数据就是 SKUName，SKUNum，SKUSum，因为SKUPrice是空值<br>它不会被记录<br><img src="/2018/01/24/HBase01/datamate1.png" alt="avatar"></li>
</ol>
<ol start="3">
<li>HBase 数据模型多版本<br>看下这个图，对于row2，可以看到SKUName 有两个值<br>但是它们的Timestamp 不一样，这就是 HBase的特点，它会把<br>更新的值也insert 进去，只不过时间戳不一样，这就决定了它<br>的写很快的特点</li>
</ol>
<p><img src="/2018/01/24/HBase01/datamate2.png" alt="avatar"></p>
<h3 id="二-HBase-写流程"><a href="#二-HBase-写流程" class="headerlink" title="二. HBase 写流程"></a>二. HBase 写流程</h3><ul>
<li>第一步：和zookeeper通信，get /hasbe/meta-region-server<br>作用是找到 存储 hbase:meta 表的server</li>
</ul>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[zk: localhost:2181(CONNECTED) 2] get &#x2F;hbase&#x2F;meta-region-server</span><br><span class="line">�regionserver:60020���I�&#39;_PBUF</span><br><span class="line"></span><br><span class="line">cZxid &#x3D; 0x300000576</span><br><span class="line"></span><br><span class="line">ctime &#x3D; Sun Feb 16 10:34:37 CST 2020</span><br><span class="line"></span><br><span class="line">mZxid &#x3D; 0x300000576</span><br><span class="line"></span><br><span class="line">mtime &#x3D; Sun Feb 16 10:34:37 CST 2020</span><br><span class="line"></span><br><span class="line">pZxid &#x3D; 0x300000576</span><br><span class="line"></span><br><span class="line">cversion &#x3D; 0</span><br><span class="line"></span><br><span class="line">dataVersion &#x3D; 0</span><br><span class="line"></span><br><span class="line">aclVersion &#x3D; 0</span><br><span class="line"></span><br><span class="line">ephemeralOwner &#x3D; 0x0</span><br><span class="line"></span><br><span class="line">dataLength &#x3D; 62</span><br><span class="line"></span><br><span class="line">numChildren &#x3D; 0</span><br></pre></td></tr></table></figure>

<ul>
<li>第二步 读取 hbase:meta 表，主要获取如下信息</li>
</ul>
<p>表名称/startkey/endkey/字段名称</p>
<p>这样就知道了哪个表在哪台server上，要写入哪个rowkey</p>
<p>这个是 hasbe:meta 的信息：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">tedata:orderinfo,,1576370203149.63c59b5a54 column&#x3D;info:regioninfo, timestamp&#x3D;1581820480055, value&#x3D;&#123;ENCODED &#x3D;&gt; 63c59b5a54b02a27d8969ac352cb6e81, NAME &#x3D;&gt; &#39;tedata:orderinfo,</span><br><span class="line"> b02a27d8969ac352cb6e81.                    ,1576370203149.63c59b5a54b02a27d8969ac352cb6e81.&#39;, STARTKEY &#x3D;&gt; &#39;&#39;, ENDKEY &#x3D;&gt; &#39;&#39;&#125;</span><br></pre></td></tr></table></figure>

<ul>
<li>第三步 写HLOG 文件</li>
</ul>
<p>HBase先会写入 HLOG 日志文件，也成为预写日志文件，只有HLOG 写入成功了，才会写入 去写memstore，这样就保证了数据不会丢失</p>
<ul>
<li>第四步 写store </li>
</ul>
<p>HLOG写入成功后，开始把数据写入 memstore，当memstore写满，flush到storefile中<br>storefile 和 DFS Client 通信，写入到 HDFS 中</p>
<h3 id="三-HBase-读流程"><a href="#三-HBase-读流程" class="headerlink" title="三. HBase 读流程"></a>三. HBase 读流程</h3><ul>
<li>第一步：和zookeeper通信，get /hasbe/meta-region-server<br>作用是找到 存储 hbase:meta 表的server</li>
</ul>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[zk: localhost:2181(CONNECTED) 2] get &#x2F;hbase&#x2F;meta-region-server</span><br><span class="line">�regionserver:60020���I�&#39;_PBUF</span><br><span class="line"></span><br><span class="line">cZxid &#x3D; 0x300000576</span><br><span class="line"></span><br><span class="line">ctime &#x3D; Sun Feb 16 10:34:37 CST 2020</span><br><span class="line"></span><br><span class="line">mZxid &#x3D; 0x300000576</span><br><span class="line"></span><br><span class="line">mtime &#x3D; Sun Feb 16 10:34:37 CST 2020</span><br><span class="line"></span><br><span class="line">pZxid &#x3D; 0x300000576</span><br><span class="line"></span><br><span class="line">cversion &#x3D; 0</span><br><span class="line"></span><br><span class="line">dataVersion &#x3D; 0</span><br><span class="line"></span><br><span class="line">aclVersion &#x3D; 0</span><br><span class="line"></span><br><span class="line">ephemeralOwner &#x3D; 0x0</span><br><span class="line"></span><br><span class="line">dataLength &#x3D; 62</span><br><span class="line"></span><br><span class="line">numChildren &#x3D; 0</span><br></pre></td></tr></table></figure>

<ul>
<li>第二步 读取 hbase:meta 表，主要获取如下信息</li>
</ul>
<p>表名称/startkey/endkey/字段名称</p>
<p>这样就知道了哪个表在哪台server上，要写入哪个rowkey</p>
<p>这个是 hasbe:meta 的信息：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">tedata:orderinfo,,1576370203149.63c59b5a54 column&#x3D;info:regioninfo, timestamp&#x3D;1581820480055, value&#x3D;&#123;ENCODED &#x3D;&gt; 63c59b5a54b02a27d8969ac352cb6e81, NAME &#x3D;&gt; &#39;tedata:orderinfo,</span><br><span class="line"> b02a27d8969ac352cb6e81.                    ,1576370203149.63c59b5a54b02a27d8969ac352cb6e81.&#39;, STARTKEY &#x3D;&gt; &#39;&#39;, ENDKEY &#x3D;&gt; &#39;&#39;&#125;</span><br></pre></td></tr></table></figure>

<ul>
<li>第三步</li>
</ul>
<p>先去memstore读取数据，如果没有，从Blockcache 读取，如果没有，<br>去HDFS 读取数据</p>
]]></content>
      <tags>
        <tag>tedata</tag>
      </tags>
  </entry>
  <entry>
    <title>CDH 完美卸载</title>
    <url>/2017/07/20/CDH%E5%AE%8C%E7%BE%8E%E5%8D%B8%E8%BD%BD/</url>
    <content><![CDATA[<h1 id="CDH完美卸载"><a href="#CDH完美卸载" class="headerlink" title="CDH完美卸载"></a>CDH完美卸载</h1><ul>
<li><input checked disabled type="checkbox"> 学习目标：</li>
<li><input checked disabled type="checkbox"> CDH 卸载</li>
</ul>
<h3 id="一-卸载前的规划"><a href="#一-卸载前的规划" class="headerlink" title="一. 卸载前的规划"></a>一. 卸载前的规划</h3><ul>
<li>关闭集群 及 MySQL服务</li>
<li>删除部署文件夹 /opt/cloudera*</li>
<li>删除数据文件夹</li>
</ul>
<h3 id="二-如何完美卸载集群"><a href="#二-如何完美卸载集群" class="headerlink" title="二.如何完美卸载集群"></a>二.如何完美卸载集群</h3><ol>
<li>HDFS YARN ZK存储数据目录</li>
</ol>
<ul>
<li>/dfs/nn</li>
<li>/dfs/dn</li>
<li>/dfs/snn</li>
<li>/yarn/nm</li>
<li>/var/lib/zookeeper</li>
</ul>
<ol start="2">
<li><p>关闭集群 及 MySQL服务</p>
</li>
<li><p>杀进程(执行2次)</p>
</li>
</ol>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">kill -9 $(pgrep -f cloudera)</span><br><span class="line">pgrep -f cloudera</span><br></pre></td></tr></table></figure>


<p>2.4 卸载</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">umount &#x2F;opt&#x2F;cloudera-manager&#x2F;cm-5.16.2&#x2F;run&#x2F;cloudera-scm-agent&#x2F;process</span><br></pre></td></tr></table></figure>


<p>假如无法卸载，夯住了 就强制kill</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">yum install -y lsof</span><br><span class="line">lsof &#x2F;opt&#x2F;cloudera-manager&#x2F;cm-5.16.2&#x2F;run&#x2F;cloudera-scm-agent&#x2F;process</span><br><span class="line">kill -9 $(lsof &#x2F;opt&#x2F;cloudera-manager&#x2F;cm-5.16.2&#x2F;run&#x2F;cloudera-scm-agent&#x2F;process | awk &#39;&#123;print $2&#125;&#39;)</span><br></pre></td></tr></table></figure>



<p>一定要再df -h 校验一下</p>
<p>2.5 删除cloudera部署文件夹</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">rm -rf &#x2F;opt&#x2F;cloudera*</span><br><span class="line">rm -rf &#x2F;dfs &#x2F;yarn &#x2F;var&#x2F;lib&#x2F;zookeeper</span><br><span class="line"></span><br><span class="line">rm -rf &#x2F;usr&#x2F;share&#x2F;cmf</span><br><span class="line">rm -rf &#x2F;var&#x2F;lib&#x2F;cloudera*</span><br><span class="line">rm -rf &#x2F;var&#x2F;log&#x2F;cloudera*</span><br><span class="line">rm -rf &#x2F;run&#x2F;cloudera-scm-agent</span><br><span class="line">rm -rf &#x2F;etc&#x2F;security&#x2F;limits.d&#x2F;cloudera-scm.conf </span><br><span class="line">rm -rf &#x2F;etc&#x2F;cloudera*</span><br><span class="line">rm -rf &#x2F;etc&#x2F;hadoop* &#x2F;etc&#x2F;zookeeper &#x2F;etc&#x2F;hive* &#x2F;etc&#x2F;hbase* &#x2F;etc&#x2F;impala &#x2F;etc&#x2F;spark &#x2F;etc&#x2F;solr &#x2F;etc&#x2F;sqoop*</span><br><span class="line">rm -rf &#x2F;tmp&#x2F;scm_*   &#x2F;tmp&#x2F;.scm_prepare_node.lock</span><br></pre></td></tr></table></figure>


<p>2.6 全局搜索 删除</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">find &#x2F; -name &#39;*cloudera*&#39; | while read line; do rm -rf $&#123;line&#125;; done</span><br></pre></td></tr></table></figure>


<p>2.7 MySQL的数据库</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">cmf db--》cm server</span><br><span class="line">amon db--》amon</span><br><span class="line"></span><br><span class="line">mysql&gt; drop database cmf;</span><br><span class="line">Query OK, 47 rows affected (0.60 sec)</span><br><span class="line"></span><br><span class="line">mysql&gt; drop database amon;</span><br><span class="line">Query OK, 64 rows affected (0.23 sec)</span><br><span class="line"></span><br><span class="line">mysql&gt; drop user cmf;</span><br><span class="line">Query OK, 0 rows affected (0.00 sec)</span><br><span class="line"></span><br><span class="line">mysql&gt; drop user amon;</span><br><span class="line">Query OK, 0 rows affected (0.00 sec)</span><br><span class="line"></span><br><span class="line">mysql&gt; use mysql;</span><br><span class="line">Database changed</span><br><span class="line">mysql&gt; select user from user;</span><br><span class="line">+-----------+</span><br><span class="line">| user      |</span><br><span class="line">+-----------+</span><br><span class="line">| root      |</span><br><span class="line">| mysql.sys |</span><br><span class="line">| root      |</span><br><span class="line">+-----------+</span><br><span class="line">3 rows in set (0.00 sec)</span><br></pre></td></tr></table></figure>


<h3 id="三-有个坑需要踩"><a href="#三-有个坑需要踩" class="headerlink" title="三. 有个坑需要踩"></a>三. 有个坑需要踩</h3><p>alternatives 一个组件多版本 动态管理</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[root@ruozedata001 alternatives]# alternatives --config hadoop</span><br><span class="line"></span><br><span class="line">There is 1 program that provides &#39;hadoop&#39;.</span><br><span class="line"></span><br><span class="line">  Selection    Command</span><br><span class="line">-----------------------------------------------</span><br><span class="line">*+ 1           &#x2F;opt&#x2F;cloudera&#x2F;parcels&#x2F;CDH-5.16.2-1.cdh5.16.2.p0.8&#x2F;bin&#x2F;hadoop</span><br><span class="line"> + 2 	       &#x2F;opt&#x2F;cloudera&#x2F;parcels&#x2F;CDH-5.16.3-1.cdh5.16.3.p0.8&#x2F;bin&#x2F;hadoop</span><br><span class="line"></span><br><span class="line">Enter to keep the current selection[+], or type selection number: 1</span><br><span class="line">[root@ruozedata001 alternatives]#</span><br></pre></td></tr></table></figure>



<p>未来，本套机器需要升级 假如CDH5.16.3,就是hadoop命令找不到<br>修复这个版本切换：  alternatives –config hadoop<br><a href="http://blog.itpub.net/30089851/viewspace-2128683/" target="_blank" rel="noopener">http://blog.itpub.net/30089851/viewspace-2128683/</a></p>
]]></content>
      <tags>
        <tag>tedata</tag>
      </tags>
  </entry>
  <entry>
    <title>CDH 通过实例恢复集群</title>
    <url>/2017/07/12/CDH%E9%80%9A%E8%BF%87%E5%AE%9E%E4%BE%8B%E6%81%A2%E5%A4%8D%E9%9B%86%E7%BE%A4/</url>
    <content><![CDATA[<h1 id="CDH通过实例恢复集群"><a href="#CDH通过实例恢复集群" class="headerlink" title="CDH通过实例恢复集群"></a>CDH通过实例恢复集群</h1><ol>
<li><p>修改新的 内网IP</p>
</li>
<li><p>启动 mysql</p>
</li>
<li><p>server<br>修改 db<br>cat db.properties<br><img src="/2017/07/12/CDH%E9%80%9A%E8%BF%87%E5%AE%9E%E4%BE%8B%E6%81%A2%E5%A4%8D%E9%9B%86%E7%BE%A4/cdh0301.png" alt="avatar"><br><img src="/2017/07/12/CDH%E9%80%9A%E8%BF%87%E5%AE%9E%E4%BE%8B%E6%81%A2%E5%A4%8D%E9%9B%86%E7%BE%A4/cdh0302.png" alt="avatar"></p>
</li>
</ol>
<ol start="4">
<li><p>agent  (有agent的机器都要检查)<br>config.ini<br><img src="/2017/07/12/CDH%E9%80%9A%E8%BF%87%E5%AE%9E%E4%BE%8B%E6%81%A2%E5%A4%8D%E9%9B%86%E7%BE%A4/cdh0303.png" alt="avatar"></p>
</li>
<li><p>mysql 表<br>use cmf;<br>desc hosts;<br>update hosts set ip = xxx where hosts = xxx<br><img src="/2017/07/12/CDH%E9%80%9A%E8%BF%87%E5%AE%9E%E4%BE%8B%E6%81%A2%E5%A4%8D%E9%9B%86%E7%BE%A4/cdh0304.png" alt="avatar"></p>
</li>
</ol>
<p>tail -f /-F 的区别是什么？</p>
<ol start="6">
<li><p>启动 server<br>./cloudera-scm-server start</p>
</li>
<li><p>启动 所以 agent</p>
</li>
<li><p>web 界面启动 cms<br>可以看到，ip发生了变化<br><img src="/2017/07/12/CDH%E9%80%9A%E8%BF%87%E5%AE%9E%E4%BE%8B%E6%81%A2%E5%A4%8D%E9%9B%86%E7%BE%A4/cdh0305.png" alt="avatar"></p>
</li>
</ol>
<p>需要重新部署下 客户端</p>
]]></content>
      <tags>
        <tag>tedata</tag>
      </tags>
  </entry>
  <entry>
    <title>CDH 常规使用</title>
    <url>/2017/07/09/CDH01/</url>
    <content><![CDATA[<h1 id="CDH-常规使用"><a href="#CDH-常规使用" class="headerlink" title="CDH 常规使用"></a>CDH 常规使用</h1><p>学习目标：</p>
<ol>
<li>cdh官网解读</li>
<li>cdh启动/停止</li>
<li>cdh架构</li>
<li>日志解读</li>
<li>web页面分析</li>
<li>怎样添加服务</li>
<li>监控</li>
</ol>
<h3 id="一-cloudera官网"><a href="#一-cloudera官网" class="headerlink" title="一. cloudera官网"></a>一. cloudera官网</h3><p><a href="http://www.cloudera.com" target="_blank" rel="noopener">www.cloudera.com</a>  CM不开源  CDH: CM+apache组件和cloudera公司的组件</p>
<p>CDH 6.x 6.3.1   HDFS3.0 HBase2.0<br>    5.x 5.16.1  HDFS2.6 HBase1.2</p>
<p><a href="https://www.cloudera.com/downloads/manager/5-16-2.html" target="_blank" rel="noopener">https://www.cloudera.com/downloads/manager/5-16-2.html</a><br><a href="https://docs.cloudera.com/documentation/index.html" target="_blank" rel="noopener">https://docs.cloudera.com/documentation/index.html</a><br><a href="https://docs.cloudera.com/documentation/enterprise/5-16-x.html" target="_blank" rel="noopener">https://docs.cloudera.com/documentation/enterprise/5-16-x.html</a></p>
<h3 id="二-正常启动停止顺序"><a href="#二-正常启动停止顺序" class="headerlink" title="二. 正常启动停止顺序"></a>二. 正常启动停止顺序</h3><p>su - mysqladmin</p>
<p>service mysql start</p>
<p>/opt/cloudera-manager/cm-5.16.2/etc/init.d/cloudera-scm-server start 1个节点<br>/opt/cloudera-manager/cm-5.16.2/etc/init.d/cloudera-scm-agent start 所有节点</p>
<p>进入web7180，先启动 CMS 5个进程服务</p>
<p>启动Cluster1服务: HDFS YARN ZK KAFKA HBASE</p>
<p>停止顺序?</p>
<p>坑: mysql 单点 cm metadata </p>
<p>cm挂了 启动 初始化  cm metadata + hive 表数据 重新初始化</p>
<p>1.mysql没有开启binlog<br>2.mysql没有 定期备份 1天<br>mysqldump命令 cmf &gt;cmf.sql</p>
<h3 id="三-架构"><a href="#三-架构" class="headerlink" title="三. 架构"></a>三. 架构</h3><p><img src="/2017/07/09/CDH01/cma.png" alt="avatar"></p>
<p>假如 CM web界面server服务挂了，HDFS YARN这些服务正常吗？</p>
<p>配置:<br>服务端   /opt/cloudera-manager/cm-5.16.2/run/cloudera-scm-agent/process/366-hdfs-NAMENODE<br>客户端   /etc/hadoop/conf</p>
<p>1.cmf.config表<br>2.服务端 带序号的<br>3.客户端 不带序号 默认的</p>
<p>务必从web界面修改参数值 </p>
<p>应用开发 配置 /etc/hadoop/conf</p>
<p>客户端  gateway<br>    在web上点击 添加gateway服务，添加后，不需要重启服务，只需要重新部署客户端即可<br><img src="/2017/07/09/CDH01/gateway.png" alt="avatar"></p>
<h3 id="四-日志"><a href="#四-日志" class="headerlink" title="四. 日志"></a>四. 日志</h3><p><a href="http://106.14.180.252:7180/cmf/config2?task=ALL_LOG_DIRECTORIES" target="_blank" rel="noopener">http://106.14.180.252:7180/cmf/config2?task=ALL_LOG_DIRECTORIES</a></p>
<p>Configruation –&gt; log service 查看日志目录</p>
<p>组件服务的日志： /var/log/xxx</p>
<p>TAR CM的 /opt/cloudera-manager/cm-5.16.2/log/cloudera-scm-server<br>     /opt/cloudera-manager/cm-5.16.2/log/cloudera-scm-agent</p>
<p>RPM部署CM ：</p>
<pre><code>/var/log/cloudera-scm-server 
/var/log/cloudera-scm-agent</code></pre><p>xxxxx..log.out  进程的日志  出现error  优先排查<br>Will not attempt to authenticate using SASL (unknown error)</p>
<p>stdout 和 stderr 相当于 shell 脚本的debug的输出</p>
<p>xxxx.stdout</p>
<p>xxxx.stderr</p>
<p><a href="http://blog.itpub.net/30089851/viewspace-2136372/" target="_blank" rel="noopener">http://blog.itpub.net/30089851/viewspace-2136372/</a></p>
<h3 id="五-界面解读"><a href="#五-界面解读" class="headerlink" title="五. 界面解读"></a>五. 界面解读</h3><p>进程 process  instance  role</p>
<p>NameNode Advanced Configuration Snippet (Safety Valve) for hdfs-site.xml</p>
<p>ranger HDP</p>
<h3 id="六-添加服务-amp-HOST"><a href="#六-添加服务-amp-HOST" class="headerlink" title="六. 添加服务&amp;HOST"></a>六. 添加服务&amp;HOST</h3><p>先手工的部署agent 启动/opt/cloudera-manager/cm-5.16.2/etc/init.d/cloudera-scm-agent start</p>
<h3 id="七-监控"><a href="#七-监控" class="headerlink" title="七. 监控"></a>七. 监控</h3><p>TS query language  TSQL</p>
<p>生产者<br>SELECT total_kafka_bytes_received_rate_across_kafka_broker_topics<br>WHERE entityName = “kafka:DSHS” AND category = KAFKA_TOPIC</p>
<p>消费者<br>SELECT total_kafka_bytes_fetched_rate_across_kafka_broker_topics<br>WHERE entityName = “kafka:DSHS” AND category = KAFKA_TOPIC</p>
<p>SELECT<br>total_kafka_bytes_received_rate_across_kafka_broker_topics,<br>total_kafka_bytes_fetched_rate_across_kafka_broker_topics<br>WHERE entityName = “kafka:DSHS” AND category = KAFKA_TOPIC</p>
<p>charts –》 Chart Builder ，粘贴SQL，出图，给 title 命个名，save –》 HOME PAGE</p>
<p>如果 对监控指标 不是很明确，以HDFS 为例，可以点击 “HDFS” –》 “Charts Library” 去里面选择想要的图的SQL</p>
<p>丢: 配置邮件预警 +….+Kafka+Spark2</p>
]]></content>
      <tags>
        <tag>tedata</tag>
      </tags>
  </entry>
  <entry>
    <title>Spark SQL 第二课</title>
    <url>/2017/04/12/SparkSQL02/</url>
    <content><![CDATA[<h1 id="SparkSQL02"><a href="#SparkSQL02" class="headerlink" title="SparkSQL02"></a>SparkSQL02</h1><p>学习目标：</p>
<ol>
<li>saveAsTable 和 insertInto 的区别</li>
<li>怎么创建视图，即DF 创建表</li>
<li>Catalog 使用</li>
<li>DF/DS/RDD 相互转换</li>
<li>UDF 函数</li>
</ol>
<h3 id="一-saveAsTable-和-insertInto-的区别"><a href="#一-saveAsTable-和-insertInto-的区别" class="headerlink" title="一. saveAsTable 和 insertInto 的区别"></a>一. saveAsTable 和 insertInto 的区别</h3><p>sql 写出的数据 保存成一张表, 操作hive<br>方式一</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">spark.sql(&quot;&quot;).write.saveAsTable(&quot;&quot;)</span><br></pre></td></tr></table></figure>


<p>方式二</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">spark.sql(&quot;&quot;).write.insertInto(&quot;&quot;)</span><br></pre></td></tr></table></figure>


<p>saveAsTable 和 insertInto 的区别:</p>
<p>insertInto 忽略字段名称, 而是基于位置插入的, 即按顺序插入</p>
<p>看下下面的测试:</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">scala&gt; Seq((1,2)).toDF(&quot;i&quot;,&quot;j&quot;).write.mode(&quot;overwrite&quot;).saveAsTable(&quot;t1&quot;)</span><br><span class="line">scala&gt; sql(&quot;select * from t1&quot;).show</span><br><span class="line">+---+---+</span><br><span class="line">|  i|  j|</span><br><span class="line">+---+---+</span><br><span class="line">|  1|  2|</span><br><span class="line">+---+---+</span><br></pre></td></tr></table></figure>

<p><img src="/2017/04/12/SparkSQL02/insertinto1.png" alt="avatar"></p>
<p>可以看到, saveAsTable 是根据字段插入的, 字段i,j 和 值是对应的</p>
<p>可以看到, insertInto 是根据顺序来的, i, j 并没有和 值对应<br><img src="/2017/04/12/SparkSQL02/saveastable.png" alt="avatar"></p>
<p>测试表存在和不存在上面两种方式的区别?</p>
<h3 id="二-怎么创建视图，即DF-创建表"><a href="#二-怎么创建视图，即DF-创建表" class="headerlink" title="二. 怎么创建视图，即DF 创建表"></a>二. 怎么创建视图，即DF 创建表</h3><p>spark.sql(“create table…”) //这种方式不推荐, 因为 创建表是需要权限的, 提前创建好最好</p>
<p>临时视图和 全局视图<br>创建临时视图</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">df.createOrReplaceTempView(&quot;people&quot;)</span><br></pre></td></tr></table></figure>

<p>创建全局视图</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">df.createGlobalTempView(&quot;people&quot;)</span><br></pre></td></tr></table></figure>

<p>全局视图必须在 视图前加 “ global_temp”</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">spark.sql(&quot;SELECT * FROM global_temp.people&quot;).show()</span><br></pre></td></tr></table></figure>


<p>platform 组内 province访问次数最多的 TOPN</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">select platform, province, count(1) from  log group  by  platform, province</span><br></pre></td></tr></table></figure>



<h3 id="三-Catalog-使用"><a href="#三-Catalog-使用" class="headerlink" title="三. Catalog 使用"></a>三. Catalog 使用</h3><p>catalog</p>
<p>创建catalog<br>scala&gt; val catalog = spark.catalog</p>
<p>查看数据库</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">scala&gt; catalog.listDatabases.show</span><br><span class="line">+-------+--------------------+--------------------+</span><br><span class="line">|   name|         description|         locationUri|</span><br><span class="line">+-------+--------------------+--------------------+</span><br><span class="line">|    bdp|                    |hdfs:&#x2F;&#x2F;hdcluster&#x2F;...|</span><br><span class="line">|default|Default Hive data...|hdfs:&#x2F;&#x2F;hdcluster&#x2F;...|</span><br><span class="line">|   test|                    |hdfs:&#x2F;&#x2F;hdcluster&#x2F;...|</span><br><span class="line">+-------+--------------------+--------------------+</span><br></pre></td></tr></table></figure>




<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">scala&gt; catalog.listDatabases.show(false)</span><br><span class="line">+-------+---------------------+--------------------------------------------+</span><br><span class="line">|name   |description          |locationUri                                 |</span><br><span class="line">+-------+---------------------+--------------------------------------------+</span><br><span class="line">|bdp    |                     |hdfs:&#x2F;&#x2F;hdcluster&#x2F;user&#x2F;hive&#x2F;warehouse&#x2F;bdp.db |</span><br><span class="line">|default|Default Hive database|hdfs:&#x2F;&#x2F;hdcluster&#x2F;user&#x2F;hive&#x2F;warehouse        |</span><br><span class="line">|test   |                     |hdfs:&#x2F;&#x2F;hdcluster&#x2F;user&#x2F;hive&#x2F;warehouse&#x2F;test.db|</span><br><span class="line">+-------+---------------------+--------------------------------------------+</span><br></pre></td></tr></table></figure>

<p>查看表</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">scala&gt; catalog.listTables(&quot;test&quot;).show</span><br><span class="line">+-------+--------+-----------+---------+-----------+</span><br><span class="line">|   name|database|description|tableType|isTemporary|</span><br><span class="line">+-------+--------+-----------+---------+-----------+</span><br><span class="line">|  test2|    test|       null|  MANAGED|      false|</span><br><span class="line">| test22|    test|       null|  MANAGED|      false|</span><br><span class="line">|tratest|    test|       null|  MANAGED|      false|</span><br><span class="line">+-------+--------+-----------+---------+-----------+</span><br></pre></td></tr></table></figure>


<p>查看所有函数</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">scala&gt; catalog.listFunctions.show</span><br><span class="line">+----------+--------+-----------+--------------------+-----------+</span><br><span class="line">|      name|database|description|           className|isTemporary|</span><br><span class="line">+----------+--------+-----------+--------------------+-----------+</span><br><span class="line">|         !|    null|       null|org.apache.spark....|       true|</span><br><span class="line">|         %|    null|       null|org.apache.spark....|       true|</span><br><span class="line">|         &amp;|    null|       null|org.apache.spark....|       true|</span><br><span class="line">|         *|    null|       null|org.apache.spark....|       true|</span><br><span class="line">|         +|    null|       null|org.apache.spark....|       true|</span><br><span class="line">|         -|    null|       null|org.apache.spark....|       true|</span><br></pre></td></tr></table></figure>


<p>查看字段</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">scala&gt; catalog.listColumns(&quot;test.test22&quot;).show</span><br><span class="line">+----+-----------+--------+--------+-----------+--------+</span><br><span class="line">|name|description|dataType|nullable|isPartition|isBucket|</span><br><span class="line">+----+-----------+--------+--------+-----------+--------+</span><br><span class="line">| uid|       null|  string|    true|      false|   false|</span><br><span class="line">| pid|       null|  string|    true|      false|   false|</span><br></pre></td></tr></table></figure>


<h3 id="四-DF-DS-RDD-相互转换"><a href="#四-DF-DS-RDD-相互转换" class="headerlink" title="四. DF/DS/RDD 相互转换"></a>四. DF/DS/RDD 相互转换</h3><p>DF/DF/RDD</p>
<p>ROW DF 弱类型</p>
<ul>
<li>df 转ds</li>
</ul>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">import spark.implicits._</span><br><span class="line">val df &#x3D; spark.read.option(&quot;header&quot;,&quot;true&quot;)</span><br><span class="line">.option(&quot;inferSchema&quot;,&quot;true&quot;).csv(&quot;data&#x2F;sales.csv&quot;)</span><br><span class="line">val ds &#x3D; df.as[Sales]</span><br><span class="line">&#x2F;&#x2F; Sales 是一个case class</span><br><span class="line">case class Sales(transactionId:Int,customerId:Int,itemId:Int,amountPaid:Double)</span><br></pre></td></tr></table></figure>


<ul>
<li>ds 转df</li>
</ul>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">import spark.implicits._</span><br><span class="line"></span><br><span class="line">val ds &#x3D; spark.read.textFile(&quot;data&#x2F;access.log&quot;)</span><br><span class="line">      .map(x &#x3D;&gt; &#123;</span><br><span class="line">        val splits &#x3D; x.split(&quot;\t&quot;)</span><br><span class="line">        val platform &#x3D; splits(1)</span><br><span class="line">        val traffic &#x3D; splits(6).toLong</span><br><span class="line">        val province &#x3D; splits(8)</span><br><span class="line">        val city &#x3D; splits(9)</span><br><span class="line">        val isp &#x3D; splits(10)</span><br><span class="line">        (platform, traffic, province, city, isp)</span><br><span class="line">      &#125;)</span><br><span class="line">val df &#x3D; ds.toDF(&quot;platform&quot;, &quot;traffic&quot;, &quot;province&quot;, &quot;city&quot;, &quot;isp&quot;)</span><br></pre></td></tr></table></figure>


<pre><code>  // 如果你想使用SQL来进行处理，那么就是将df注册成一个临时视图
df.createOrReplaceTempView(&quot;log&quot;)</code></pre><p>row_number<br>rank<br>dense_rank<br>的区别</p>
<ul>
<li>RDD =&gt; DF</li>
</ul>
<ol>
<li>反射</li>
</ol>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">import spark.implicits._</span><br><span class="line">  val peopleDF &#x3D; spark.sparkContext</span><br><span class="line">     .textFile(&quot;data&#x2F;people.txt&quot;)</span><br><span class="line">     .map(_.split(&quot;,&quot;))</span><br><span class="line">     .map(x &#x3D;&gt; Person(x(0), x(1).trim.toInt))</span><br><span class="line">     .toDF()</span><br><span class="line"></span><br><span class="line">   peopleDF.show(false)</span><br><span class="line"></span><br><span class="line">case class Person(name:String,age:Int)</span><br></pre></td></tr></table></figure>


<p>分解:</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">val peopleDF &#x3D; spark.sparkContext</span><br><span class="line">     .textFile(&quot;data&#x2F;people.txt&quot;) </span><br><span class="line">得到一个 RDD</span><br></pre></td></tr></table></figure>



<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">.map(_.split(&quot;,&quot;))</span><br><span class="line">&#x2F;&#x2F; 这一步是一个反射, Person是一个case class</span><br><span class="line">&#x2F;&#x2F;已经有schema信息了, 而 RDD 相对于 DF</span><br><span class="line">&#x2F;&#x2F;缺少的就是schema</span><br><span class="line">     .map(x &#x3D;&gt; Person(x(0), x(1).trim.toInt))</span><br><span class="line">     .toDF()</span><br><span class="line">转成一个DF</span><br></pre></td></tr></table></figure>


<ol start="2">
<li>编程自定义<br>// step1: Create an RDD</li>
</ol>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">val peopleRDD &#x3D; spark.sparkContext.textFile(&quot;data&#x2F;people.txt&quot;)</span><br></pre></td></tr></table></figure>



<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">&#x2F;&#x2F; step2: The schema is encoded in a string</span><br><span class="line">    &#x2F;&#x2F; 和官网提供的不同, 因为字段类型多样, 官网只给了字符串类型写法</span><br><span class="line">    &#x2F;&#x2F; 下面是更适用的写法</span><br><span class="line"></span><br><span class="line">    val schema &#x3D; StructType(Array(</span><br><span class="line">      StructField(&quot;name&quot;,StringType),</span><br><span class="line">      StructField(&quot;age&quot;,IntegerType)</span><br><span class="line">    ))</span><br><span class="line"></span><br><span class="line">    &#x2F;&#x2F; Convert records of the RDD (people) to Rows</span><br><span class="line">    val rowRDD &#x3D; peopleRDD</span><br><span class="line">      .map(_.split(&quot;,&quot;))</span><br><span class="line">      .map(attributes &#x3D;&gt; Row(attributes(0), attributes(1).trim.toInt)) &#x2F;&#x2F; 要写toInt</span><br><span class="line"></span><br><span class="line">    &#x2F;&#x2F; Apply the schema to the RDD</span><br><span class="line">    val peopleDF &#x3D; spark.createDataFrame(rowRDD, schema)</span><br><span class="line"></span><br><span class="line">    peopleDF.show()</span><br></pre></td></tr></table></figure>



<p>注意下这段代码, 起了两个 spark context, 会报spark context 异常</p>
<p><img src="/2017/04/12/SparkSQL02/sperror1.png" alt="avatar"><br><img src="/2017/04/12/SparkSQL02/sperror2.png" alt="avatar"></p>
<p>正确的做法是:<br>用 spark session 起 spark context<br><img src="/2017/04/12/SparkSQL02/spok.png" alt="avatar"></p>
<h3 id="五-UDF-函数"><a href="#五-UDF-函数" class="headerlink" title="五. UDF 函数"></a>五. UDF 函数</h3><p>spark sql 注册 udf 的两种方式:</p>
<ol>
<li>sqlContext.udf.register()<br>只对 sql 有效</li>
<li>spark.sql.function.udf()<br>此时注册的方法，对外部可见, 可以使用api</li>
</ol>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">select AD.* ,</span><br><span class="line">ROUND(AD.adbidseccesscounts&#x2F;AD.adbidcounts * 100, 2) bidrate,</span><br><span class="line">ROUND(AD.adclickcounts&#x2F;AD.addispalycounts * 100, 2) clickrate</span><br><span class="line">from</span><br><span class="line">(</span><br><span class="line">select</span><br><span class="line">province,</span><br><span class="line">city,</span><br><span class="line">sum (case when requestmode&#x3D;1 and processnode&gt;&#x3D;1 then 1 else 0 end) requestmodecounts,</span><br><span class="line">sum (case when requestmode&#x3D;1 and processnode&gt;&#x3D;2 then 1 else 0 end) processnodecounts,</span><br><span class="line">sum (case when requestmode&#x3D;1 and processnode&#x3D;3 then 1 else 0 end) adrequestcounts,</span><br><span class="line">sum (case when adplatformproviderid&gt;&#x3D;100000 and iseffective&#x3D;1</span><br><span class="line">     and isbilling&#x3D;1 and isbid&#x3D;1 and adorderid!&#x3D;0 then 1 else 0 end) adbidcounts,</span><br><span class="line">sum (case when adplatformproviderid&gt;&#x3D;100000 and iseffective&#x3D;1</span><br><span class="line">     and isbilling&#x3D;1 and iswin&#x3D;1 then 1 else 0 end) adbidseccesscounts,</span><br><span class="line">sum (case when requestmode&#x3D;2 and iseffective&#x3D;1 then 1 else 0 end) addispalycounts,</span><br><span class="line">sum (case when requestmode&#x3D;3 and iseffective&#x3D;1 then 1 else 0 end) adclickcounts,</span><br><span class="line">sum (case when requestmode&#x3D;2 and iseffective&#x3D;1 and isbilling&#x3D;1 then 1 else 0 end) mediadispalycounts,</span><br><span class="line">sum (case when requestmode&#x3D;3 and iseffective&#x3D;1 and isbilling&#x3D;1 then 1 else 0 end) mediaclickcounts,</span><br><span class="line">sum (case when adplatformproviderid&gt;&#x3D;100000 and iseffective&#x3D;1</span><br><span class="line">     and isbilling&#x3D;1 and iswin&#x3D;1 and adorderid&gt;200000 then 1 else 0 end) adconsumecounts,</span><br><span class="line">sum (case when adplatformproviderid&gt;&#x3D;100000 and iseffective&#x3D;1</span><br><span class="line">     and isbilling&#x3D;1 and iswin&#x3D;1 and adorderid&gt;200000 then 1 else 0 end) adcostcounts</span><br><span class="line">from SYS_AD  GROUP BY province,city) AD</span><br></pre></td></tr></table></figure>

]]></content>
      <tags>
        <tag>tedata</tag>
      </tags>
  </entry>
  <entry>
    <title>Spark SQL 第一课</title>
    <url>/2017/04/10/SparkSQL01/</url>
    <content><![CDATA[<h1 id="SparkSQL01"><a href="#SparkSQL01" class="headerlink" title="SparkSQL01"></a>SparkSQL01</h1><p>学习目标：</p>
<ol>
<li>SparkSQL 介绍及与 hive on spark 区别</li>
<li>SparkSQL 编程 POM 依赖</li>
<li>SparkSQL 编程规范</li>
<li>SparkSQL 连接 数据源演示</li>
</ol>
<h3 id="一-SparkSQL-介绍及与-hive-on-spark-区别"><a href="#一-SparkSQL-介绍及与-hive-on-spark-区别" class="headerlink" title="一. SparkSQL 介绍及与 hive on spark 区别"></a>一. SparkSQL 介绍及与 hive on spark 区别</h3><p>SparkSQL 支持hive 语法, 同时支持 hive 序列化/反序列化 , UDF 等<br>支持 访问存在的 hive 仓库</p>
<p>ExtDS : 外部数据源</p>
<p>Hive是进程级别的<br>Spark是线程级别的， 如果用Shark，会存在线程安全的问题</p>
<p>Hive On Spark:  hive 跑在 spark 引擎之上(原来是跑在MR之上)<br>和 Spark SQL 不是一回事<br>set hive.execution.engine = spark 即可</p>
<p>Spark SQL 是在 Spark 里面的<br>Hive On Spark 是在 hive 里面</p>
<p>Spark SQL<br>1.0<br>称为<br>SchemaRdd ==&gt; Table<br>==&gt; DataFrame<br>==&gt; DataSets  1.6  complie-time type safety, 编译时的类型检查, 提前抛出异常</p>
<p>Dataset:<br>dataset 是一个分布式的数据集<br>A Dataset is a distributed collection of data.<br>named columns , 带名字的列, 可以理解为一个表<br>In scala API, DataFrame is simply a type alias of DataSet[ROW]</p>
<p>Dataset API 仅支持 scala, java, 不支持 python</p>
<p>Spark SQL 添加依赖</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">&lt;dependency&gt;</span><br><span class="line">    &lt;groupId&gt;org.apache.spark&lt;&#x2F;groupId&gt;</span><br><span class="line">    &lt;artifactId&gt;spark-sql_2.11&lt;&#x2F;artifactId&gt;</span><br><span class="line">    &lt;version&gt;$&#123;spark.version&#125;&lt;&#x2F;version&gt;</span><br><span class="line">&lt;&#x2F;dependency&gt;</span><br></pre></td></tr></table></figure>



<p>DataFrame 和 Dataset的关系在 源码中的体现</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">def toDF(): DataFrame &#x3D; new Dataset[Row](sparkSession, queryExecution, RowEncoder(schema))</span><br></pre></td></tr></table></figure>

<p>==A DataFrame is a DataSet organized into named columns==</p>
<p>DataSet API 只在 Scala 和 Java中能用, python 中用不了</p>
<p>Spark Session 是 spark DF/DS 编程的入口点<br>The entry point to programming Spark with the Dataset and DataFrame API</p>
<h3 id="二-SparkSQL-编程-POM-依赖"><a href="#二-SparkSQL-编程-POM-依赖" class="headerlink" title="二. SparkSQL 编程 POM 依赖"></a>二. SparkSQL 编程 POM 依赖</h3><p>IDEA 运行SparkSQL 需要加的依赖</p>
<ol>
<li></li>
</ol>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">&lt;dependency&gt;</span><br><span class="line">  &lt;groupId&gt;org.codehaus.janino&lt;&#x2F;groupId&gt;</span><br><span class="line">  &lt;artifactId&gt;janino&lt;&#x2F;artifactId&gt;</span><br><span class="line">  &lt;version&gt;3.0.8&lt;&#x2F;version&gt;</span><br><span class="line">&lt;&#x2F;dependency&gt;</span><br></pre></td></tr></table></figure>

<p>否则会报错:</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">Exception in thread &quot;main&quot; java.lang.NoClassDefFoundError: org&#x2F;codehaus&#x2F;janino&#x2F;InternalCompilerException</span><br></pre></td></tr></table></figure>



<ol start="2">
<li></li>
</ol>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">&lt;dependency&gt;</span><br><span class="line">  &lt;groupId&gt;org.apache.hive&lt;&#x2F;groupId&gt;</span><br><span class="line">  &lt;artifactId&gt;hive-exec&lt;&#x2F;artifactId&gt;</span><br><span class="line">  &lt;version&gt;$&#123;hive.version&#125;&lt;&#x2F;version&gt;</span><br><span class="line">&lt;&#x2F;dependency&gt;</span><br></pre></td></tr></table></figure>

<p>注意这个 hive.version 要写 ==1.2.1==, 如果选择 cdh 版本的hive 会报错</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">Exception in thread &quot;main&quot; java.lang.NoSuchFieldError: METASTORE_CLIENT_SOCKET_LIFETIME</span><br></pre></td></tr></table></figure>


<p>SparkSQL</p>
<p>show : 展示前20条数据</p>
<p>def show(): Unit = show(20)</p>
<p>如果执行中, 报 mysql 驱动找不到的 错误, 需要指定 mysql-connect 包</p>
<p>./spark-shell –jars mysql-connect-xxx.jar</p>
<p>show(3) //表示展示3条<br><img src="/2017/04/10/SparkSQL01/show1.png" alt="avatar"></p>
<p>def show(truncate: Boolean): Unit = show(20, truncate)<br>此处的truncate 表示 一个数据长度大于 20, 就把它截断, 展示的不完整<br>测试如下<br><img src="/2017/04/10/SparkSQL01/show2.png" alt="avatar"></p>
<p>由于 spark-shell 每次使用比较麻烦, spark 提供了 sql 接口<br>./spark-sql    </p>
<p>如果 报 mysql 连接不上的问题, 可按如下解决:<br>./spark-sql  <br>–jars  mysql-connect-xx.jar \   //虽然官方说 –jars 会在 driver和 executor端都加上驱动, 但是实际上driver端并没有加上, 需要通过下面参数指定<br>–driver-class-path  mysql-connect-xx.jar</p>
<p>进入 spark-sql 后, 就能像sql客户端一样使用了<br><img src="/2017/04/10/SparkSQL01/sparksql.png" alt="avatar"></p>
<h3 id="三-SparkSQL-编程规范"><a href="#三-SparkSQL-编程规范" class="headerlink" title="三. SparkSQL 编程规范"></a>三. SparkSQL 编程规范</h3><p>编程:<br>spark 入口及参数设置</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">val spark &#x3D; SparkSession.builder()</span><br><span class="line">  .master(&quot;local&quot;)</span><br><span class="line">  .appName(&quot;SparkSessionApp&quot;)</span><br><span class="line">  .getOrCreate()</span><br></pre></td></tr></table></figure>

<p>可以通过 .config 设置各种参数</p>
<p>读文本操作<br>第1中写法</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">val df: DataFrame &#x3D; spark.read.format(&quot;text&quot;).load(&quot;data&#x2F;people.txt&quot;)</span><br></pre></td></tr></table></figure>


<p>第2种写法</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">spark.read.text(&quot;data&#x2F;people.txt&quot;).show() &#x2F;&#x2F;read.text &#x3D;&#x3D; read.format(&quot;text&quot;).load(&quot;&quot;)</span><br><span class="line"></span><br><span class="line">def text(paths: String*): DataFrame &#x3D; format(&quot;text&quot;).load(paths : _*)</span><br></pre></td></tr></table></figure>


<p>第3种写法</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">val ds: Dataset[String] &#x3D; spark.read.textFile(&quot;data&#x2F;people.txt&quot;)</span><br><span class="line">    ds.show()</span><br></pre></td></tr></table></figure>

<p>可以传多个路径进去</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">def text(path: String): DataFrame &#x3D; &#123;</span><br><span class="line">  &#x2F;&#x2F; This method ensures that calls that explicit need single argument works, see SPARK-16009</span><br><span class="line">  text(Seq(path): _*)</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">spark.read.format(&quot;text&quot;)</span><br><span class="line">spark.read.textFile</span><br></pre></td></tr></table></figure>

<p>这两个的返回值是不同的，前者是 DataFrame，后面的是 DataSet，所以<br>前面是不能map的，比如加上rdd才可以，后面可以直接map</p>
<p>实例：读取 csv格式数据</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">val df &#x3D; spark.read.format(&quot;csv&quot;)</span><br><span class="line">  .option(&quot;timestampFormat&quot;, &quot;yyyy&#x2F;MM&#x2F;dd HH:mm:ss ZZ&quot;) &#x2F;&#x2F;指定 时间戳格式</span><br><span class="line">  .option(&quot;inferSchema&quot;, &quot;true&quot;)  &#x2F;&#x2F;内部推导Schema开启</span><br><span class="line">  .option(&quot;sep&quot;, &quot;,&quot;)  &#x2F;&#x2F;分隔符</span><br><span class="line">  .option(&quot;header&quot;, &quot;true&quot;) &#x2F;&#x2F; 把行首作为字段</span><br><span class="line">  .load(&quot;data&#x2F;user.csv&quot;)</span><br></pre></td></tr></table></figure>


<p>写数据, 如果路径存在报错<br><img src="/2017/04/10/SparkSQL01/error1.png" alt="avatar"></p>
<p>需要追加一个 写的模式</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">resultDS</span><br><span class="line">.write</span><br><span class="line">.mode(&quot;overwrite&quot;)</span><br><span class="line">.format(&quot;text&quot;)</span><br><span class="line">.save(&quot;out&quot;)</span><br><span class="line"></span><br><span class="line">resultDS.write.mode(SaveMode.Overwrite).format(&quot;text&quot;).save(&quot;out&quot;)</span><br></pre></td></tr></table></figure>


<p>此时执行还是报错<br><img src="/2017/04/10/SparkSQL01/error2.png" alt="avatar"></p>
<p>因为此时有两列,<br>(splits(0), splits(1))<br>改成一列, 输出正常,<br>怎么样可以输出多列?</p>
<p>压缩</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">option(&quot;compression&quot;,&quot;gzip&quot;)</span><br><span class="line">resultDS</span><br><span class="line">.write</span><br><span class="line">.option(&quot;compression&quot;,&quot;gzip&quot;).mode(&quot;overwrite&quot;)</span><br><span class="line">.format(&quot;text&quot;).save(&quot;out&quot;)</span><br></pre></td></tr></table></figure>

<p>如果是 lzo压缩会报错<br><img src="/2017/04/10/SparkSQL01/lzoerror.png" alt="avatar"></p>
<p>源码中 压缩格式如下<br><img src="/2017/04/10/SparkSQL01/compress1.png" alt="avatar"></p>
<p>setCodecConfiguration 的设置, 底层和 MR 一样的<br><img src="/2017/04/10/SparkSQL01/compression2.png" alt="avatar"></p>
<p>导入json 数据, 读取会变成如下错误</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">Exception in thread &quot;main&quot; java.lang.IllegalArgumentException: Illegal pattern component: XXX</span><br></pre></td></tr></table></figure>


<p>原因:<br>maven升级的时候，没有自动加载完整依赖包，jsonAPI对于timeStampFormat有特殊需求，默认为下面这个格式这种格式，是无法被scala-lang包识别的。我们看报错的源码可以看出。</p>
<p>解决<br>加上: option(“timestampFormat”, “yyyy/MM/dd HH:mm:ss ZZ”)</p>
<p>即</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">spark.read</span><br><span class="line">.option(&quot;timestampFormat&quot;, &quot;yyyy&#x2F;MM&#x2F;dd HH:mm:ss ZZ&quot;)</span><br><span class="line">.format(&quot;json&quot;)</span><br><span class="line">.load(&quot;data&#x2F;people.json&quot;)</span><br></pre></td></tr></table></figure>


<p>json/csv 的报错以上方法 都适用</p>
]]></content>
      <tags>
        <tag>tedata</tag>
      </tags>
  </entry>
  <entry>
    <title>Hive SQL 记录1</title>
    <url>/2017/03/25/HiveSQL1/</url>
    <content><![CDATA[<h1 id="Hive-SQL-记录"><a href="#Hive-SQL-记录" class="headerlink" title="Hive SQL 记录"></a>Hive SQL 记录</h1><h3 id="案例一"><a href="#案例一" class="headerlink" title="案例一"></a>案例一</h3><p>表中有如下字段：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">domain           time     traffic(T)</span><br><span class="line">gifshow.com   2019&#x2F;01&#x2F;01    5</span><br><span class="line">yy.com        2019&#x2F;01&#x2F;01    4</span><br><span class="line">huya.com      2019&#x2F;01&#x2F;01    1</span><br><span class="line">gifshow.com   2019&#x2F;01&#x2F;20    6</span><br><span class="line">gifshow.com   2019&#x2F;02&#x2F;01    8</span><br><span class="line">yy.com        2019&#x2F;01&#x2F;20    5</span><br><span class="line">gifshow.com   2019&#x2F;02&#x2F;02    7</span><br></pre></td></tr></table></figure>

<p>需求是按月统计每个用户的累计访问量(只能用一个 SQL)，结果如下</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">domain          month     traffics   totals</span><br><span class="line">gifshow.com     2019-01      11         11</span><br><span class="line">gifshow.com     2019-02      15         26</span><br><span class="line">yy.com          2019-01       9         9</span><br><span class="line">huya.com        2019-01       1         1</span><br></pre></td></tr></table></figure>

<p>需求分析：</p>
<p>每个用户每月的访问量：group by 用户，月；然后再 sum<br>相同每月数据累加：<br>第一步的按月和用户统计：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">select domain,substr(time,1,7) ,sum(traffic) from domain_traffic group by domain,substr(time,1,7);</span><br></pre></td></tr></table></figure>


<p>结果输出如下：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">gifshow.com     2019&#x2F;01 11</span><br><span class="line">gifshow.com     2019&#x2F;02 15</span><br><span class="line">huya.com        2019&#x2F;01 1</span><br><span class="line">yy.com  2019&#x2F;01 9</span><br></pre></td></tr></table></figure>

<p>第二步，相同用户每月累加</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">select ms.domain,ms.m,ms.s,sum(ms.s)over(partition by ms.domain order by ms.m) from </span><br><span class="line">(select domain,substr(time,1,7) m ,sum(traffic) s from domain_traffic group by domain,substr(time,1,7)) ms;</span><br><span class="line">使用 sum()over() 函数实现累加功能，over 实现分组排序，sum 实现就有点意思：将本组内当前行以及之前的行全部相加(01之前没有最终只有01，02之前是01最终是01+02)。</span><br></pre></td></tr></table></figure>


<h3 id="案例二："><a href="#案例二：" class="headerlink" title="案例二："></a>案例二：</h3><p>表数据如下：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">uid		pid</span><br><span class="line">li  	a</span><br><span class="line">zhang   b</span><br><span class="line">li  	a</span><br><span class="line">zhang   a</span><br><span class="line">wang    a</span><br><span class="line">john    a</span><br><span class="line">zhang   a</span><br><span class="line">wang    a</span><br><span class="line">tom 	b</span><br><span class="line">ao  	b</span><br><span class="line">wang    b</span><br><span class="line">tom 	b</span><br><span class="line">wang    b</span><br><span class="line">wang    b</span><br><span class="line">wang    b</span><br><span class="line">zhang   b</span><br><span class="line">zhang   b</span><br><span class="line">ao  	a</span><br></pre></td></tr></table></figure>

<ol>
<li>统计产品 UV</li>
<li>统计每个产品 top3 用户</li>
</ol>
<p>UV：按产品分组，count (uid 排重 )</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">select pid,count(distinct uid) from uid_pid group by pid;</span><br></pre></td></tr></table></figure>

<p>结果：</p>
<p>a       5<br>b       4<br>top3：</p>
<p>按产品、用户分组，count(1) 排序，limit 取 top (整体取 top)<br>select uid,pid,count(1) m from uid_pid group by uid,pid order by m desc limit 3</p>
<p>产品 top ：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">select t.uid,t.pid,t.m from (select s.uid,s.pid,s.m,row_number()over(partition by pid order by s.m desc) as rank from (select uid,pid,count(1) m from uid_pid group by uid,pid) s ) t where t.rank &lt;&#x3D; 3;</span><br></pre></td></tr></table></figure>


<p>输出</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">zhang   a       2</span><br><span class="line">wang    a       2</span><br><span class="line">li      a       2</span><br><span class="line">wang    b       4</span><br><span class="line">zhang   b       3</span><br><span class="line">tom     b       2</span><br></pre></td></tr></table></figure>

<p>使用 row_number()over 函数实现分组 Top。</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">row_number()over() 作用就是分组排序并加上序号标记：over 中按 pid 分组，并按 次数 m 降序排列，row_numbe()</span><br></pre></td></tr></table></figure>
<p> 记录排序相当于增加了一列序号 rank。</p>
<p>总结<br>以上两个案例都用到 over 这个函数，我们从 MR 角度来讲解 over 作用。</p>
<h3 id="行列转换"><a href="#行列转换" class="headerlink" title="行列转换"></a>行列转换</h3><h3 id="行转列"><a href="#行转列" class="headerlink" title="行转列"></a>行转列</h3><p>将多行数据合并成某列</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">user_id	order_id</span><br><span class="line">104399	2105395</span><br><span class="line">104399	1715131</span><br><span class="line">104400	1609001</span><br><span class="line">104400	2986088</span><br><span class="line">104400	1795054</span><br><span class="line">select user_id,concat_ws(&#39;,&#39;,collect_set(order_id)) as order_value</span><br><span class="line">from table</span><br><span class="line">group by user_id ;</span><br></pre></td></tr></table></figure>

<p>结果输出</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">user_id	order_value</span><br><span class="line">104399	2105395,1715131</span><br><span class="line">104400	1609001,2986088,1795054</span><br></pre></td></tr></table></figure>

<p>按 user_id 分组后，每个user_id 都对应多个order_id；接着 collect_set 收集多个 order_id 并去重；最后由 concat_ws 指定分隔符将数组中的 order_id 合并成一个字符串</p>
<p>collect_list 不去重，collect_set 去重，但类型要求是 string</p>
<p>将多行数据转成多列</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">name	subject	score</span><br><span class="line">张三	语文	80</span><br><span class="line">张三	数学	90</span><br><span class="line">张三	英语	60</span><br></pre></td></tr></table></figure>


<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">select name,</span><br><span class="line">	max(case when subject &#x3D;&#39;语文&#39; then score else 0 end) as Chinese,</span><br><span class="line">	max(case when subject &#x3D;&#39;数学&#39; then score else 0 end) as Math,</span><br><span class="line">	max(case when subject &#x3D;&#39;英语&#39; then score else 0 end) as English</span><br><span class="line">from table</span><br><span class="line">group by name</span><br></pre></td></tr></table></figure>

<p>name    Chinese    Math    English<br>张三    80    90    60</p>
<h3 id="列转行"><a href="#列转行" class="headerlink" title="列转行"></a>列转行</h3><p>将某列数据扩展成多行</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">user_id	order_value</span><br><span class="line">104399	2105395,1715131</span><br><span class="line">104400	1609001,2986088,1795054</span><br><span class="line">select user_id,order_id</span><br><span class="line">from table</span><br><span class="line">lateral view explode(split(order_value,&#39;,&#39;)) t as order_id;</span><br></pre></td></tr></table></figure>

<p>结果输出</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">user_id	order_id</span><br><span class="line">104399	2105395</span><br><span class="line">104399	1715131</span><br><span class="line">104400	1609001</span><br><span class="line">104400	2986088</span><br><span class="line">104400	1795054</span><br><span class="line">lateral view explode(数组)</span><br></pre></td></tr></table></figure>
<p> 将数组中每个值都扩展成一行中列值。上面例子中，由 split 将 order_value 变成一个 order 数组，然后再由 explode 扩展成列。</p>
<p>将多列数据转成多行</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">name	Chinese	Math	English</span><br><span class="line">张三	80	90	60</span><br><span class="line"> select a.name,b.label,b.value</span><br><span class="line"> from (select * from table )a</span><br><span class="line">    lateral view explode(map(</span><br><span class="line">        &#39;语文&#39;,Chinese,</span><br><span class="line">        &#39;数学&#39;,Math,</span><br><span class="line">        &#39;英语&#39;,English</span><br><span class="line">    )) b as label ,value</span><br><span class="line">name	subject	score</span><br><span class="line">张三	语文	80</span><br><span class="line">张三	数学	90</span><br><span class="line">张三	英语	60</span><br></pre></td></tr></table></figure>

<p>留存<br>用户留存率一般是面向新增用户的概念，指某一天注册后的几天还是否活跃,是以每天为单位进行计算的。一般收到的需求都是一个时间段内的新增用户的几天留存。</p>
<p>根据留存的定义可知，我们需要求两个数<br>：新增和某日留存数，两者相除可得留存率。新增数很简单一般都会有标识，留存数需要有条件限定：假设求2018-05-18日的3日留存，先获取2018-05-18的新增 user_ids，然后判断当前活跃用户的 id 是否包含在 user_ids 且 ( 当前活跃的日期 - 2018-05-18 ) = 2。</p>
<p>/<em>计算某日新增登录设备的次日、3日、7日、14日、30日、90日留存率</em>/</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">SELECT</span><br><span class="line">	log_day &#39;日期&#39;,</span><br><span class="line">	count(user_id_d0) &#39;新增数量&#39;,</span><br><span class="line">	count(user_id_d1) &#x2F; count(user_id_d0) &#39;次日留存&#39;,</span><br><span class="line">	count(user_id_d3) &#x2F; count(user_id_d0) &#39;3日留存&#39;,</span><br><span class="line">	count(user_id_d7) &#x2F; count(user_id_d0) &#39;7日留存&#39;,</span><br><span class="line">	count(user_id_d14) &#x2F; count(user_id_d0) &#39;14日留存&#39;,</span><br><span class="line">	count(user_id_d30) &#x2F; count(user_id_d0) &#39;30日留存&#39;,</span><br><span class="line">	count(user_id_d90) &#x2F; count(user_id_d0) &#39;90日留存&#39;</span><br><span class="line">FROM</span><br><span class="line">	(</span><br><span class="line">		SELECT DISTINCT</span><br><span class="line">			log_day,</span><br><span class="line">			a.user_id_d0,</span><br><span class="line">			b.device_id AS user_id_d1,</span><br><span class="line">			c.device_id AS user_id_d3,</span><br><span class="line">			d.device_id AS user_id_d7,</span><br><span class="line">			e.device_id AS user_id_d14,</span><br><span class="line">			f.device_id AS user_id_d30,</span><br><span class="line">			g.device_id AS user_id_d90</span><br><span class="line">		FROM</span><br><span class="line">			(</span><br><span class="line">				SELECT DISTINCT</span><br><span class="line">					Date(event_time) AS log_day,</span><br><span class="line">					device_id AS user_id_d0</span><br><span class="line">				FROM</span><br><span class="line">					role_login_back</span><br><span class="line">				GROUP BY</span><br><span class="line">					device_id</span><br><span class="line">				ORDER BY</span><br><span class="line">					log_day</span><br><span class="line">			) a</span><br><span class="line">		LEFT JOIN role_login_back b ON DATEDIFF(DATE(b.event_time),a.log_day) &#x3D; 1</span><br><span class="line">		AND a.user_id_d0 &#x3D; b.device_id</span><br><span class="line">		LEFT JOIN role_login_back c ON DATEDIFF(DATE(c.event_time),a.log_day) &#x3D; 2</span><br><span class="line">		AND a.user_id_d0 &#x3D; c.device_id</span><br><span class="line">		LEFT JOIN role_login_back d ON DATEDIFF(DATE(d.event_time),a.log_day) &#x3D; 6</span><br><span class="line">		AND a.user_id_d0 &#x3D; d.device_id</span><br><span class="line">		LEFT JOIN role_login_back e ON DATEDIFF(DATE(e.event_time),a.log_day) &#x3D; 13</span><br><span class="line">		AND a.user_id_d0 &#x3D; e.device_id</span><br><span class="line">		LEFT JOIN role_login_back f ON DATEDIFF(DATE(f.event_time),a.log_day) &#x3D; 29</span><br><span class="line">		AND a.user_id_d0 &#x3D; f.device_id</span><br><span class="line">		LEFT JOIN role_login_back g ON DATEDIFF(DATE(g.event_time),a.log_day) &#x3D; 89</span><br><span class="line">		AND a.user_id_d0 &#x3D; g.device_id</span><br><span class="line">	) AS temp</span><br><span class="line">GROUP BY</span><br><span class="line">log_day</span><br></pre></td></tr></table></figure>
]]></content>
      <tags>
        <tag>tedata</tag>
      </tags>
  </entry>
  <entry>
    <title>Hadoop 支持 lzo压缩算法</title>
    <url>/2017/03/12/hadooplzo/</url>
    <content><![CDATA[<h1 id="Hadoop-支持-lzo压缩算法"><a href="#Hadoop-支持-lzo压缩算法" class="headerlink" title="Hadoop 支持 lzo压缩算法"></a>Hadoop 支持 lzo压缩算法</h1><p>学习目标：</p>
<ol>
<li>安装lzo相关依赖</li>
<li>编译lzo</li>
<li>编译Hadoop-lzo</li>
<li>修改hadoop配置</li>
<li>准备数据</li>
<li>wordcount</li>
<li>文件添加index</li>
<li>安装lzo相关依赖</li>
</ol>
<h3 id="一-lzo-概念和优点"><a href="#一-lzo-概念和优点" class="headerlink" title="一. lzo 概念和优点"></a>一. lzo 概念和优点</h3><p>Hadoop经常用于处理大量的数据，如果期间的输出数据、中间数据能压缩存储，对系统的I/O性能会有提升。综合考虑压缩、解压速度、是否支持split，目前lzo是最好的选择。LZO（LZO是Lempel-Ziv-Oberhumer的缩写）是一种高压缩比和解压速度极快的编码，它的特点是解压缩速度非常快，无损压缩，压缩后的数据能准确还原，lzo是基于block分块的，允许数据被分解成chunk，能够被并行的解压。LZO库实现了许多有下述特点的算法：</p>
<p>　　（1）、解压简单，速度非常快。</p>
<p>　　（2）、解压不需要内存。</p>
<p>　　（3）、压缩相当地快。</p>
<p>　　（4）、压缩需要64 kB的内存。</p>
<p>　　（5）、允许在压缩部分以损失压缩速度为代价提高压缩率，解压速度不会降低。</p>
<p>　　（6）、包括生成预先压缩数据的压缩级别，这样可以得到相当有竞争力的压缩比。</p>
<p>　　（7）、另外还有一个只需要8 kB内存的压缩级别。</p>
<p>　　（8）、算法是线程安全的。</p>
<p>　　（9）、算法是无损的。</p>
<h3 id="二-安装lzo相关依赖"><a href="#二-安装lzo相关依赖" class="headerlink" title="二. 安装lzo相关依赖"></a>二. 安装lzo相关依赖</h3><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[root@JD ~]# yum install -y svn ncurses-devel</span><br><span class="line">[root@JD ~]# yum install -y gcc gcc-c++ make cmake</span><br><span class="line">[root@JD ~]# yum install -y openssl openssl-devel svn ncurses-devel zlib-devel libtool </span><br><span class="line">[root@JD ~]# yum install -y lzo lzo-devel lzop autoconf automake cmake</span><br></pre></td></tr></table></figure>


<h3 id="三-编译lzo"><a href="#三-编译lzo" class="headerlink" title="三. 编译lzo"></a>三. 编译lzo</h3><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[hadoop@JD ~]$ wget http:&#x2F;&#x2F;www.oberhumer.com&#x2F;opensource&#x2F;lzo&#x2F;download&#x2F;lzo-2.06.tar.gz</span><br><span class="line">[hadoop@JD ~]$ tar -zxvf lzo-2.06.tar.gz</span><br><span class="line">[hadoop@JD ~]$ cd lzo-2.06</span><br><span class="line">[hadoop@JD ~]$ export CFLAGS&#x3D;-m64</span><br><span class="line">[hadoop@JD ~]$ mkdir compile</span><br><span class="line">[hadoop@JD ~]$ .&#x2F;configure -enable-shared -prefix&#x3D;&#x2F;home&#x2F;hadoop&#x2F;app&#x2F;lzo-2.06&#x2F;compile</span><br><span class="line">[hadoop@JD ~]$ make &amp;&amp;  make install</span><br></pre></td></tr></table></figure>


<h3 id="三-编译Hadoop-lzo"><a href="#三-编译Hadoop-lzo" class="headerlink" title="三 编译Hadoop-lzo"></a>三 编译Hadoop-lzo</h3><p>下载源码</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">wget https:&#x2F;&#x2F;github.com&#x2F;twitter&#x2F;hadoop-lzo&#x2F;archive&#x2F;master.zip</span><br><span class="line">解压</span><br><span class="line"></span><br><span class="line">[hadoop@JD software]$ unzip -d ~&#x2F;app&#x2F; hadoop-lzo-master.zip</span><br></pre></td></tr></table></figure>


<p>进入解压后的目录</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[hadoop@JD app]$ cd hadoop-lzo-master&#x2F;</span><br><span class="line">[hadoop@JD hadoop-lzo-master]$</span><br></pre></td></tr></table></figure>


<p>修改此目录下pom.xml文件</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">&lt;repositories&gt;</span><br><span class="line">	#添加cloudera仓库</span><br><span class="line">    &lt;repository&gt;</span><br><span class="line">     &lt;id&gt;cloudera&lt;&#x2F;id&gt;</span><br><span class="line">     &lt;url&gt;https:&#x2F;&#x2F;repository.cloudera.com&#x2F;artifactory&#x2F;cloudera-repos&lt;&#x2F;url&gt;</span><br><span class="line">    &lt;&#x2F;repository&gt;</span><br><span class="line">  &lt;&#x2F;repositories&gt;</span><br><span class="line">&lt;properties&gt;</span><br><span class="line">    &lt;project.build.sourceEncoding&gt;UTF-8&lt;&#x2F;project.build.sourceEncoding&gt;</span><br><span class="line">    #因为用的是cdh的</span><br><span class="line">    &lt;hadoop.current.version&gt;2.6.0-cdh5.15.1&lt;&#x2F;hadoop.current.version&gt;</span><br><span class="line">    &lt;hadoop.old.version&gt;1.0.4&lt;&#x2F;hadoop.old.version&gt;</span><br><span class="line">&lt;&#x2F;properties&gt;</span><br></pre></td></tr></table></figure>


<p>添加环境变量</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[hadoop@JD hadoop-lzo-master]$ export CFLAGS&#x3D;-m64</span><br><span class="line">[hadoop@JD hadoop-lzo-master]$ export CXXFLAGS&#x3D;-m64</span><br></pre></td></tr></table></figure>



<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">#Modify the actual path for your hadoop</span><br><span class="line">[hadoop@JD hadoop-lzo-master]$ export C_INCLUDE_PATH&#x3D;&#x2F;home&#x2F;hadoop&#x2F;app&#x2F;lzo-2.06&#x2F;compile&#x2F;include</span><br><span class="line">[hadoop@JD hadoop-lzo-master]$ export LIBRARY_PATH&#x3D;&#x2F;home&#x2F;hadoop&#x2F;app&#x2F;lzo-2.06&#x2F;compile&#x2F;lib</span><br></pre></td></tr></table></figure>


<p>编译源码：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">mvn clean package -DskipTests</span><br></pre></td></tr></table></figure>
<p><img src="/2017/03/12/hadooplzo/hl1.png" alt="avatar"></p>
<p>进入target/native/Linux-amd64-64</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[hadoop@JD hadoop-lzo-master]$ cd target&#x2F;native&#x2F;Linux-amd64-64&#x2F;</span><br><span class="line">[hadoop@JD Linux-amd64-64]$ mkdir ~&#x2F;app&#x2F;hadoop-lzo-files</span><br><span class="line">[hadoop@JD Linux-amd64-64]$ tar -cBf - -C lib . | tar -xBvf - -C ~&#x2F;app&#x2F;hadoop-lzo-files</span><br></pre></td></tr></table></figure>

<p>拷贝文件</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[hadoop@hadoop-01 hadoop-lzo-files]$ cp ~&#x2F;app&#x2F;hadoop-lzo-files&#x2F;libgplcompression* $HADOOP_HOME&#x2F;lib&#x2F;native&#x2F;</span><br></pre></td></tr></table></figure>


<h3 id="四-修改hadoop配置"><a href="#四-修改hadoop配置" class="headerlink" title="四 修改hadoop配置"></a>四 修改hadoop配置</h3><p>vi core-site.xml</p>
<p>修改core-site.xml的配置文件</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">    &lt;property&gt;</span><br><span class="line">        &lt;name&gt;io.compression.codecs&lt;&#x2F;name&gt;</span><br><span class="line">        &lt;value&gt;org.apache.hadoop.io.compress.GzipCodec,</span><br><span class="line">               org.apache.hadoop.io.compress.DefaultCodec,</span><br><span class="line">               org.apache.hadoop.io.compress.BZip2Codec,</span><br><span class="line">               org.apache.hadoop.io.compress.SnappyCodec,</span><br><span class="line">               com.hadoop.compression.lzo.LzoCodec,</span><br><span class="line">               com.hadoop.compression.lzo.LzopCodec</span><br><span class="line">        &lt;&#x2F;value&gt;</span><br><span class="line">    &lt;&#x2F;property&gt;</span><br><span class="line"></span><br><span class="line">  </span><br><span class="line"></span><br><span class="line">&lt;property&gt;</span><br><span class="line">      &lt;name&gt;io.compression.codec.lzo.class&lt;&#x2F;name&gt;</span><br><span class="line">      &lt;value&gt;com.hadoop.compression.lzo.LzoCodec&lt;&#x2F;value&gt;</span><br><span class="line">  &lt;&#x2F;property&gt;</span><br></pre></td></tr></table></figure>

<p>修改mapred-site.xml配置文件</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">&lt;property&gt;</span><br><span class="line">  &lt;name&gt;mapred.compress.map.output&lt;&#x2F;name&gt;</span><br><span class="line">  &lt;value&gt;true&lt;&#x2F;value&gt;</span><br><span class="line">&lt;&#x2F;property&gt;</span><br><span class="line"> </span><br><span class="line">&lt;property&gt;</span><br><span class="line">    &lt;name&gt;mapred.map.output.compression.codec&lt;&#x2F;name&gt;</span><br><span class="line">    &lt;value&gt;com.hadoop.compression.lzo.LzoCodec&lt;&#x2F;value&gt;</span><br><span class="line">&lt;&#x2F;property&gt;</span><br><span class="line"> </span><br><span class="line">&lt;property&gt;</span><br><span class="line">    &lt;name&gt;mapred.child.env&lt;&#x2F;name&gt;</span><br><span class="line">    &lt;value&gt;LD_LIBRARY_PATH&#x3D;&#x2F;usr&#x2F;local&#x2F;hadoop&#x2F;lzo&#x2F;lib&lt;&#x2F;value&gt;</span><br><span class="line">&lt;&#x2F;property&gt;</span><br></pre></td></tr></table></figure>

<h3 id="五-准备数据"><a href="#五-准备数据" class="headerlink" title="五 准备数据"></a>五 准备数据</h3><p>准备一个753M的数据<br><img src="/2017/03/12/hadooplzo/hl2.png" alt="avatar"></p>
<p>然后压缩此文件<br><img src="/2017/03/12/hadooplzo/hl3.png" alt="avatar"></p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">lzop -f access.txt</span><br></pre></td></tr></table></figure>



<h3 id="六-wordcount"><a href="#六-wordcount" class="headerlink" title="六 wordcount"></a>六 wordcount</h3><p>首先把数据上传到hdfs</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">hdfs dfs -put access.txt.lzo &#x2F;lzo-data&#x2F;input</span><br></pre></td></tr></table></figure>


<p>计算wordcount</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">hadoop jar \</span><br><span class="line">&#x2F;home&#x2F;hadoop&#x2F;app&#x2F;hadoop&#x2F;share&#x2F;hadoop&#x2F;mapreduce&#x2F;hadoop-mapreduce-examples-2.6.0-cdh5.15.1.jar \</span><br><span class="line">wordcount \</span><br><span class="line">-Dmapreduce.job.inputformat.class&#x3D;com.hadoop.mapreduce.LzoTextInputFormat \</span><br><span class="line">&#x2F;lzo-data&#x2F;input&#x2F;access.txt.lzo \</span><br><span class="line">&#x2F;lzo-data&#x2F;output3</span><br></pre></td></tr></table></figure>

<p>从下图可以看出，没有分片<br><img src="/2017/03/12/hadooplzo/hl5.png" alt="avatar"></p>
<h3 id="七-文件添加index"><a href="#七-文件添加index" class="headerlink" title="七 文件添加index"></a>七 文件添加index</h3><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">hadoop jar \</span><br><span class="line">&#x2F;home&#x2F;hadoop&#x2F;app&#x2F;hadoop-lzo-master&#x2F;target&#x2F;hadoop-lzo-0.4.21-SNAPSHOT.jar \</span><br><span class="line">com.hadoop.compression.lzo.DistributedLzoIndexer  \</span><br><span class="line">&#x2F;lzo-data&#x2F;input&#x2F;access.txt.lzo</span><br></pre></td></tr></table></figure>



<p>再次计算wordcount</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">hadoop jar \</span><br><span class="line">&#x2F;home&#x2F;hadoop&#x2F;app&#x2F;hadoop&#x2F;share&#x2F;hadoop&#x2F;mapreduce&#x2F;hadoop-mapreduce-examples-2.6.0-cdh5.15.1.jar \</span><br><span class="line">wordcount \</span><br><span class="line">-Dmapreduce.job.inputformat.class&#x3D;com.hadoop.mapreduce.LzoTextInputFormat \</span><br><span class="line">&#x2F;lzo-data&#x2F;input&#x2F;access.txt.lzo \</span><br><span class="line">&#x2F;lzo-data&#x2F;output4</span><br></pre></td></tr></table></figure>


<p>从下图，我们可以看出分成3个<br><img src="/2017/03/12/hadooplzo/hl7.png" alt="avatar"></p>
<p>参考文章：</p>
<p><a href="https://www.iteblog.com/archives/992.html" target="_blank" rel="noopener">https://www.iteblog.com/archives/992.html</a></p>
]]></content>
      <tags>
        <tag>tedata</tag>
      </tags>
  </entry>
  <entry>
    <title>SparkCore 03</title>
    <url>/2017/03/10/SparkCore03/</url>
    <content><![CDATA[<h1 id="Spark-Core-03"><a href="#Spark-Core-03" class="headerlink" title="Spark Core 03"></a>Spark Core 03</h1><p>学习目标：</p>
<ol>
<li>RDD 的依赖</li>
<li>Persist/Cache</li>
<li>repartition/coalesce</li>
</ol>
<p>一.  RDD 的依赖</p>
<ol>
<li>先看一个例子</li>
</ol>
<p>执行一个 wc</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">def main(args: Array[String]): Unit &#x3D; &#123;</span><br><span class="line">  val sc &#x3D; ContextUtils.getContext(this.getClass.getSimpleName)</span><br><span class="line"></span><br><span class="line">  val data &#x3D; Array(&quot;hadoop hbase scala&quot;, &quot;hadoop hive scala&quot;, &quot;hadoop spark hive&quot;)</span><br><span class="line">  val input &#x3D; sc.parallelize(data)</span><br><span class="line">  input.flatMap(_.split(&quot; &quot;)).map((_,1)).reduceByKey(_+_).foreach(println)</span><br><span class="line"></span><br><span class="line">  sc.stop()</span><br><span class="line"></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>看下 spark ui 的DAG 图<br>![avatar](/Spark Core03/stageDAG.png)</p>
<p>可以看到，此时产生了两个 Stage：Stage0，Stage1</p>
<p>stage0 产生的是 reduceByKey 之前的DAG<br>![avatar](/Spark Core03/stage0.png)</p>
<p>stage1 产生的是<br>![avatar](/Spark Core03/stage1.png)</p>
<p>可以看到，在reduceByKey 处产生了 ShuffleRDD，stage 一分为二</p>
<ol start="2">
<li><p>什么是ShuffleRDD？<br>所谓 shuffle，指的是相同key的 元素聚合到一起的过程，这个过程因为可能存在跨主机，跨机架，所以是一个开销非常大的操作</p>
</li>
<li><p>图解 RDD 的依赖<br>先根据上面的例子，画一张图<br>![avatar](/Spark Core03/wcDep.png)</p>
</li>
</ol>
<p>看上图，从 flatMap –》Map，Map –》 Combiner，每个父RDD仅被子RDD 使用一次<br>这种依赖称为窄依赖</p>
<p>ReduceByKey中，父RDD 被 子RDD 使用多次，这种依赖称为 宽依赖</p>
<p>一般：map，fliter，union 这些操作都是窄依赖<br>    reduceByKey，groupByKey，countByKey这些操作都是宽依赖</p>
<p>二.  Persist/ Cache<br>persist/cache 指的是把数据缓存起来，下次调用的时候可以直接使用，而不用去再计算生成</p>
<p>persist 有四个参数</p>
<p>UseDisk：是否使用磁盘</p>
<p>UseMemory：是否使用内存</p>
<p>UseOffHeap：是否使用对外内存</p>
<p>Deserialization：是否不使用序列化</p>
<p>比如：<br>MEMORY_ONLY 的设置就是：(false, true, false, true)， 即只使用内存<br>MEMORY_AND_SER 就是：(fasle, true, false, false) 即使用内存并且序列化</p>
<p>persist/cache 的区别？<br>cache 是 persist 的特列，即是 MEMORY_ONLY 这种情况</p>
<p>怎么选择persist？</p>
<ol>
<li>优先选择 MEMORY_ONLY, 这也是spark的默认设置</li>
<li>如果1 不能满足，就选择 MEMORY_AND_SER，但要注意，序列化会增加 CPU的开销</li>
<li>不要选择 disk的方式，这种方式还不如重新算一遍</li>
</ol>
<p>三. repartition/coalease</p>
<p>repartition: 重新分区</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">scala&gt; inputre.mapPartitionsWithIndex((index, partition) &#x3D;&gt; &#123;</span><br><span class="line">     |       partition.map( x &#x3D;&gt; &#123;</span><br><span class="line">     |         println(s&quot;$index, $x&quot;)</span><br><span class="line">     |       &#125;)</span><br><span class="line">     |     &#125;).collect</span><br><span class="line">0, hadoop spark hive</span><br><span class="line">1, hadoop hbase scala</span><br><span class="line">2, hadoop hive scala</span><br><span class="line">res7: Array[Unit] &#x3D; Array((), (), ())</span><br></pre></td></tr></table></figure>



<p>colaease：减小分区</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">scala&gt; inputce.partition</span><br><span class="line">partitioner   partitions</span><br><span class="line"></span><br><span class="line">scala&gt; inputce.partitions.size</span><br><span class="line">res10: Int &#x3D; 2</span><br></pre></td></tr></table></figure>


<p>默认情况下，coalesce 是不能增大分区的，除非在引用方法时在分区数后面加上 true</p>
<p>不加 true，partition 最多增大到本来的分区数</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">scala&gt; val inputce &#x3D; inputre.coalesce(4)</span><br><span class="line">inputce: org.apache.spark.rdd.RDD[String] &#x3D; CoalescedRDD[15] at coalesce at &lt;console&gt;:25</span><br><span class="line"></span><br><span class="line">scala&gt; inputce.partitions.size</span><br><span class="line">res11: Int &#x3D; 3</span><br></pre></td></tr></table></figure>

<p>加上true，分区数可以增加到指定数目</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">scala&gt; val inputce &#x3D; inputre.coalesce(4, true)</span><br><span class="line">inputce: org.apache.spark.rdd.RDD[String] &#x3D; MapPartitionsRDD[20] at coalesce at &lt;console&gt;:25</span><br><span class="line"></span><br><span class="line">scala&gt; inputce.partitions.size</span><br><span class="line">res13: Int &#x3D; 4</span><br></pre></td></tr></table></figure>

]]></content>
      <tags>
        <tag>tedata</tag>
      </tags>
  </entry>
  <entry>
    <title>Spark Core 第二课</title>
    <url>/2017/03/07/SparkCore%2002/</url>
    <content><![CDATA[<p>Spark Core02</p>
<p>学习目标：</p>
<ol>
<li>RDD 的 Action 算子</li>
<li>关于RDD 的一些操作</li>
<li>排序的实现</li>
<li>Spark 关键术语</li>
</ol>
<p>一. RDD 的Action 算子</p>
<ol>
<li><p>foreach</p>
</li>
<li><p>foreachPartition</p>
</li>
<li><p>countByKey</p>
</li>
</ol>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">scala&gt; rdd1.map((_,1)).countByKey</span><br><span class="line">res10: scala.collection.Map[Int,Long] &#x3D; Map(5 -&gt; 1, 1 -&gt; 1, 6 -&gt; 1, 2 -&gt; 1, 3 -&gt; 1, 4 -&gt; 1)</span><br></pre></td></tr></table></figure>


<ol start="4">
<li>collectAsMap</li>
</ol>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">scala&gt; rdd1.map((_,1)).collectAsMap</span><br><span class="line">res14: scala.collection.Map[Int,Int] &#x3D; Map(2 -&gt; 1, 5 -&gt; 1, 4 -&gt; 1, 1 -&gt; 1, 3 -&gt; 1, 6 -&gt; 1)</span><br></pre></td></tr></table></figure>



<p>二. 关于RDD 的一些操作</p>
<ol>
<li>rdd.count</li>
<li>rdd.partitions.size</li>
<li>rdd.first</li>
<li>rdd.top(n)</li>
<li>rdd.takeOrder(n)</li>
</ol>
<p>三.  排序实现</p>
<ol>
<li>算子实现</li>
</ol>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">def main(args: Array[String]): Unit &#x3D; &#123;</span><br><span class="line"></span><br><span class="line">    val sc &#x3D; ContextUtils.getContext(this.getClass.getSimpleName)</span><br><span class="line"></span><br><span class="line">    val rdd1 &#x3D; sc.parallelize(List(&quot;iphone11 7000 20&quot;,&quot;hwpro30 5000 100&quot;,&quot;xiaomi 3000 200&quot;,&quot;sumsung 6000 1000&quot;))</span><br><span class="line"></span><br><span class="line">rdd1.map( x &#x3D;&gt; &#123;</span><br><span class="line">val splits &#x3D; x.split(&quot; &quot;)</span><br><span class="line">  (splits(0), splits(1).trim.toInt, splits(2).trim.toInt)</span><br><span class="line">&#125;).sortBy(_._2).foreach(println)</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>


<ol start="2">
<li>继承 Ordered 类实现</li>
</ol>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">  rdd1.map( x &#x3D;&gt; &#123;</span><br><span class="line">      val splits &#x3D; x.split(&quot; &quot;)</span><br><span class="line">      val product &#x3D; splits(0)</span><br><span class="line">      val price &#x3D; splits(1).toInt</span><br><span class="line">      val amount &#x3D; splits(2).toInt</span><br><span class="line">      MyProduct(product, price, amount)</span><br><span class="line">    &#125;).sortBy(x &#x3D;&gt; x ).foreach(println)</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">case class MyProduct(product: String, price: Int, amount: Int) extends Ordered[MyProduct] &#123;</span><br><span class="line">  override def compare(that: MyProduct): Int &#x3D; &#123;</span><br><span class="line">    -(this.amount - that.amount)</span><br><span class="line">  &#125;</span><br></pre></td></tr></table></figure>


<ol start="3">
<li>隐式转换</li>
</ol>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">implicit def myproduct2orderproduct(myproduct: MyProduct2) &#x3D; new Ordered[MyProduct2] &#123;</span><br><span class="line">      override def compare(that: MyProduct2): Int &#x3D; &#123;</span><br><span class="line">        myproduct.amount - that.amount</span><br><span class="line">      &#125;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    rdd1.map( x &#x3D;&gt; &#123;</span><br><span class="line">      val splits &#x3D; x.split(&quot; &quot;)</span><br><span class="line">      MyProduct2(splits(0).trim, splits(1).trim.toInt, splits(2).trim.toInt)</span><br><span class="line">    &#125;).sortBy(y &#x3D;&gt; y)</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">case class MyProduct2(product: String, price: Int, amount: Int)</span><br></pre></td></tr></table></figure>


<p>四. 关键术语</p>
<h5 id="Application"><a href="#Application" class="headerlink" title="Application:"></a>Application:</h5><p>构建在spark上的应用程序, 包含一个 driver + 多个 executor</p>
<h5 id="Application-jar"><a href="#Application-jar" class="headerlink" title="Application jar:"></a>Application jar:</h5><p>包含 user的Spark Application, . User Jar 不允许包含 Hadoop/Spark的 lib包,然而, 他们可以在运行时候添加进去</p>
<h5 id="Driver-Program"><a href="#Driver-Program" class="headerlink" title="Driver Program:"></a>Driver Program:</h5><p>main 方法, 包含一个 sc, 一个应用程序里面有driver</p>
<h5 id="Cluster-Manager"><a href="#Cluster-Manager" class="headerlink" title="Cluster Manager:"></a>Cluster Manager:</h5><h6 id="Deploy-Mode"><a href="#Deploy-Mode" class="headerlink" title="Deploy Mode:"></a>Deploy Mode:</h6><p>区分driver模式跑在哪里</p>
<p>YARN: RM NM(container)</p>
<p>cluster: Driver 跑在container</p>
<p>client: Driver就运行在你提交的机器的本地</p>
<p>Executor:</p>
<p>process</p>
<p>run tasks</p>
<p>keep data in memory or disk storage across them Each application has its own executors 对应于YARN 上的 container </p>
<h5 id="Task"><a href="#Task" class="headerlink" title="Task"></a>Task</h5><p>发送到 executor上 运行  基本单元</p>
<h5 id="RDD"><a href="#RDD" class="headerlink" title="RDD:"></a>RDD:</h5><p>partitions , 每一个 partition 对应一个 Task</p>
<h5 id="Job"><a href="#Job" class="headerlink" title="Job:"></a>Job:</h5><p>只要遇到 action, 就产生 job</p>
<h5 id="Stage"><a href="#Stage" class="headerlink" title="Stage:"></a>Stage:</h5><p>一组 tasks的集合</p>
<ul>
<li>一个 application: 1到n个 job</li>
<li>一个 Job: 1到 n 个 stage</li>
<li>一个 Stage: 1到n个 task, task与 partition 一一对应 </li>
</ul>
<p>Executor<br>是一个较大的概念<br>对应于YARN 是 Container<br>对应 Master-Slave 就是 Worker Node</p>
<p>Spark application run as independent sets of process on cluster: 指的就是 executor</p>
]]></content>
      <tags>
        <tag>tedata</tag>
      </tags>
  </entry>
  <entry>
    <title>Spark Core 第一课</title>
    <url>/2017/03/05/SparkCore%2001/</url>
    <content><![CDATA[<h1 id="Spark-Core01"><a href="#Spark-Core01" class="headerlink" title="Spark Core01"></a>Spark Core01</h1><p>学习目标：</p>
<ol>
<li>RDD 5大特性</li>
<li>SparkCore 的编程规范</li>
<li>RDD 创建方式</li>
<li>RDD 的算子</li>
</ol>
<p>一. RDD 的5 大特性</p>
<ul>
<li><ol>
<li>RDD 是 一系列的 partition ： A list of partition<br>比如一个数据 val data = Array(1,2,3,4,5,6,7,8,9)<br>会把数据分成一个一个的partition，比如<br>(1,2,3) , (4,5,6), (7,8,9)</li>
</ol>
</li>
<li><ol start="2">
<li>函数(方法) 是作用在每一个 partition 上的: A function for computing each split<br>比如 data.map(_ * 2)  表示 对 每个分区的 元素 * 2</li>
</ol>
</li>
<li><ol start="3">
<li>RDD 之间存在依赖关系：A list of dependency on other RDDS<br>这种依赖指的是 某个RDD 是由另外的RDD 计算推导出来的<br>比如 data.map(_ * 10 )  ==&gt;  data2<br>那么 data2中的 rdd就是由 data 每个元素 * 10 得到的</li>
</ol>
</li>
<li><ol start="4">
<li>对于K,V 结构的RDD，可以进行 partitioner，即把具有相同特征的RDD分到一组</li>
</ol>
</li>
<li><ol start="5">
<li>把计算拉到数据节点</li>
</ol>
</li>
</ul>
<p>二. SparKCore 的编程规范</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">val conf &#x3D; new SparkConf()</span><br><span class="line">  .setMaster(&quot;local[2]&quot;)  传入 master 的类型，一般测试传入 local[2] 即可 </span><br><span class="line">  .setAppName(this.getClass.getSimpleName)  app的名称</span><br><span class="line"></span><br><span class="line">val sc &#x3D; new SparkContext(conf)</span><br></pre></td></tr></table></figure>


<p>如果不传入这两个会报错误，在源码中的体现：<br><img src="/2017/03/05/SparkCore%2001/HEXO/tedatablog/source/pictures/sparkcore01.png" alt="avatar"></p>
<p>三.  RDD 的创建方式</p>
<p>rdd创建方式一:  通过生成数据转换创建</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">scala&gt; val rdd1 &#x3D; sc.parallelize(List(1,2,3,4,5))</span><br><span class="line">rdd1: org.apache.spark.rdd.RDD[Int] &#x3D; ParallelCollectionRDD[6] at parallelize at &lt;console&gt;:24</span><br><span class="line"></span><br><span class="line">scala&gt; rdd1.map(_*2).collect</span><br><span class="line">res4: Array[Int] &#x3D; Array(2, 4, 6, 8, 10)</span><br></pre></td></tr></table></figure>


<p>rdd创建方式二：通过读取文件创建</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">scala&gt; val rdd2 &#x3D; sc.textFile(&quot;file:&#x2F;&#x2F;&#x2F;home&#x2F;bdp&#x2F;tmp&#x2F;wc.data&quot;)</span><br><span class="line">rdd2: org.apache.spark.rdd.RDD[String] &#x3D; file:&#x2F;&#x2F;&#x2F;home&#x2F;bdp&#x2F;tmp&#x2F;wc.data MapPartitionsRDD[5] at textFile at &lt;console&gt;:24</span><br><span class="line"></span><br><span class="line">scala&gt; rdd2.collect</span><br><span class="line">res3: Array[String] &#x3D; Array(boming,boming,boming, niuniu,niuniu, simao)</span><br></pre></td></tr></table></figure>


<p>rdd 创建方式三:<br>通过其他的rdd创建</p>
<p>四.  RDD 算子</p>
<p>RDD 算子可以分为两种类型：transformation 和 action<br>transformation 指的是中间的转换过程，是没有真正执行的<br>action 是真正执行的</p>
<p>transformation 算子</p>
<p>一定要记得，现在的操作都是对RDD 进行的操作，一定要转换成RDD</p>
<ol>
<li>map：作用在rdd 的每个元素上做相同的操作</li>
</ol>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">val rdd &#x3D; sc.parallelize(Array(1,2,3,4,5,6))</span><br><span class="line">scala&gt; rdd.map(x &#x3D;&gt; &#123;x*2&#125;)</span><br><span class="line">res6: Array[Int] &#x3D; Array(2, 4, 6, 8, 10, 12)</span><br></pre></td></tr></table></figure>




<ol start="2">
<li>filter： 过滤出符合条件的元素</li>
</ol>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">scala&gt; rdd.map(x &#x3D;&gt; &#123;x*2&#125;).filter(_ &gt; 6)</span><br><span class="line">res8: Array[Int] &#x3D; Array(8, 10, 12)</span><br><span class="line"></span><br><span class="line">scala&gt; rdd.map(x &#x3D;&gt; &#123;x*2&#125;).filter(x &#x3D;&gt; &#123;x &gt; 6 &#125;)</span><br><span class="line">res9: Array[Int] &#x3D; Array(8, 10, 12)</span><br></pre></td></tr></table></figure>


<ol start="3">
<li>mapPartitions</li>
</ol>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">scala&gt; rdd.mapPartitions( par &#x3D;&gt; &#123;par.map(_*2)&#125;).collect</span><br><span class="line">res20: Array[Int] &#x3D; Array(2, 4, 6, 8, 10, 12)</span><br></pre></td></tr></table></figure>

<p>这个 算子的作用, 比如 有 100个元素, 10个分区, 使用 map 要调用 100次, 使用 mapPartitions 只需10次<br>可以引申到 MySQL的connection, 使用 mapPartitions可以减少很多的连接</p>
<ol start="4">
<li>flatMap : map 之后把 元素拍扁</li>
</ol>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">scala&gt;  val rdd2 &#x3D; sc.parallelize(Array(Array(1,2,3),Array(4,5,6),Array(7,8,9)))</span><br><span class="line">scala&gt; rdd2.flatMap(x &#x3D;&gt; x).collect</span><br><span class="line">res26: Array[Int] &#x3D; Array(1, 2, 3, 4, 5, 6, 7, 8, 9)</span><br></pre></td></tr></table></figure>




<ol start="5">
<li>mapPartitionWIthIndex</li>
</ol>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">rdd1.mapPartitionsWithIndex((index, partition) &#x3D;&gt; &#123;</span><br><span class="line">  partition.map( x &#x3D;&gt; &#123;</span><br><span class="line">    println(s&quot;$&#123;index&#125;, $x&quot;)</span><br><span class="line">  &#125;)</span><br><span class="line">&#125;)</span><br></pre></td></tr></table></figure>

<p>把 partition 和 index 输出</p>
<ol start="6">
<li>mapValues<br>只对KV 结构的V做相同的操作</li>
</ol>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">scala&gt; rdd1.map((_,1)).mapValues(_*3).collect</span><br><span class="line">res4: Array[(String, Int)] &#x3D; Array((iphone11 7000 20,3), (hwpro30 5000 100,3), (xiaomi 3000 200,3), (sumsung 6000 1000,3))</span><br></pre></td></tr></table></figure>


<ol start="7">
<li>reduceByKey</li>
</ol>
<p>把相同的key的元素放在一块，再进行两两相邻的操作</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">scala&gt; val rdd2 &#x3D; sc.parallelize(List(1,2,1,2,3,6,3,1,5))</span><br><span class="line">rdd2: org.apache.spark.rdd.RDD[Int] &#x3D; ParallelCollectionRDD[11] at parallelize at &lt;console&gt;:24</span><br><span class="line"></span><br><span class="line">scala&gt; rdd2.map((_,1)).reduceByKey(_+_).collect</span><br><span class="line">res9: Array[(Int, Int)] &#x3D; Array((1,3), (6,1), (3,2), (5,1), (2,2))</span><br></pre></td></tr></table></figure>



<ol start="8">
<li>union<br>把相同类型的数据放在一起</li>
</ol>
<p>注意，rdd1 和 rdd2 如果类型不同，不能进行 union<br>比如 rdd1为 String，rdd2 为 Int，会报下面的错误</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">scala&gt; rdd1.union(rdd2).collect</span><br><span class="line">&lt;console&gt;:28: error: type mismatch;</span><br><span class="line"> found   : org.apache.spark.rdd.RDD[Int]</span><br><span class="line"> required: org.apache.spark.rdd.RDD[String]</span><br><span class="line">       rdd1.union(rdd2).collect</span><br><span class="line"></span><br><span class="line">scala&gt; val rdd2 &#x3D; sc.parallelize(Array(&quot;ok haixing&quot;))</span><br><span class="line">rdd2: org.apache.spark.rdd.RDD[String] &#x3D; ParallelCollectionRDD[21] at parallelize at &lt;console&gt;:24</span><br><span class="line"></span><br><span class="line">scala&gt; rdd1.union(rdd2).collect</span><br><span class="line">res16: Array[String] &#x3D; Array(iphone11 7000 20, hwpro30 5000 100, xiaomi 3000 200, sumsung 6000 1000, ok haixing)</span><br></pre></td></tr></table></figure>


<ol start="9">
<li>distinct</li>
</ol>
<p>去除 rdd的重复的元素</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">scala&gt; val rdd2 &#x3D; sc.parallelize(List(1,2,1,2,3,6,3,1,5))</span><br><span class="line">rdd2: org.apache.spark.rdd.RDD[Int] &#x3D; ParallelCollectionRDD[23] at parallelize at &lt;console&gt;:24</span><br><span class="line"></span><br><span class="line">scala&gt; rdd2.distinct.collect</span><br><span class="line">res19: Array[Int] &#x3D; Array(1, 6, 3, 5, 2)</span><br></pre></td></tr></table></figure>


<ol start="10">
<li>groupByKey<br>把相同key的元素放到一组，得到的是 CompactBuffer 类型的value</li>
</ol>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">scala&gt; rdd2.map((_,1)).groupByKey().collect</span><br><span class="line">res27: Array[(Int, Iterable[Int])] &#x3D; Array((1,CompactBuffer(1, 1, 1)), (6,CompactBuffer(1)), (3,CompactBuffer(1, 1)), (5,CompactBuffer(1)), (2,CompactBuffer(1, 1)))</span><br></pre></td></tr></table></figure>


<ol start="11">
<li><p>groupBy<br>可以按照不同的分组条件进行分组<br>比如，对 value 进行分组<br>rdd.map((<em>,1)).groupBy(</em>._2).foreach(println)<br>输出：<br>因为value都是1，所以输出的就只有1组<br>(1,CompactBuffer((1,1), (2,1), (3,1), (4,1), (5,1), (6,1)))</p>
</li>
<li><p>join</p>
</li>
</ol>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">val rdd1 &#x3D; sc.textFile(&quot;data&#x2F;dept.txt&quot;）</span><br><span class="line">val rdd2 &#x3D; sc.textFile(&quot;data&#x2F;emp.txt&quot;)</span><br><span class="line"></span><br><span class="line">val deptRdd &#x3D; rdd1.map(x &#x3D;&gt; &#123;</span><br><span class="line">  val splits &#x3D; x.split(&quot;,&quot;)</span><br><span class="line">  (splits(0).trim, (splits(1).trim, splits(2).trim))</span><br><span class="line">&#125;)</span><br><span class="line">val empRdd &#x3D; rdd2.map(x &#x3D;&gt; &#123;</span><br><span class="line">  val splits &#x3D; x.split(&quot; &quot;)</span><br><span class="line">  (splits(4).trim, (splits(0).trim, splits(1).trim, splits(2).trim, splits(3).trim))</span><br><span class="line">&#125;)</span><br><span class="line">deptRdd.join(empRdd)</span><br><span class="line">deptRdd.leftOutJoin(empRdd)</span><br><span class="line">deptRdd.rightOutJoin(empRdd)</span><br><span class="line">deptRdd.fullOutJoin(empRdd)</span><br><span class="line">(20,(Some((RESEARCH,DALLAS)),Some((1002,bill,ceo,55))))</span><br><span class="line">(30,(Some((SALES,CHICAGO)),Some((1003,cindy,cw,32))))</span><br><span class="line">(40,(Some((OPERATIONS,BOSTON)),None))</span><br><span class="line">(10,(Some((ACCOUNTIN,NEW YORK)),Some((1001,bob,sales,30))))</span><br></pre></td></tr></table></figure>


<ol start="13">
<li>cogroup</li>
</ol>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">deptRdd.cogroup(empRdd).foreach(println)</span><br><span class="line">输出：</span><br><span class="line">(20,(CompactBuffer((RESEARCH,DALLAS)),CompactBuffer((1002,bill,ceo,55))))</span><br><span class="line">(30,(CompactBuffer((SALES,CHICAGO)),CompactBuffer((1003,cindy,cw,32))))</span><br><span class="line">(40,(CompactBuffer((OPERATIONS,BOSTON)),CompactBuffer()))</span><br><span class="line">(10,(CompactBuffer((ACCOUNTIN,NEW YORK)),CompactBuffer((1001,bob,sales,30))))</span><br><span class="line"></span><br><span class="line">join 和 从group的关系</span><br><span class="line">join 底层调用 了 从group</span><br><span class="line">def join[W](other: RDD[(K, W)], partitioner: Partitioner): RDD[(K, (V, W))] &#x3D; 				  self.withScope &#123;</span><br><span class="line">  this.cogroup(other, partitioner).flatMapValues( pair &#x3D;&gt;</span><br><span class="line">    for (v &lt;- pair._1.iterator; w &lt;- pair._2.iterator) yield (v, w)</span><br><span class="line">  )</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>



<ol start="14">
<li>zipWithIndex<br>相当于给每个元素加个索引，形成K,V 结构    </li>
</ol>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">scala&gt; rdd1.zipWithIndex.collectAsMap</span><br><span class="line">res18: scala.collection.Map[Int,Long] &#x3D; Map(2 -&gt; 1, 5 -&gt; 4, 4 -&gt; 3, 1 -&gt; 0, 3 -&gt; 2, 6 -&gt; 5)</span><br></pre></td></tr></table></figure>
]]></content>
      <tags>
        <tag>tedata</tag>
      </tags>
  </entry>
  <entry>
    <title>[转载]NameNode 高可用整体架构概述</title>
    <url>/2016/09/13/namenodeha/</url>
    <content><![CDATA[<h1 id="NameNode-高可用整体架构概述"><a href="#NameNode-高可用整体架构概述" class="headerlink" title="NameNode 高可用整体架构概述"></a>NameNode 高可用整体架构概述</h1><p>在 Hadoop 1.0 时代，Hadoop 的两大核心组件 HDFS NameNode 和 JobTracker 都存在着单点问题，这其中以 NameNode 的单点问题尤为严重。因为 NameNode 保存了整个 HDFS 的元数据信息，一旦 NameNode 挂掉，整个 HDFS 就无法访问，同时 Hadoop 生态系统中依赖于 HDFS 的各个组件，包括 MapReduce、Hive、Pig 以及 HBase 等也都无法正常工作，并且重新启动 NameNode 和进行数据恢复的过程也会比较耗时。这些问题在给 Hadoop 的使用者带来困扰的同时，也极大地限制了 Hadoop 的使用场景，使得 Hadoop 在很长的时间内仅能用作离线存储和离线计算，无法应用到对可用性和数据一致性要求很高的在线应用场景中。</p>
<p>所幸的是，在 Hadoop2.0 中，HDFS NameNode 和 YARN ResourceManger(JobTracker 在 2.0 中已经被整合到 YARN ResourceManger 之中) 的单点问题都得到了解决，经过多个版本的迭代和发展，目前已经能用于生产环境。HDFS NameNode 和 YARN ResourceManger 的高可用 (High Availability，HA) 方案基本类似，两者也复用了部分代码，但是由于 HDFS NameNode 对于数据存储和数据一致性的要求比 YARN ResourceManger 高得多，所以 HDFS NameNode 的高可用实现更为复杂一些，本文从内部实现的角度对 HDFS NameNode 的高可用机制进行详细的分析。</p>
<p>HDFS NameNode 的高可用整体架构如图 1 所示 (图片来源于参考文献 [1])：</p>
<p>图 1.HDFS NameNode 高可用整体架构<br><img src="/2016/09/13/namenodeha/img001.png" alt="avatar"></p>
<p>点击查看大图</p>
<p>从上图中，我们可以看出 NameNode 的高可用架构主要分为下面几个部分：</p>
<ul>
<li>Active NameNode 和 Standby NameNode：</li>
</ul>
<p>两台 NameNode 形成互备，一台处于 Active 状态，为主 NameNode，另外一台处于 Standby 状态，为备 NameNode，只有主 NameNode 才能对外提供读写服务。</p>
<p>主备切换控制器 ZKFailoverController：ZKFailoverController 作为独立的进程运行，对 NameNode 的主备切换进行总体控制。ZKFailoverController 能及时检测到 NameNode 的健康状况，在主 NameNode 故障时借助 Zookeeper 实现自动的主备选举和切换，当然 NameNode 目前也支持不依赖于 Zookeeper 的手动主备切换。</p>
<ul>
<li>Zookeeper 集群：为主备切换控制器提供主备选举支持。</li>
<li>共享存储系统：</li>
</ul>
<p>共享存储系统是实现 NameNode 的高可用最为关键的部分，共享存储系统保存了 NameNode 在运行过程中所产生的 HDFS 的元数据。主 NameNode 和</p>
<p>NameNode 通过共享存储系统实现元数据同步。在进行主备切换的时候，新的主 NameNode 在确认元数据完全同步之后才能继续对外提供服务。</p>
<p>DataNode 节点：除了通过共享存储系统共享 HDFS 的元数据信息之外，主 NameNode 和备 NameNode 还需要共享 HDFS 的数据块和 DataNode 之间的映射关系。DataNode 会同时向主 NameNode 和备 NameNode 上报数据块的位置信息。</p>
<p>下面开始分别介绍 NameNode 的主备切换实现和共享存储系统的实现，在文章的最后会结合笔者的实践介绍一下在 NameNode 的高可用运维中的一些注意事项。</p>
<h3 id="一-NameNode-的主备切换实现"><a href="#一-NameNode-的主备切换实现" class="headerlink" title="一. NameNode 的主备切换实现"></a>一. NameNode 的主备切换实现</h3><p>NameNode 主备切换主要由 ZKFailoverController、HealthMonitor 和 ActiveStandbyElector 这 3 个组件来协同实现：</p>
<p>ZKFailoverController 作为 NameNode 机器上一个独立的进程启动 (在 hdfs 启动脚本之中的进程名为 zkfc)，启动的时候会创建 HealthMonitor 和 ActiveStandbyElector 这两个主要的内部组件，ZKFailoverController 在创建 HealthMonitor 和 ActiveStandbyElector 的同时，也会向 HealthMonitor 和 ActiveStandbyElector 注册相应的回调方法。</p>
<p>HealthMonitor 主要负责检测 NameNode 的健康状态，如果检测到 NameNode 的状态发生变化，会回调 ZKFailoverController 的相应方法进行自动的主备选举。</p>
<p>ActiveStandbyElector 主要负责完成自动的主备选举，内部封装了 Zookeeper 的处理逻辑，一旦 Zookeeper 主备选举完成，会回调 ZKFailoverController 的相应方法来进行 NameNode 的主备状态切换。</p>
<p>NameNode 实现主备切换的流程如图 2 所示，有以下几步：</p>
<ol>
<li>HealthMonitor 初始化完成之后会启动内部的线程来定时调用对应 NameNode 的 HAServiceProtocol RPC 接口的方法，对 NameNode 的健康状态进行检测。</li>
<li>HealthMonitor 如果检测到 NameNode 的健康状态发生变化，会回调 ZKFailoverController 注册的相应方法进行处理。</li>
<li>如果 ZKFailoverController 判断需要进行主备切换，会首先使用 ActiveStandbyElector 来进行自动的主备选举。</li>
<li>ActiveStandbyElector 与 Zookeeper 进行交互完成自动的主备选举。</li>
<li>ActiveStandbyElector 在主备选举完成后，会回调 ZKFailoverController 的相应方法来通知当前的 NameNode 成为主 NameNode 或备 NameNode。</li>
<li>ZKFailoverController 调用对应 NameNode 的 HAServiceProtocol RPC 接口的方法将 NameNode 转换为 Active 状态或 Standby 状态。<br>图 2.NameNode 的主备切换流程<br><img src="/2016/09/13/namenodeha/img002.png" alt="avatar"><br>点击查看大图</li>
</ol>
<p>下面分别对 HealthMonitor、ActiveStandbyElector 和 ZKFailoverController 的实现细节进行分析：</p>
<p>HealthMonitor 实现分析<br>ZKFailoverController 在初始化的时候会创建 HealthMonitor，HealthMonitor 在内部会启动一个线程来循环调用 NameNode 的 HAServiceProtocol RPC 接口的方法来检测 NameNode 的状态，并将状态的变化通过回调的方式来通知 ZKFailoverController。</p>
<p>HealthMonitor 主要检测 NameNode 的两类状态，分别是 HealthMonitor.State 和 HAServiceStatus。HealthMonitor.State 是通过 HAServiceProtocol RPC 接口的 monitorHealth 方法来获取的，反映了 NameNode 节点的健康状况，主要是磁盘存储资源是否充足。HealthMonitor.State 包括下面几种状态：</p>
<ul>
<li>INITIALIZING：HealthMonitor 在初始化过程中，还没有开始进行健康状况检测；</li>
<li>SERVICE_HEALTHY：NameNode 状态正常；</li>
<li>SERVICE_NOT_RESPONDING：调用 NameNode 的 monitorHealth 方法调用无响应或响应超时；</li>
<li>SERVICE_UNHEALTHY：NameNode 还在运行，但是 monitorHealth 方法返回状态不正常，磁盘存储资源不足；</li>
<li>HEALTH_MONITOR_FAILED：HealthMonitor 自己在运行过程中发生了异常，不能继续检测 NameNode 的健康状况，会导致 ZKFailoverController 进程退出；</li>
</ul>
<p>HealthMonitor.State 在状态检测之中起主要的作用，在 HealthMonitor.State 发生变化的时候，HealthMonitor 会回调 ZKFailoverController 的相应方法来进行处理，具体处理见后文 ZKFailoverController 部分所述。</p>
<p>而 HAServiceStatus 则是通过 HAServiceProtocol RPC 接口的 getServiceStatus 方法来获取的，主要反映的是 NameNode 的 HA 状态，包括：</p>
<ul>
<li>INITIALIZING：NameNode 在初始化过程中；</li>
<li>ACTIVE：当前 NameNode 为主 NameNode；</li>
<li>STANDBY：当前 NameNode 为备 NameNode；</li>
<li>STOPPING：当前 NameNode 已停止；</li>
</ul>
<p>HAServiceStatus 在状态检测之中只是起辅助的作用，在 HAServiceStatus 发生变化时，HealthMonitor 也会回调 ZKFailoverController 的相应方法来进行处理，具体处理见后文 ZKFailoverController 部分所述。</p>
<h3 id="二-ActiveStandbyElector-实现分析"><a href="#二-ActiveStandbyElector-实现分析" class="headerlink" title="二. ActiveStandbyElector 实现分析"></a>二. ActiveStandbyElector 实现分析</h3><p>Namenode(包括 YARN ResourceManager) 的主备选举是通过 ActiveStandbyElector 来完成的，ActiveStandbyElector 主要是利用了 Zookeeper 的写一致性和临时节点机制，具体的主备选举实现如下：</p>
<h5 id="创建锁节点"><a href="#创建锁节点" class="headerlink" title="创建锁节点"></a>创建锁节点</h5><p>如果 HealthMonitor 检测到对应的 NameNode 的状态正常，那么表示这个 NameNode 有资格参加 Zookeeper 的主备选举。如果目前还没有进行过主备选举的话，那么相应的 ActiveStandbyElector 就会发起一次主备选举，尝试在 Zookeeper 上创建一个路径为/hadoop-ha/${dfs.nameservices}/ActiveStandbyElectorLock 的临时节点 (${dfs.nameservices} 为 Hadoop 的配置参数 dfs.nameservices 的值，下同)，Zookeeper 的写一致性会保证最终只会有一个 ActiveStandbyElector 创建成功，那么创建成功的 ActiveStandbyElector 对应的 NameNode 就会成为主 NameNode，ActiveStandbyElector 会回调 ZKFailoverController 的方法进一步将对应的 NameNode 切换为 Active 状态。而创建失败的 ActiveStandbyElector 对应的 NameNode 成为备 NameNode，ActiveStandbyElector 会回调 ZKFailoverController 的方法进一步将对应的 NameNode 切换为 Standby 状态。</p>
<h5 id="注册-Watcher-监听"><a href="#注册-Watcher-监听" class="headerlink" title="注册 Watcher 监听"></a>注册 Watcher 监听</h5><p>不管创建/hadoop-ha/${dfs.nameservices}/ActiveStandbyElectorLock 节点是否成功，ActiveStandbyElector 随后都会向 Zookeeper 注册一个 Watcher 来监听这个节点的状态变化事件，ActiveStandbyElector 主要关注这个节点的 NodeDeleted 事件。</p>
<h5 id="自动触发主备选举"><a href="#自动触发主备选举" class="headerlink" title="自动触发主备选举"></a>自动触发主备选举</h5><p>如果 Active NameNode 对应的 HealthMonitor 检测到 NameNode 的状态异常时， ZKFailoverController 会主动删除当前在 Zookeeper 上建立的临时节点/hadoop-ha/${dfs.nameservices}/ActiveStandbyElectorLock，这样处于 Standby 状态的 NameNode 的 ActiveStandbyElector 注册的监听器就会收到这个节点的 NodeDeleted 事件。收到这个事件之后，会马上再次进入到创建/hadoop-ha/${dfs.nameservices}/ActiveStandbyElectorLock 节点的流程，如果创建成功，这个本来处于 Standby 状态的 NameNode 就选举为主 NameNode 并随后开始切换为 Active 状态。</p>
<p>当然，如果是 Active 状态的 NameNode 所在的机器整个宕掉的话，那么根据 Zookeeper 的临时节点特性，/hadoop-ha/${dfs.nameservices}/ActiveStandbyElectorLock 节点会自动被删除，从而也会自动进行一次主备切换。</p>
<h5 id="防止脑裂"><a href="#防止脑裂" class="headerlink" title="防止脑裂"></a>防止脑裂</h5><p>Zookeeper 在工程实践的过程中经常会发生的一个现象就是 Zookeeper 客户端“假死”，所谓的“假死”是指如果 Zookeeper 客户端机器负载过高或者正在进行 JVM Full GC，那么可能会导致 Zookeeper 客户端到 Zookeeper 服务端的心跳不能正常发出，一旦这个时间持续较长，超过了配置的 Zookeeper Session Timeout 参数的话，Zookeeper 服务端就会认为客户端的 session 已经过期从而将客户端的 Session 关闭。“假死”有可能引起分布式系统常说的双主或脑裂 (brain-split) 现象。具体到本文所述的 NameNode，假设 NameNode1 当前为 Active 状态，NameNode2 当前为 Standby 状态。如果某一时刻 NameNode1 对应的 ZKFailoverController 进程发生了“假死”现象，那么 Zookeeper 服务端会认为 NameNode1 挂掉了，根据前面的主备切换逻辑，NameNode2 会替代 NameNode1 进入 Active 状态。但是此时 NameNode1 可能仍然处于 Active 状态正常运行，即使随后 NameNode1 对应的 ZKFailoverController 因为负载下降或者 Full GC 结束而恢复了正常，感知到自己和 Zookeeper 的 Session 已经关闭，但是由于网络的延迟以及 CPU 线程调度的不确定性，仍然有可能会在接下来的一段时间窗口内 NameNode1 认为自己还是处于 Active 状态。这样 NameNode1 和 NameNode2 都处于 Active 状态，都可以对外提供服务。这种情况对于 NameNode 这类对数据一致性要求非常高的系统来说是灾难性的，数据会发生错乱且无法恢复。Zookeeper 社区对这种问题的解决方法叫做 fencing，中文翻译为隔离，也就是想办法把旧的 Active NameNode 隔离起来，使它不能正常对外提供服务。</p>
<p>ActiveStandbyElector 为了实现 fencing，会在成功创建 Zookeeper 节点 hadoop-ha/${dfs.nameservices}/ActiveStandbyElectorLock 从而成为 Active NameNode 之后，创建另外一个路径为/hadoop-ha/${dfs.nameservices}/ActiveBreadCrumb 的持久节点，这个节点里面保存了这个 Active NameNode 的地址信息。Active NameNode 的 ActiveStandbyElector 在正常的状态下关闭 Zookeeper Session 的时候 (注意由于/hadoop-ha/${dfs.nameservices}/ActiveStandbyElectorLock 是临时节点，也会随之删除)，会一起删除节点/hadoop-ha/${dfs.nameservices}/ActiveBreadCrumb。但是如果 ActiveStandbyElector 在异常的状态下 Zookeeper Session 关闭 (比如前述的 Zookeeper 假死)，那么由于/hadoop-ha/${dfs.nameservices}/ActiveBreadCrumb 是持久节点，会一直保留下来。后面当另一个 NameNode 选主成功之后，会注意到上一个 Active NameNode 遗留下来的这个节点，从而会回调 ZKFailoverController 的方法对旧的 Active NameNode 进行 fencing，具体处理见后文 ZKFailoverController 部分所述。</p>
<h3 id="三-ZKFailoverController-实现分析"><a href="#三-ZKFailoverController-实现分析" class="headerlink" title="三. ZKFailoverController 实现分析"></a>三. ZKFailoverController 实现分析</h3><p>ZKFailoverController 在创建 HealthMonitor 和 ActiveStandbyElector 的同时，会向 HealthMonitor 和 ActiveStandbyElector 注册相应的回调函数，ZKFailoverController 的处理逻辑主要靠 HealthMonitor 和 ActiveStandbyElector 的回调函数来驱动。</p>
<h5 id="对-HealthMonitor-状态变化的处理"><a href="#对-HealthMonitor-状态变化的处理" class="headerlink" title="对 HealthMonitor 状态变化的处理"></a>对 HealthMonitor 状态变化的处理</h5><p>如前所述，HealthMonitor 会检测 NameNode 的两类状态，HealthMonitor.State 在状态检测之中起主要的作用，ZKFailoverController 注册到 HealthMonitor 上的处理 HealthMonitor.State 状态变化的回调函数主要关注 SERVICE_HEALTHY、SERVICE_NOT_RESPONDING 和 SERVICE_UNHEALTHY 这 3 种状态：</p>
<p>如果检测到状态为 SERVICE_HEALTHY，表示当前的 NameNode 有资格参加 Zookeeper 的主备选举，如果目前还没有进行过主备选举的话，ZKFailoverController 会调用 ActiveStandbyElector 的 joinElection 方法发起一次主备选举。<br>如果检测到状态为 SERVICE_NOT_RESPONDING 或者是 SERVICE_UNHEALTHY，就表示当前的 NameNode 出现问题了，ZKFailoverController 会调用 ActiveStandbyElector 的 quitElection 方法删除当前已经在 Zookeeper 上建立的临时节点退出主备选举，这样其它的 NameNode 就有机会成为主 NameNode。<br>而 HAServiceStatus 在状态检测之中仅起辅助的作用，在 HAServiceStatus 发生变化时，ZKFailoverController 注册到 HealthMonitor 上的处理 HAServiceStatus 状态变化的回调函数会判断 NameNode 返回的 HAServiceStatus 和 ZKFailoverController 所期望的是否一致，如果不一致的话，ZKFailoverController 也会调用 ActiveStandbyElector 的 quitElection 方法删除当前已经在 Zookeeper 上建立的临时节点退出主备选举。</p>
<h5 id="对-ActiveStandbyElector-主备选举状态变化的处理"><a href="#对-ActiveStandbyElector-主备选举状态变化的处理" class="headerlink" title="对 ActiveStandbyElector 主备选举状态变化的处理"></a>对 ActiveStandbyElector 主备选举状态变化的处理</h5><p>在 ActiveStandbyElector 的主备选举状态发生变化时，会回调 ZKFailoverController 注册的回调函数来进行相应的处理：</p>
<ul>
<li>如果 ActiveStandbyElector 选主成功，那么 ActiveStandbyElector 对应的 NameNode 成为主 NameNode，ActiveStandbyElector 会回调 ZKFailoverController 的 becomeActive 方法，这个方法通过调用对应的 NameNode 的 HAServiceProtocol RPC 接口的 transitionToActive 方法，将 NameNode 转换为 Active 状态。</li>
<li>如果 ActiveStandbyElector 选主失败，那么 ActiveStandbyElector 对应的 NameNode 成为备 NameNode，ActiveStandbyElector 会回调 ZKFailoverController 的 becomeStandby 方法，这个方法通过调用对应的 NameNode 的 HAServiceProtocol RPC 接口的 transitionToStandby 方法，将 NameNode 转换为 Standby 状态。</li>
<li>如果 ActiveStandbyElector 选主成功之后，发现了上一个 Active NameNode 遗留下来的/hadoop-ha/${dfs.nameservices}/ActiveBreadCrumb 节点 (见“ActiveStandbyElector 实现分析”一节“防止脑裂”部分所述)，那么 ActiveStandbyElector 会首先回调 ZKFailoverController 注册的 fenceOldActive 方法，尝试对旧的 Active NameNode 进行 fencing，在进行 fencing 的时候，会执行以下的操作：<br>首先尝试调用这个旧 Active NameNode 的 HAServiceProtocol RPC 接口的 transitionToStandby 方法，看能不能把它转换为 Standby 状态。<br>如果 transitionToStandby 方法调用失败，那么就执行 Hadoop 配置文件之中预定义的隔离措施，Hadoop 目前主要提供两种隔离措施，通常会选择 sshfence：<br>sshfence：通过 SSH 登录到目标机器上，执行命令 fuser 将对应的进程杀死；<br>shellfence：执行一个用户自定义的 shell 脚本来将对应的进程隔离；<br>只有在成功地执行完成 fencing 之后，选主成功的 ActiveStandbyElector 才会回调 ZKFailoverController 的 becomeActive 方法将对应的 NameNode 转换为 Active 状态，开始对外提供服务。</li>
</ul>
<h3 id="四-NameNode-的共享存储实现"><a href="#四-NameNode-的共享存储实现" class="headerlink" title="四. NameNode 的共享存储实现"></a>四. NameNode 的共享存储实现</h3><p>过去几年中 Hadoop 社区涌现过很多的 NameNode 共享存储方案，比如 shared NAS+NFS、BookKeeper、BackupNode 和 QJM(Quorum Journal Manager) 等等。目前社区已经把由 Clouderea 公司实现的基于 QJM 的方案合并到 HDFS 的 trunk 之中并且作为默认的共享存储实现，本部分只针对基于 QJM 的共享存储方案的内部实现原理进行分析。为了理解 QJM 的设计和实现，首先要对 NameNode 的元数据存储结构有所了解。</p>
<h5 id="NameNode-的元数据存储概述"><a href="#NameNode-的元数据存储概述" class="headerlink" title="NameNode 的元数据存储概述"></a>NameNode 的元数据存储概述</h5><p>一个典型的 NameNode 的元数据存储目录结构如图 3 所示 (图片来源于参考文献 [4])，这里主要关注其中的 EditLog 文件和 FSImage 文件：</p>
<p>图 3 .NameNode 的元数据存储目录结构<br><img src="/2016/09/13/namenodeha/img003.png" alt="avatar"><br>点击查看大图</p>
<h5 id="NameNode-在执行-HDFS"><a href="#NameNode-在执行-HDFS" class="headerlink" title="NameNode 在执行 HDFS"></a>NameNode 在执行 HDFS</h5><p>客户端提交的创建文件或者移动文件这样的写操作的时候，会首先把这些操作记录在 EditLog 文件之中，然后再更新内存中的文件系统镜像。内存中的文件系统镜像用于 NameNode 向客户端提供读服务，而 EditLog 仅仅只是在数据恢复的时候起作用。记录在 EditLog 之中的每一个操作又称为一个事务，每个事务有一个整数形式的事务 id 作为编号。EditLog 会被切割为很多段，每一段称为一个 Segment。正在写入的 EditLog Segment 处于 in-progress 状态，其文件名形如 edits_inprogress_${start_txid}，其中${start_txid} 表示这个 segment 的起始事务 id，例如上图中的 edits_inprogress_0000000000000000020。而已经写入完成的 EditLog Segment 处于 finalized 状态，其文件名形如 edits_${start_txid}-${end_txid}，其中${start_txid} 表示这个 segment 的起始事务 id，${end_txid} 表示这个 segment 的结束事务 id，例如上图中的 edits_0000000000000000001-0000000000000000019。</p>
<p>NameNode 会定期对内存中的文件系统镜像进行 checkpoint 操作，在磁盘上生成 FSImage 文件，FSImage 文件的文件名形如 fsimage_${end_txid}，其中${end_txid} 表示这个 fsimage 文件的结束事务 id，例如上图中的 fsimage_0000000000000000020。在 NameNode 启动的时候会进行数据恢复，首先把 FSImage 文件加载到内存中形成文件系统镜像，然后再把 EditLog 之中 FsImage 的结束事务 id 之后的 EditLog 回放到这个文件系统镜像上。</p>
<h3 id="五-基于-QJM-的共享存储系统的总体架构"><a href="#五-基于-QJM-的共享存储系统的总体架构" class="headerlink" title="五. 基于 QJM 的共享存储系统的总体架构"></a>五. 基于 QJM 的共享存储系统的总体架构</h3><p>基于 QJM 的共享存储系统主要用于保存 EditLog，并不保存 FSImage 文件。FSImage 文件还是在 NameNode 的本地磁盘上。QJM 共享存储的基本思想来自于 Paxos 算法 (参见参考文献 [3])，采用多个称为 JournalNode 的节点组成的 JournalNode 集群来存储 EditLog。每个 JournalNode 保存同样的 EditLog 副本。每次 NameNode 写 EditLog 的时候，除了向本地磁盘写入 EditLog 之外，也会并行地向 JournalNode 集群之中的每一个 JournalNode 发送写请求，只要大多数 (majority) 的 JournalNode 节点返回成功就认为向 JournalNode 集群写入 EditLog 成功。如果有 2N+1 台 JournalNode，那么根据大多数的原则，最多可以容忍有 N 台 JournalNode 节点挂掉。</p>
<p>基于 QJM 的共享存储系统的内部实现架构图如图 4 所示，主要包含下面几个主要的组件：</p>
<p>图 4 . 基于 QJM 的共享存储系统的内部实现架构图<br><img src="/2016/09/13/namenodeha/img004.png" alt="avatar"><br>点击查看大图</p>
<ul>
<li>FSEditLog：这个类封装了对 EditLog 的所有操作，是 NameNode 对 EditLog 的所有操作的入口。</li>
<li>JournalSet： 这个类封装了对本地磁盘和 JournalNode 集群上的 EditLog 的操作，内部包含了两类 JournalManager，一类为 FileJournalManager，用于实现对本地磁盘上 EditLog 的操作。一类为 QuorumJournalManager，用于实现对 JournalNode 集群上共享目录的 EditLog 的操作。FSEditLog 只会调用 JournalSet 的相关方法，而不会直接使用 FileJournalManager 和 QuorumJournalManager。</li>
<li>FileJournalManager：封装了对本地磁盘上的 EditLog 文件的操作，不仅 NameNode 在向本地磁盘上写入 EditLog 的时候使用 FileJournalManager，JournalNode 在向本地磁盘写入 EditLog 的时候也复用了 FileJournalManager 的代码和逻辑。</li>
<li>QuorumJournalManager：封装了对 JournalNode 集群上的 EditLog 的操作，它会根据 JournalNode 集群的 URI 创建负责与 JournalNode 集群通信的类 AsyncLoggerSet， QuorumJournalManager 通过 AsyncLoggerSet 来实现对 JournalNode 集群上的 EditLog 的写操作，对于读操作，QuorumJournalManager 则是通过 Http 接口从 JournalNode 上的 JournalNodeHttpServer 读取 EditLog 的数据。</li>
<li>AsyncLoggerSet：内部包含了与 JournalNode 集群进行通信的 AsyncLogger 列表，每一个 AsyncLogger 对应于一个 JournalNode 节点，另外 AsyncLoggerSet 也包含了用于等待大多数 JournalNode 返回结果的工具类方法给 QuorumJournalManager 使用。</li>
<li>AsyncLogger：具体的实现类是 IPCLoggerChannel，IPCLoggerChannel 在执行方法调用的时候，会把调用提交到一个单线程的线程池之中，由线程池线程来负责向对应的 JournalNode 的 JournalNodeRpcServer 发送 RPC 请求。</li>
<li>JournalNodeRpcServer：运行在 JournalNode 节点进程中的 RPC 服务，接收 NameNode 端的 AsyncLogger 的 RPC 请求。</li>
<li>JournalNodeHttpServer：运行在 JournalNode 节点进程中的 Http 服务，用于接收处于 Standby 状态的 NameNode 和其它 JournalNode 的同步 EditLog 文件流的请求。</li>
</ul>
<p>下面对基于 QJM 的共享存储系统的两个关键性问题同步数据和恢复数据进行详细分析。</p>
<h3 id="六-基于-QJM-的共享存储系统的数据同步机制分析"><a href="#六-基于-QJM-的共享存储系统的数据同步机制分析" class="headerlink" title="六. 基于 QJM 的共享存储系统的数据同步机制分析"></a>六. 基于 QJM 的共享存储系统的数据同步机制分析</h3><p>Active NameNode 和 StandbyNameNode 使用 JouranlNode 集群来进行数据同步的过程如图 5 所示，Active NameNode 首先把 EditLog 提交到 JournalNode 集群，然后 Standby NameNode 再从 JournalNode 集群定时同步 EditLog：</p>
<p>图 5 . 基于 QJM 的共享存储的数据同步机制<br><img src="/2016/09/13/namenodeha/img005.png" alt="avatar"><br>点击查看大图</p>
<h5 id="Active-NameNode-提交-EditLog-到-JournalNode-集群"><a href="#Active-NameNode-提交-EditLog-到-JournalNode-集群" class="headerlink" title="Active NameNode 提交 EditLog 到 JournalNode 集群"></a>Active NameNode 提交 EditLog 到 JournalNode 集群</h5><p>当处于 Active 状态的 NameNode 调用 FSEditLog 类的 logSync 方法来提交 EditLog 的时候，会通过 JouranlSet 同时向本地磁盘目录和 JournalNode 集群上的共享存储目录写入 EditLog。写入 JournalNode 集群是通过并行调用每一个 JournalNode 的 QJournalProtocol RPC 接口的 journal 方法实现的，如果对大多数 JournalNode 的 journal 方法调用成功，那么就认为提交 EditLog 成功，否则 NameNode 就会认为这次提交 EditLog 失败。提交 EditLog 失败会导致 Active NameNode 关闭 JournalSet 之后退出进程，留待处于 Standby 状态的 NameNode 接管之后进行数据恢复。</p>
<p>从上面的叙述可以看出，Active NameNode 提交 EditLog 到 JournalNode 集群的过程实际上是同步阻塞的，但是并不需要所有的 JournalNode 都调用成功，只要大多数 JournalNode 调用成功就可以了。如果无法形成大多数，那么就认为提交 EditLog 失败，NameNode 停止服务退出进程。如果对应到分布式系统的 CAP 理论的话，虽然采用了 Paxos 的“大多数”思想对 C(consistency，一致性) 和 A(availability，可用性) 进行了折衷，但还是可以认为 NameNode 选择了 C 而放弃了 A，这也符合 NameNode 对数据一致性的要求。</p>
<h5 id="Standby-NameNode-从-JournalNode-集群同步-EditLog"><a href="#Standby-NameNode-从-JournalNode-集群同步-EditLog" class="headerlink" title="Standby NameNode 从 JournalNode 集群同步 EditLog"></a>Standby NameNode 从 JournalNode 集群同步 EditLog</h5><p>当 NameNode 进入 Standby 状态之后，会启动一个 EditLogTailer 线程。这个线程会定期调用 EditLogTailer 类的 doTailEdits 方法从 JournalNode 集群上同步 EditLog，然后把同步的 EditLog 回放到内存之中的文件系统镜像上 (并不会同时把 EditLog 写入到本地磁盘上)。</p>
<p>这里需要关注的是：从 JournalNode 集群上同步的 EditLog 都是处于 finalized 状态的 EditLog Segment。“NameNode 的元数据存储概述”一节说过 EditLog Segment 实际上有两种状态，处于 in-progress 状态的 Edit Log 当前正在被写入，被认为是处于不稳定的中间态，有可能会在后续的过程之中发生修改，比如被截断。Active NameNode 在完成一个 EditLog Segment 的写入之后，就会向 JournalNode 集群发送 finalizeLogSegment RPC 请求，将完成写入的 EditLog Segment finalized，然后开始下一个新的 EditLog Segment。一旦 finalizeLogSegment 方法在大多数的 JournalNode 上调用成功，表明这个 EditLog Segment 已经在大多数的 JournalNode 上达成一致。一个 EditLog Segment 处于 finalized 状态之后，可以保证它再也不会变化。</p>
<p>从上面描述的过程可以看出，虽然 Active NameNode 向 JournalNode 集群提交 EditLog 是同步的，但 Standby NameNode 采用的是定时从 JournalNode 集群上同步 EditLog 的方式，那么 Standby NameNode 内存中文件系统镜像有很大的可能是落后于 Active NameNode 的，所以 Standby NameNode 在转换为 Active NameNode 的时候需要把落后的 EditLog 补上来。</p>
<h3 id="七-基于-QJM-的共享存储系统的数据恢复机制分析"><a href="#七-基于-QJM-的共享存储系统的数据恢复机制分析" class="headerlink" title="七. 基于 QJM 的共享存储系统的数据恢复机制分析"></a>七. 基于 QJM 的共享存储系统的数据恢复机制分析</h3><p>处于 Standby 状态的 NameNode 转换为 Active 状态的时候，有可能上一个 Active NameNode 发生了异常退出，那么 JournalNode 集群中各个 JournalNode 上的 EditLog 就可能会处于不一致的状态，所以首先要做的事情就是让 JournalNode 集群中各个节点上的 EditLog 恢复为一致。另外如前所述，当前处于 Standby 状态的 NameNode 的内存中的文件系统镜像有很大的可能是落后于旧的 Active NameNode 的，所以在 JournalNode 集群中各个节点上的 EditLog 达成一致之后，接下来要做的事情就是从 JournalNode 集群上补齐落后的 EditLog。只有在这两步完成之后，当前新的 Active NameNode 才能安全地对外提供服务。</p>
<p>补齐落后的 EditLog 的过程复用了前面描述的 Standby NameNode 从 JournalNode 集群同步 EditLog 的逻辑和代码，最终调用 EditLogTailer 类的 doTailEdits 方法来完成 EditLog 的补齐。使 JournalNode 集群上的 EditLog 达成一致的过程是一致性算法 Paxos 的典型应用场景，QJM 对这部分的处理可以看做是 Single Instance Paxos(参见参考文献 [3]) 算法的一个实现，在达成一致的过程中，Active NameNode 和 JournalNode 集群之间的交互流程如图 6 所示，具体描述如下：</p>
<p>图 6.Active NameNode 和 JournalNode 集群的交互流程图<br><img src="/2016/09/13/namenodeha/img006.png" alt="avatar"><br>点击查看大图</p>
<h5 id="生成一个新的-Epoch"><a href="#生成一个新的-Epoch" class="headerlink" title="生成一个新的 Epoch"></a>生成一个新的 Epoch</h5><p>Epoch 是一个单调递增的整数，用来标识每一次 Active NameNode 的生命周期，每发生一次 NameNode 的主备切换，Epoch 就会加 1。这实际上是一种 fencing 机制，为什么需要 fencing 已经在前面“ActiveStandbyElector 实现分析”一节的“防止脑裂”部分进行了说明。产生新 Epoch 的流程与 Zookeeper 的 ZAB(Zookeeper Atomic Broadcast) 协议在进行数据恢复之前产生新 Epoch 的过程完全类似：</p>
<ol>
<li>Active NameNode 首先向 JournalNode 集群发送 getJournalState RPC 请求，每个 JournalNode 会返回自己保存的最近的那个 Epoch(代码中叫 lastPromisedEpoch)。</li>
<li>NameNode 收到大多数的 JournalNode 返回的 Epoch 之后，在其中选择最大的一个加 1 作为当前的新 Epoch，然后向各个 JournalNode 发送 newEpoch RPC 请求，把这个新的 Epoch 发给各个 JournalNode。</li>
<li>每一个 JournalNode 在收到新的 Epoch 之后，首先检查这个新的 Epoch 是否比它本地保存的 lastPromisedEpoch 大，如果大的话就把 lastPromisedEpoch 更新为这个新的 Epoch，并且向 NameNode 返回它自己的本地磁盘上最新的一个 EditLogSegment 的起始事务 id，为后面的数据恢复过程做好准备。如果小于或等于的话就向 NameNode 返回错误。</li>
<li>NameNode 收到大多数 JournalNode 对 newEpoch 的成功响应之后，就会认为生成新的 Epoch 成功。</li>
</ol>
<p>在生成新的 Epoch 之后，每次 NameNode 在向 JournalNode 集群提交 EditLog 的时候，都会把这个 Epoch 作为参数传递过去。每个 JournalNode 会比较传过来的 Epoch 和它自己保存的 lastPromisedEpoch 的大小，如果传过来的 epoch 的值比它自己保存的 lastPromisedEpoch 小的话，那么这次写相关操作会被拒绝。一旦大多数 JournalNode 都拒绝了这次写操作，那么这次写操作就失败了。如果原来的 Active NameNode 恢复正常之后再向 JournalNode 写 EditLog，那么因为它的 Epoch 肯定比新生成的 Epoch 小，并且大多数的 JournalNode 都接受了这个新生成的 Epoch，所以拒绝写入的 JournalNode 数目至少是大多数，这样原来的 Active NameNode 写 EditLog 就肯定会失败，失败之后这个 NameNode 进程会直接退出，这样就实现了对原来的 Active NameNode 的隔离了。</p>
<h5 id="选择需要数据恢复的-EditLog-Segment-的-id"><a href="#选择需要数据恢复的-EditLog-Segment-的-id" class="headerlink" title="选择需要数据恢复的 EditLog Segment 的 id"></a>选择需要数据恢复的 EditLog Segment 的 id</h5><p>需要恢复的 Edit Log 只可能是各个 JournalNode 上的最后一个 Edit Log Segment，如前所述，JournalNode 在处理完 newEpoch RPC 请求之后，会向 NameNode 返回它自己的本地磁盘上最新的一个 EditLog Segment 的起始事务 id，这个起始事务 id 实际上也作为这个 EditLog Segment 的 id。NameNode 会在所有这些 id 之中选择一个最大的 id 作为要进行数据恢复的 EditLog Segment 的 id。</p>
<h5 id="向-JournalNode-集群发送-prepareRecovery-RPC-请求"><a href="#向-JournalNode-集群发送-prepareRecovery-RPC-请求" class="headerlink" title="向 JournalNode 集群发送 prepareRecovery RPC 请求"></a>向 JournalNode 集群发送 prepareRecovery RPC 请求</h5><p>NameNode 接下来向 JournalNode 集群发送 prepareRecovery RPC 请求，请求的参数就是选出的 EditLog Segment 的 id。JournalNode 收到请求后返回本地磁盘上这个 Segment 的起始事务 id、结束事务 id 和状态 (in-progress 或 finalized)。</p>
<p>这一步对应于 Paxos 算法的 Phase 1a 和 Phase 1b(参见参考文献 [3]) 两步。Paxos 算法的 Phase1 是 prepare 阶段，这也与方法名 prepareRecovery 相对应。并且这里以前面产生的新的 Epoch 作为 Paxos 算法中的提案编号 (proposal number)。只要大多数的 JournalNode 的 prepareRecovery RPC 调用成功返回，NameNode 就认为成功。</p>
<p>选择进行同步的基准数据源，向 JournalNode 集群发送 acceptRecovery RPC 请求 NameNode 根据 prepareRecovery 的返回结果，选择一个 JournalNode 上的 EditLog Segment 作为同步的基准数据源。选择基准数据源的原则大致是：在 in-progress 状态和 finalized 状态的 Segment 之间优先选择 finalized 状态的 Segment。如果都是 in-progress 状态的话，那么优先选择 Epoch 比较高的 Segment(也就是优先选择更新的)，如果 Epoch 也一样，那么优先选择包含的事务数更多的 Segment。</p>
<p>在选定了同步的基准数据源之后，NameNode 向 JournalNode 集群发送 acceptRecovery RPC 请求，将选定的基准数据源作为参数。JournalNode 接收到 acceptRecovery RPC 请求之后，从基准数据源 JournalNode 的 JournalNodeHttpServer 上下载 EditLog Segment，将本地的 EditLog Segment 替换为下载的 EditLog Segment。</p>
<p>这一步对应于 Paxos 算法的 Phase 2a 和 Phase 2b(参见参考文献 [3]) 两步。Paxos 算法的 Phase2 是 accept 阶段，这也与方法名 acceptRecovery 相对应。只要大多数 JournalNode 的 acceptRecovery RPC 调用成功返回，NameNode 就认为成功。</p>
<h5 id="向-JournalNode-集群发送-finalizeLogSegment-RPC-请求，数据恢复完成"><a href="#向-JournalNode-集群发送-finalizeLogSegment-RPC-请求，数据恢复完成" class="headerlink" title="向 JournalNode 集群发送 finalizeLogSegment RPC 请求，数据恢复完成"></a>向 JournalNode 集群发送 finalizeLogSegment RPC 请求，数据恢复完成</h5><p>上一步执行完成之后，NameNode 确认大多数 JournalNode 上的 EditLog Segment 已经从基准数据源进行了同步。接下来，NameNode 向 JournalNode 集群发送 finalizeLogSegment RPC 请求，JournalNode 接收到请求之后，将对应的 EditLog Segment 从 in-progress 状态转换为 finalized 状态，实际上就是将文件名从 edits_inprogress_${startTxid} 重命名为 edits_${startTxid}-${endTxid}，见“NameNode 的元数据存储概述”一节的描述。</p>
<p>只要大多数 JournalNode 的 finalizeLogSegment RPC 调用成功返回，NameNode 就认为成功。此时可以保证 JournalNode 集群的大多数节点上的 EditLog 已经处于一致的状态，这样 NameNode 才能安全地从 JournalNode 集群上补齐落后的 EditLog 数据。</p>
<p>需要注意的是，尽管基于 QJM 的共享存储方案看起来理论完备，设计精巧，但是仍然无法保证数据的绝对强一致，下面选取参考文献 [2] 中的一个例子来说明：</p>
<p>假设有 3 个 JournalNode：JN1、JN2 和 JN3，Active NameNode 发送了事务 id 为 151、152 和 153 的 3 个事务到 JournalNode 集群，这 3 个事务成功地写入了 JN2，但是在还没能写入 JN1 和 JN3 之前，Active NameNode 就宕机了。同时，JN3 在整个写入的过程中延迟较大，落后于 JN1 和 JN2。最终成功写入 JN1 的事务 id 为 150，成功写入 JN2 的事务 id 为 153，而写入到 JN3 的事务 id 仅为 125，如图 7 所示 (图片来源于参考文献 [2])。按照前面描述的只有成功地写入了大多数的 JournalNode 才认为写入成功的原则，显然事务 id 为 151、152 和 153 的这 3 个事务只能算作写入失败。在进行数据恢复的过程中，会发生下面两种情况：</p>
<p>图 7.JournalNode 集群写入的事务 id 情况<br><img src="/2016/09/13/namenodeha/img007.png" alt="avatar"><br>点击查看大图</p>
<ul>
<li>如果随后的 Active NameNode 进行数据恢复时在 prepareRecovery 阶段收到了 JN2 的回复，那么肯定会以 JN2 对应的 EditLog Segment 为基准来进行数据恢复，这样最后在多数 JournalNode 上的 EditLog Segment 会恢复到事务 153。从恢复的结果来看，实际上可以认为前面宕机的 Active NameNode 对事务 id 为 151、152 和 153 的这 3 个事务的写入成功了。但是如果从 NameNode 自身的角度来看，这显然就发生了数据不一致的情况。</li>
<li>如果随后的 Active NameNode 进行数据恢复时在 prepareRecovery 阶段没有收到 JN2 的回复，那么肯定会以 JN1 对应的 EditLog Segment 为基准来进行数据恢复，这样最后在多数 JournalNode 上的 EditLog Segment 会恢复到事务 150。在这种情况下，如果从 NameNode 自身的角度来看的话，数据就是一致的了。<br>事实上不光本文描述的基于 QJM 的共享存储方案无法保证数据的绝对一致，大家通常认为的一致性程度非常高的 Zookeeper 也会发生类似的情况，这也从侧面说明了要实现一个数据绝对一致的分布式存储系统的确非常困难。</li>
</ul>
<h3 id="八-NameNode-在进行状态转换时对共享存储的处理"><a href="#八-NameNode-在进行状态转换时对共享存储的处理" class="headerlink" title="八. NameNode 在进行状态转换时对共享存储的处理"></a>八. NameNode 在进行状态转换时对共享存储的处理</h3><p>下面对 NameNode 在进行状态转换的过程中对共享存储的处理进行描述，使得大家对基于 QJM 的共享存储方案有一个完整的了解，同时也作为本部分的总结。</p>
<h5 id="NameNode-初始化启动，进入-Standby-状态"><a href="#NameNode-初始化启动，进入-Standby-状态" class="headerlink" title="NameNode 初始化启动，进入 Standby 状态"></a>NameNode 初始化启动，进入 Standby 状态</h5><p>在 NameNode 以 HA 模式启动的时候，NameNode 会认为自己处于 Standby 模式，在 NameNode 的构造函数中会加载 FSImage 文件和 EditLog Segment 文件来恢复自己的内存文件系统镜像。在加载 EditLog Segment 的时候，调用 FSEditLog 类的 initSharedJournalsForRead 方法来创建只包含了在 JournalNode 集群上的共享目录的 JournalSet，也就是说，这个时候只会从 JournalNode 集群之中加载 EditLog，而不会加载本地磁盘上的 EditLog。另外值得注意的是，加载的 EditLog Segment 只是处于 finalized 状态的 EditLog Segment，而处于 in-progress 状态的 Segment 需要后续在切换为 Active 状态的时候，进行一次数据恢复过程，将 in-progress 状态的 Segment 转换为 finalized 状态的 Segment 之后再进行读取。</p>
<p>加载完 FSImage 文件和共享目录上的 EditLog Segment 文件之后，NameNode 会启动 EditLogTailer 线程和 StandbyCheckpointer 线程，正式进入 Standby 模式。如前所述，EditLogTailer 线程的作用是定时从 JournalNode 集群上同步 EditLog。而 StandbyCheckpointer 线程的作用其实是为了替代 Hadoop 1.x 版本之中的 Secondary NameNode 的功能，StandbyCheckpointer 线程会在 Standby NameNode 节点上定期进行 Checkpoint，将 Checkpoint 之后的 FSImage 文件上传到 Active NameNode 节点。</p>
<h5 id="NameNode-从-Standby-状态切换为-Active-状态"><a href="#NameNode-从-Standby-状态切换为-Active-状态" class="headerlink" title="NameNode 从 Standby 状态切换为 Active 状态"></a>NameNode 从 Standby 状态切换为 Active 状态</h5><p>当 NameNode 从 Standby 状态切换为 Active 状态的时候，首先需要做的就是停止它在 Standby 状态的时候启动的线程和相关的服务，包括上面提到的 EditLogTailer 线程和 StandbyCheckpointer 线程，然后关闭用于读取 JournalNode 集群的共享目录上的 EditLog 的 JournalSet，接下来会调用 FSEditLog 的 initJournalSetForWrite 方法重新打开 JournalSet。不同的是，这个 JournalSet 内部同时包含了本地磁盘目录和 JournalNode 集群上的共享目录。这些工作完成之后，就开始执行“基于 QJM 的共享存储系统的数据恢复机制分析”一节所描述的流程，调用 FSEditLog 类的 recoverUnclosedStreams 方法让 JournalNode 集群中各个节点上的 EditLog 达成一致。然后调用 EditLogTailer 类的 catchupDuringFailover 方法从 JournalNode 集群上补齐落后的 EditLog。最后打开一个新的 EditLog Segment 用于新写入数据，同时启动 Active NameNode 所需要的线程和服务。</p>
<h5 id="NameNode-从-Active-状态切换为-Standby-状态"><a href="#NameNode-从-Active-状态切换为-Standby-状态" class="headerlink" title="NameNode 从 Active 状态切换为 Standby 状态"></a>NameNode 从 Active 状态切换为 Standby 状态</h5><p>当 NameNode 从 Active 状态切换为 Standby 状态的时候，首先需要做的就是停止它在 Active 状态的时候启动的线程和服务，然后关闭用于读取本地磁盘目录和 JournalNode 集群上的共享目录的 EditLog 的 JournalSet。接下来会调用 FSEditLog 的 initSharedJournalsForRead 方法重新打开用于读取 JournalNode 集群上的共享目录的 JournalSet。这些工作完成之后，就会启动 EditLogTailer 线程和 StandbyCheckpointer 线程，EditLogTailer 线程会定时从 JournalNode 集群上同步 Edit Log。</p>
<h3 id="九-NameNode-高可用运维中的注意事项"><a href="#九-NameNode-高可用运维中的注意事项" class="headerlink" title="九. NameNode 高可用运维中的注意事项"></a>九. NameNode 高可用运维中的注意事项</h3><p>本节结合笔者的实践，从初始化部署和日常运维两个方面介绍一些在 NameNode 高可用运维中的注意事项。</p>
<p>初始化部署<br>如果在开始部署 Hadoop 集群的时候就启用 NameNode 的高可用的话，那么相对会比较容易。但是如果在采用传统的单 NameNode 的架构运行了一段时间之后，升级为 NameNode 的高可用架构的话，就要特别注意在升级的时候需要按照以下的步骤进行操作：</p>
<p>对 Zookeeper 进行初始化，创建 Zookeeper 上的/hadoop-ha/${dfs.nameservices} 节点。创建节点是为随后通过 Zookeeper 进行主备选举做好准备，在进行主备选举的时候会在这个节点下面创建子节点 (具体可参照“ActiveStandbyElector 实现分析”一节的叙述)。这一步通过在原有的 NameNode 上执行命令 hdfs zkfc -formatZK 来完成。<br>启动所有的 JournalNode，这通过脚本命令 hadoop-daemon.sh start journalnode 来完成。<br>对 JouranlNode 集群的共享存储目录进行格式化，并且将原有的 NameNode 本地磁盘上最近一次 checkpoint 操作生成 FSImage 文件 (具体可参照“NameNode 的元数据存储概述”一节的叙述) 之后的 EditLog 拷贝到 JournalNode 集群上的共享目录之中，这通过在原有的 NameNode 上执行命令 hdfs namenode -initializeSharedEdits 来完成。<br>启动原有的 NameNode 节点，这通过脚本命令 hadoop-daemon.sh start namenode 完成。<br>对新增的 NameNode 节点进行初始化，将原有的 NameNode 本地磁盘上最近一次 checkpoint 操作生成 FSImage 文件拷贝到这个新增的 NameNode 的本地磁盘上，同时需要验证 JournalNode 集群的共享存储目录上已经具有了这个 FSImage 文件之后的 EditLog(已经在第 3 步完成了)。这一步通过在新增的 NameNode 上执行命令 hdfs namenode -bootstrapStandby 来完成。<br>启动新增的 NameNode 节点，这通过脚本命令 hadoop-daemon.sh start namenode 完成。<br>在这两个 NameNode 上启动 zkfc(ZKFailoverController) 进程，谁通过 Zookeeper 选主成功，谁就是主 NameNode，另一个为备 NameNode。这通过脚本命令 hadoop-daemon.sh start zkfc 完成。<br>日常维护<br>笔者在日常的维护之中主要遇到过下面两种问题：</p>
<p>Zookeeper 过于敏感：Hadoop 的配置项中 Zookeeper 的 session timeout 的配置参数 ha.zookeeper.session-timeout.ms 的默认值为 5000，也就是 5s，这个值比较小，会导致 Zookeeper 比较敏感，可以把这个值尽量设置得大一些，避免因为网络抖动等原因引起 NameNode 进行无谓的主备切换。</p>
<p>单台 JouranlNode 故障时会导致主备无法切换：在理论上，如果有 3 台或者更多的 JournalNode，那么挂掉一台 JouranlNode 应该仍然可以进行正常的主备切换。但是笔者在某次 NameNode 重启的时候，正好赶上一台 JournalNode 挂掉宕机了，这个时候虽然某一台 NameNode 通过 Zookeeper 选主成功，但是这台被选为主的 NameNode 无法成功地从 Standby 状态切换为 Active 状态。事后追查原因发现，被选为主的 NameNode 卡在退出 Standby 状态的最后一步，这个时候它需要等待到 JournalNode 的请求全部完成之后才能退出。但是由于有一台 JouranlNode 宕机，到这台 JournalNode 的请求都积压在一起并且在不断地进行重试，同时在 Hadoop 的配置项中重试次数的默认值非常大，所以就会导致被选为主的 NameNode 无法及时退出 Standby 状态。这个问题主要是 Hadoop 内部的 RPC 通信框架的设计缺陷引起的，Hadoop HA 的源代码 IPCLoggerChannel 类中有关于这个问题的 TODO，但是截止到社区发布的 2.7.1 版本这个问题仍然存在。</p>
]]></content>
      <categories>
        <category>Hadoop</category>
      </categories>
      <tags>
        <tag>Hadoop</tag>
      </tags>
  </entry>
  <entry>
    <title>MySQL双击热备实现</title>
    <url>/2016/03/28/mysql%E5%8F%8C%E6%9C%BA%E7%83%AD%E5%A4%87/</url>
    <content><![CDATA[<h1 id="mysql-双机热备"><a href="#mysql-双机热备" class="headerlink" title="mysql 双机热备"></a>mysql 双机热备</h1><h3 id="一-环境及注意事项"><a href="#一-环境及注意事项" class="headerlink" title="一. 环境及注意事项"></a>一. 环境及注意事项</h3><p>msyql双机热备即mysql 的 主主模式，即</p>
<p>mysql的机器可以互相读写，保障了数据的同步</p>
<p>下面是mysql 主主的实现过程:</p>
<p>测试环境: CentOS 6.8</p>
<p>主机 ip: 192.168.163.22;192.168.163.23</p>
<p>配置注意项:</p>
<ol>
<li>两台主机的mysql 版本要一致</li>
<li>两台mysql的初始状态要一致，即mysql库中的数据库要一致</li>
<li>关闭防火墙</li>
</ol>
<h3 id="二-配置过程"><a href="#二-配置过程" class="headerlink" title="二. 配置过程"></a>二. 配置过程</h3><p>1.配置 msyql 配置文件:</p>
<p>在 192.168.163.22 上的配置如下:</p>
<p>vim /etc/my.cnf：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">log-bin&#x3D;mysql-bin  &#x2F;&#x2F;表示mysql的二进制日志，因为主主的实现是通过读取二进制日志的位置实现的</span><br><span class="line">binlog_format&#x3D;mixed &#x2F;&#x2F;二进制日志格式</span><br><span class="line">server-id&#x3D;22 &#x2F;&#x2F;server-id号，这个号码两台主机上一定不能一样,通常可取ip的最后一位</span><br><span class="line"></span><br><span class="line">read-only&#x3D;0 &#x2F;&#x2F;只读模式为False</span><br><span class="line">binlog-ignore-db&#x3D;mysql  </span><br><span class="line">binlog-ignore-db&#x3D;information_schema</span><br><span class="line">binlog-ignore-db&#x3D;performance_schema</span><br><span class="line">binlog-ignore-db&#x3D;sys  &#x2F;&#x2F; 这几段表示 忽略同步的数据库</span><br><span class="line"></span><br><span class="line">auto-increment-increment&#x3D;1</span><br><span class="line">auto-increment-offset&#x3D;1 &#x2F;&#x2F; 用于在 双主（多主循环）互相备份。 因为每台数据库服务器都可能在同一个表中插入数据，如果表有一个自动增长的主键，那么就会在多服务器上出现主键冲突。  解决这个问题的办法就是让每个数据库的自增主键不连续。  上图说是， 我假设需要将来可能需要10台服务器做备份， 所以auto-increment-increment 设为10.   而 auto-increment-offset&#x3D;1 表示这台服务器的序号。 从1开始， 不超过auto-increment-increment。</span><br><span class="line">这样做之后， 我在这台服务器上插入的第一个id就是 1， 第二行的id就是 11了， 而不是2</span><br><span class="line"></span><br><span class="line">replicate-ignore-db&#x3D;mysql</span><br><span class="line">replicate-ignore-db&#x3D;information_schema</span><br><span class="line">replicate-ignore-db&#x3D;performance_schema</span><br><span class="line">replicate-ignore-db&#x3D;sys &#x2F;&#x2F; 这几段表示 复制忽略的数据库</span><br><span class="line">relay_log&#x3D;mysqld-relay-bin &#x2F;&#x2F;中继日志</span><br><span class="line">log-slave-updates&#x3D;ON &#x2F;&#x2F;中继日志执行之后，这些变化是否需要计入自己的binarylog。 当你的B服务器需要作为另外一个服务器的主服务器的时候需要打开。  就是双主互相备份，或者多主循环备份。 我们这里需要， 所以打开。</span><br></pre></td></tr></table></figure>



<p>在 192.168.163.23 上的配置如下:</p>
<p>vim /etc/my.cnf:</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">log-bin&#x3D;mysql-bin</span><br><span class="line">binlog_format&#x3D;mixed</span><br><span class="line">server-id&#x3D;23</span><br><span class="line"></span><br><span class="line">read-only&#x3D;0</span><br><span class="line">binlog-ignore-db&#x3D;mysql</span><br><span class="line">binlog-ignore-db&#x3D;information_schema</span><br><span class="line">binlog-ignore-db&#x3D;performance_schema</span><br><span class="line">binlog-ignore-db&#x3D;sys</span><br><span class="line">auto-increment-increment&#x3D;10</span><br><span class="line">auto-increment-offset&#x3D;2</span><br><span class="line"></span><br><span class="line">replicate-ignore-db&#x3D;mysql</span><br><span class="line">replicate-ignore-db&#x3D;information_schema</span><br><span class="line">replicate-ignore-db&#x3D;performance_schema</span><br><span class="line">replicate-ignore-db&#x3D;sys</span><br><span class="line">relay_log&#x3D;mysqld-relay-bin</span><br><span class="line">log-slave-updates&#x3D;ON</span><br></pre></td></tr></table></figure>


<p>2.对两台主机进行授权:</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">22上:</span><br><span class="line">GRANT REPLICATION SLAVE ON *.* to &#39;root&#39;@&#39;192.168.163.23&#39; identified by &#39;Bb123456@&#39;;</span><br><span class="line"></span><br><span class="line">23上:</span><br><span class="line">GRANT REPLICATION SLAVE ON *.* to &#39;root&#39;@&#39;192.168.163.22&#39; identified by &#39;Bb123456@&#39;;</span><br></pre></td></tr></table></figure>


<p>该步骤的目的是让两台主机在连接时拥有权限，否则连接不被允许</p>
<p>3.进行主从sql 设置:</p>
<p>查看 两台主机的master 状态:</p>
<p>22上:<br><img src="/2016/03/28/mysql%E5%8F%8C%E6%9C%BA%E7%83%AD%E5%A4%87/mysqls1.png" alt="avatar"></p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">在 22 上执行:</span><br><span class="line">change master to master_host&#x3D;&#39;192.168.163.23&#39;,master_user&#x3D;&#39;root&#39;,master_password&#x3D;&#39;Bb123456@&#39;,master_log_file&#x3D;&#39;mysql-bin.000003&#39;,master_log_pos&#x3D;2266;</span><br></pre></td></tr></table></figure>


<p>23上:<br><img src="/2016/03/28/mysql%E5%8F%8C%E6%9C%BA%E7%83%AD%E5%A4%87/mysqls2.png" alt="avatar"></p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">在 23 上执行:</span><br><span class="line">change master to master_host&#x3D;&#39;192.168.163.23&#39;,master_user&#x3D;&#39;root&#39;,master_password&#x3D;&#39;Bb123456@&#39;,master_log_file&#x3D;&#39;mysql-bin.000007&#39;,master_log_pos&#x3D;503;</span><br></pre></td></tr></table></figure>


<p>该操作就让mysql的两台主机 从该二级制日志的位置开始同步</p>
<p>4.开启并查看同步状态:</p>
<p>两台主机上都执行以下操作:</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">start slave;</span><br></pre></td></tr></table></figure>
<p><img src="/2016/03/28/mysql%E5%8F%8C%E6%9C%BA%E7%83%AD%E5%A4%87/mysqls3.png" alt="avatar"></p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">show slave status\G;</span><br></pre></td></tr></table></figure>
<p><img src="/2016/03/28/mysql%E5%8F%8C%E6%9C%BA%E7%83%AD%E5%A4%87/mysqls4.png" alt="avatar"></p>
<p>—两台主机上只用 这两个为 Yes是才表示同步成功</p>
<p>5.测试同步</p>
<p>首先查看两台主机数据库的初始状态:<br><img src="/2016/03/28/mysql%E5%8F%8C%E6%9C%BA%E7%83%AD%E5%A4%87/mysqls5.png" alt="avatar"></p>
<p>然后在 22上删除 数据库 bigeye_dev<br><img src="/2016/03/28/mysql%E5%8F%8C%E6%9C%BA%E7%83%AD%E5%A4%87/mysqls6.png" alt="avatar"></p>
<p>可以看到，从库中的bigeye_dev也删除了</p>
<p>最后到 23 上创建数据库 bigeye_dev<br><img src="/2016/03/28/mysql%E5%8F%8C%E6%9C%BA%E7%83%AD%E5%A4%87/mysqls7.png" alt="avatar"></p>
<p>可以看到，bigeye_dev 也被创建了</p>
<p>报错:</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">Error: &#39;Lost connection to MySQL server at &#39;reading initial communication packet&#39;, system error: 113&#39; errno: 2013 retry-time: 60 retries: 86400</span><br></pre></td></tr></table></figure>


<p>原因:没有授权，导致连接时不被允许</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">解决:GRANT REPLICATION SLAVE ON *.* to &#39;root&#39;@&#39;192.168.163.22&#39; identified by &#39;Bb123456@&#39;;</span><br></pre></td></tr></table></figure>



<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">报错:Fatal error: The slave I&#x2F;O thread stops because master and slave have equal MySQL server UUIDs; these UUIDs must be different for replication to work.</span><br></pre></td></tr></table></figure>


<p>原因:UUID冲突</p>
<p>解决:vi /var/lib/mysql/auto.cnf<br><img src="/2016/03/28/mysql%E5%8F%8C%E6%9C%BA%E7%83%AD%E5%A4%87/mysqls8.png" alt="avatar"></p>
<p>这两个uuid改成不一致即可</p>
]]></content>
      <tags>
        <tag>tedata</tag>
      </tags>
  </entry>
  <entry>
    <title>shell 使用</title>
    <url>/2015/04/23/shell%E4%B8%89%E5%89%91%E5%AE%A2/</url>
    <content><![CDATA[<h1 id="shell-三剑客-grep-sed-awk"><a href="#shell-三剑客-grep-sed-awk" class="headerlink" title="shell 三剑客- grep, sed, awk"></a>shell 三剑客- grep, sed, awk</h1><h3 id="一-awk"><a href="#一-awk" class="headerlink" title="一. awk"></a>一. awk</h3><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">awk  &#39;条件类型1&#123;动作1&#125; 条件类型2&#123;动作2&#125; ...&#39;  filename</span><br><span class="line"></span><br><span class="line">awk -F &#39;:&#39;  &#39;&#123;OFS&#x3D;&quot;:&quot;; $3&gt;100,$7&#x3D;$3+$4 &#123;print $0&#125;&#39; 12.txt</span><br><span class="line">要区分动作和条件，条件可以不用加符号，动作要加符号 &#123; &#125;</span><br><span class="line"></span><br><span class="line">例如</span><br><span class="line">awk -F &#39;:&#39; &#39;&#123;sum&#x3D;sum+$3&#125; END &#123;print sum&#125;&#39; 12.txt</span><br><span class="line">因为 sum&#x3D;sum+$3 和 print sum都是动作，所以要用 &#123; &#125;</span><br><span class="line"></span><br><span class="line">awk的内置变量 </span><br><span class="line">NR 表示行数</span><br><span class="line">NF 表示列数 $NF&#x3D;7</span><br><span class="line"></span><br><span class="line">awk的打印功能</span><br><span class="line">awk -F &#39;:&#39;  &#39;&#123;print $1,$3&#125;&#39;  12.txt 打印文档12.txt的第1和第三段</span><br><span class="line"></span><br><span class="line">awk的匹配功能</span><br><span class="line">awk -F &#39;:&#39;  &#39;&#x2F;rr&#x2F;&#39; 12.txt  匹配 12.txt中带有rr的字符</span><br><span class="line"></span><br><span class="line">awk中的判断与句匹配</span><br><span class="line">awk -F &#39;:&#39; &#39;$3&gt;100 &#123;print $7&#125;&#39; 12.txt</span><br><span class="line">awk -F &#39;:&#39; &#39;$3&#x3D;&#x3D;$4;$1~&quot;root&quot; &#123;print $7&#125;&#39; 12.txt</span><br></pre></td></tr></table></figure>



<h3 id="二-grep-用法"><a href="#二-grep-用法" class="headerlink" title="二. grep 用法"></a>二. grep 用法</h3><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">grep &#39;过滤内容’ 文档</span><br><span class="line">grep  &#39;abc&#39;  1.txt</span><br><span class="line"></span><br><span class="line">grep -E 或egrep  代表可以匹配的时候脱义特殊字符，例如 + ？</span><br><span class="line">grep --color 过滤时候给匹配到的文档加上颜色 </span><br><span class="line">grep -v 表示过滤出和 grep 匹配相反的内容</span><br></pre></td></tr></table></figure>


<h3 id="三-sed-用法"><a href="#三-sed-用法" class="headerlink" title="三. sed 用法"></a>三. sed 用法</h3><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">sed 打印功能</span><br><span class="line">sed   -n &#39;1,5&#39;p 12.txt 表示打印文档12.txt 的1-5段</span><br><span class="line">注意要加 -n 和p </span><br><span class="line">不加-n 会打印所有行</span><br><span class="line">p表示打印意思</span><br><span class="line">sed的脱义符号是 -r</span><br><span class="line"></span><br><span class="line">sed匹配功能</span><br><span class="line">sed  -n &#39;&#x2F;匹配内容&#x2F;&#39;p 12.txt</span><br><span class="line">注意也有 -n 和p</span><br><span class="line"></span><br><span class="line">sed同时进行多个任务</span><br><span class="line">sed -e &#39;&#x2F;root&#x2F;p&#39; -e &#39;&#x2F;body&#39;p 12.txt</span><br><span class="line">sed &#39;&#x2F;root&#x2F;p;&#x2F;body&#x2F;&#39;p 12.txt</span><br><span class="line"></span><br><span class="line">sed 删除功能</span><br><span class="line">sed  &#39;1,5&#39;d 12.txt 表示显示删除文档12.txt的1-5段</span><br><span class="line">sed -i ‘1,5&#39; 12.txt 表示真实删除文档12.txt的1-5段</span><br><span class="line"></span><br><span class="line">sed替换功能</span><br><span class="line">sed  &#39;s&#x2F;被替换内容&#x2F;替换内容&#x2F;g&#39; 文档 全部替换</span><br><span class="line">sed  &#39;1,5s&#x2F;被替换内容&#x2F;替换内容&#x2F;g&#39; 文档 部分替换，替换1-5行</span><br><span class="line"></span><br><span class="line">注意一个语句</span><br><span class="line">sed &#39;s&#x2F;(^[0-9a-zA-Z].*)(:.*:)(.*$)&#x2F;\3\2\1&#x2F;&#39; 12.txt</span><br><span class="line">表示把12.txt最后部分和开始部分调换，注意没有g</span><br></pre></td></tr></table></figure>


]]></content>
      <tags>
        <tag>tedata</tag>
      </tags>
  </entry>
</search>
